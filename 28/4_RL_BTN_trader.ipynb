{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "4_RL_BTN_trader.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP8QAWsIzHDa1mDRym0Ro1i",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/decoderkurt/HUF_RL_2022/blob/main/28/4_RL_BTN_trader.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w7RWWVsX4Ds0",
        "outputId": "95b8495c-7a83-4d9a-efcb-99f191cb0e89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/decoderkurt/RL-Bitcoin-trading-bot"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ucK8tb3yzJSG",
        "outputId": "5181f743-5491-4705-f697-e2c1feaaa2df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'RL-Bitcoin-trading-bot'...\n",
            "remote: Enumerating objects: 285, done.\u001b[K\n",
            "remote: Counting objects: 100% (15/15), done.\u001b[K\n",
            "remote: Compressing objects: 100% (7/7), done.\u001b[K\n",
            "remote: Total 285 (delta 8), reused 15 (delta 8), pack-reused 270\u001b[K\n",
            "Receiving objects: 100% (285/285), 226.50 MiB | 21.37 MiB/s, done.\n",
            "Resolving deltas: 100% (88/88), done.\n",
            "Checking out files: 100% (160/160), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /RL-Bitcoin-trading-bot/RL-Bitcoin-trading-bot_4/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A0_--fc0zWH_",
        "outputId": "6dd4562f-cd22-4286-d97d-058a5a0df34a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/RL-Bitcoin-trading-bot/RL-Bitcoin-trading-bot_4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "dcR5wU_wjENc",
        "outputId": "ed781ea6-2355-4b9a-87d8-f8c2eb8a2bd6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 1)) (1.19.5)\n",
            "Collecting tensorflow==2.3.1\n",
            "  Downloading tensorflow-2.3.1-cp37-cp37m-manylinux2010_x86_64.whl (320.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 320.4 MB 20 kB/s \n",
            "\u001b[?25hCollecting tensorflow-gpu==2.3.1\n",
            "  Downloading tensorflow_gpu-2.3.1-cp37-cp37m-manylinux2010_x86_64.whl (320.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 320.4 MB 22 kB/s \n",
            "\u001b[?25hRequirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 4)) (4.1.2.30)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 5)) (3.2.2)\n",
            "Collecting tensorboardx\n",
            "  Downloading tensorboardX-2.4.1-py2.py3-none-any.whl (124 kB)\n",
            "\u001b[K     |████████████████████████████████| 124 kB 51.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 7)) (1.1.5)\n",
            "Collecting mplfinance\n",
            "  Downloading mplfinance-0.12.8b6-py3-none-any.whl (64 kB)\n",
            "\u001b[K     |████████████████████████████████| 64 kB 2.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.1->-r requirements.txt (line 2)) (3.17.3)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.1->-r requirements.txt (line 2)) (3.3.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.1->-r requirements.txt (line 2)) (1.15.0)\n",
            "Collecting gast==0.3.3\n",
            "  Downloading gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
            "Collecting h5py<2.11.0,>=2.10.0\n",
            "  Downloading h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 49.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.1->-r requirements.txt (line 2)) (1.13.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.1->-r requirements.txt (line 2)) (0.2.0)\n",
            "Collecting numpy\n",
            "  Downloading numpy-1.18.5-cp37-cp37m-manylinux1_x86_64.whl (20.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 20.1 MB 1.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.1->-r requirements.txt (line 2)) (1.1.2)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.1->-r requirements.txt (line 2)) (1.0.0)\n",
            "Collecting tensorflow-estimator<2.4.0,>=2.3.0\n",
            "  Downloading tensorflow_estimator-2.3.0-py2.py3-none-any.whl (459 kB)\n",
            "\u001b[K     |████████████████████████████████| 459 kB 59.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.1->-r requirements.txt (line 2)) (1.6.3)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.1->-r requirements.txt (line 2)) (1.43.0)\n",
            "Requirement already satisfied: tensorboard<3,>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.1->-r requirements.txt (line 2)) (2.7.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.1->-r requirements.txt (line 2)) (1.1.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.1->-r requirements.txt (line 2)) (0.37.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.1->-r requirements.txt (line 2)) (0.4.6)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.1->-r requirements.txt (line 2)) (57.4.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.1->-r requirements.txt (line 2)) (1.8.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.1->-r requirements.txt (line 2)) (3.3.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.1->-r requirements.txt (line 2)) (1.35.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.1->-r requirements.txt (line 2)) (0.6.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.1->-r requirements.txt (line 2)) (2.23.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.1->-r requirements.txt (line 2)) (1.0.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.1->-r requirements.txt (line 2)) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.1->-r requirements.txt (line 2)) (4.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.1->-r requirements.txt (line 2)) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow==2.3.1->-r requirements.txt (line 2)) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow==2.3.1->-r requirements.txt (line 2)) (4.10.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow==2.3.1->-r requirements.txt (line 2)) (3.7.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow==2.3.1->-r requirements.txt (line 2)) (3.10.0.2)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.1->-r requirements.txt (line 2)) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.1->-r requirements.txt (line 2)) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.1->-r requirements.txt (line 2)) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.1->-r requirements.txt (line 2)) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.1->-r requirements.txt (line 2)) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow==2.3.1->-r requirements.txt (line 2)) (3.1.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r requirements.txt (line 5)) (3.0.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r requirements.txt (line 5)) (1.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r requirements.txt (line 5)) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r requirements.txt (line 5)) (0.11.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->-r requirements.txt (line 7)) (2018.9)\n",
            "Installing collected packages: numpy, tensorflow-estimator, h5py, gast, tensorflow-gpu, tensorflow, tensorboardx, mplfinance\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.19.5\n",
            "    Uninstalling numpy-1.19.5:\n",
            "      Successfully uninstalled numpy-1.19.5\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.7.0\n",
            "    Uninstalling tensorflow-estimator-2.7.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.7.0\n",
            "  Attempting uninstall: h5py\n",
            "    Found existing installation: h5py 3.1.0\n",
            "    Uninstalling h5py-3.1.0:\n",
            "      Successfully uninstalled h5py-3.1.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.4.0\n",
            "    Uninstalling gast-0.4.0:\n",
            "      Successfully uninstalled gast-0.4.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.7.0\n",
            "    Uninstalling tensorflow-2.7.0:\n",
            "      Successfully uninstalled tensorflow-2.7.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed gast-0.3.3 h5py-2.10.0 mplfinance-0.12.8b6 numpy-1.18.5 tensorboardx-2.4.1 tensorflow-2.3.1 tensorflow-estimator-2.3.0 tensorflow-gpu-2.3.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /RL-Bitcoin-trading-bot/RL-Bitcoin-trading-bot_4/"
      ],
      "metadata": {
        "id": "NUPwg0TY3ivc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd007225-aade-4fb4-cc42-9309021435fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/RL-Bitcoin-trading-bot/RL-Bitcoin-trading-bot_4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z6uJgLBSgt9h"
      },
      "outputs": [],
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "import pandas as pd\n",
        "from collections import deque\n",
        "import matplotlib.pyplot as plt\n",
        "from mplfinance.original_flavor import candlestick_ohlc\n",
        "import matplotlib.dates as mpl_dates\n",
        "from datetime import datetime\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "def Write_to_file(Date, net_worth, filename='{}.txt'.format(datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))):\n",
        "    for i in net_worth: \n",
        "        Date += \" {}\".format(i)\n",
        "    #print(Date)\n",
        "    if not os.path.exists('logs'):\n",
        "        os.makedirs('logs')\n",
        "    file = open(\"logs/\"+filename, 'a+')\n",
        "    file.write(Date+\"\\n\")\n",
        "    file.close()\n",
        "\n",
        "class TradingGraph:\n",
        "    # A crypto trading visualization using matplotlib made to render custom prices which come in following way:\n",
        "    # Date, Open, High, Low, Close, Volume, net_worth, trades\n",
        "    # call render every step\n",
        "    def __init__(self, Render_range, Show_reward=False):\n",
        "        self.Volume = deque(maxlen=Render_range)\n",
        "        self.net_worth = deque(maxlen=Render_range)\n",
        "        self.render_data = deque(maxlen=Render_range)\n",
        "        self.Render_range = Render_range\n",
        "        self.Show_reward = Show_reward\n",
        "\n",
        "        # We are using the style ‘ggplot’\n",
        "        plt.style.use('ggplot')\n",
        "        # close all plots if there are open\n",
        "        plt.close('all')\n",
        "        # figsize attribute allows us to specify the width and height of a figure in unit inches\n",
        "        self.fig = plt.figure(figsize=(16,8)) \n",
        "\n",
        "        # Create top subplot for price axis\n",
        "        self.ax1 = plt.subplot2grid((6,1), (0,0), rowspan=5, colspan=1)\n",
        "        \n",
        "        # Create bottom subplot for volume which shares its x-axis\n",
        "        self.ax2 = plt.subplot2grid((6,1), (5,0), rowspan=1, colspan=1, sharex=self.ax1)\n",
        "        \n",
        "        # Create a new axis for net worth which shares its x-axis with price\n",
        "        self.ax3 = self.ax1.twinx()\n",
        "\n",
        "        # Formatting Date\n",
        "        self.date_format = mpl_dates.DateFormatter('%d-%m-%Y')\n",
        "        #self.date_format = mpl_dates.DateFormatter('%d-%m-%Y')\n",
        "        \n",
        "        # Add paddings to make graph easier to view\n",
        "        #plt.subplots_adjust(left=0.07, bottom=-0.1, right=0.93, top=0.97, wspace=0, hspace=0)\n",
        "\n",
        "    # Render the environment to the screen\n",
        "    def render(self, Date, Open, High, Low, Close, Volume, net_worth, trades):\n",
        "        # append volume and net_worth to deque list\n",
        "        self.Volume.append(Volume)\n",
        "        self.net_worth.append(net_worth)\n",
        "\n",
        "        # before appending to deque list, need to convert Date to special format\n",
        "        Date = mpl_dates.date2num([pd.to_datetime(Date)])[0]\n",
        "        self.render_data.append([Date, Open, High, Low, Close])\n",
        "        \n",
        "        # Clear the frame rendered last step\n",
        "        self.ax1.clear()\n",
        "        candlestick_ohlc(self.ax1, self.render_data, width=0.8/24, colorup='green', colordown='red', alpha=0.8)\n",
        "\n",
        "        # Put all dates to one list and fill ax2 sublot with volume\n",
        "        Date_Render_range = [i[0] for i in self.render_data]\n",
        "        self.ax2.clear()\n",
        "        self.ax2.fill_between(Date_Render_range, self.Volume, 0)\n",
        "\n",
        "        # draw our net_worth graph on ax3 (shared with ax1) subplot\n",
        "        self.ax3.clear()\n",
        "        self.ax3.plot(Date_Render_range, self.net_worth, color=\"blue\")\n",
        "        \n",
        "        # beautify the x-labels (Our Date format)\n",
        "        self.ax1.xaxis.set_major_formatter(self.date_format)\n",
        "        self.fig.autofmt_xdate()\n",
        "\n",
        "        minimum = np.min(np.array(self.render_data)[:,1:])\n",
        "        maximum = np.max(np.array(self.render_data)[:,1:])\n",
        "        RANGE = maximum - minimum\n",
        "\n",
        "        # sort sell and buy orders, put arrows in appropiate order positions\n",
        "        for trade in trades:\n",
        "            trade_date = mpl_dates.date2num([pd.to_datetime(trade['Date'])])[0]\n",
        "            if trade_date in Date_Render_range:\n",
        "                if trade['type'] == 'buy':\n",
        "                    high_low = trade['Low'] - RANGE*0.02\n",
        "                    ycoords = trade['Low'] - RANGE*0.08\n",
        "                    self.ax1.scatter(trade_date, high_low, c='green', label='green', s = 120, edgecolors='none', marker=\"^\")\n",
        "                else:\n",
        "                    high_low = trade['High'] + RANGE*0.02\n",
        "                    ycoords = trade['High'] + RANGE*0.06\n",
        "                    self.ax1.scatter(trade_date, high_low, c='red', label='red', s = 120, edgecolors='none', marker=\"v\")\n",
        "\n",
        "                if self.Show_reward:\n",
        "                    try:\n",
        "                        self.ax1.annotate('{0:.2f}'.format(trade['Reward']), (trade_date-0.02, high_low), xytext=(trade_date-0.02, ycoords),\n",
        "                                                   bbox=dict(boxstyle='round', fc='w', ec='k', lw=1), fontsize=\"small\")\n",
        "                    except:\n",
        "                        pass\n",
        "\n",
        "        # we need to set layers every step, because we are clearing subplots every step\n",
        "        self.ax2.set_xlabel('Date')\n",
        "        self.ax1.set_ylabel('Price')\n",
        "        self.ax3.set_ylabel('Balance')\n",
        "\n",
        "        # I use tight_layout to replace plt.subplots_adjust\n",
        "        self.fig.tight_layout()\n",
        "\n",
        "        \"\"\"Display image with matplotlib - interrupting other tasks\"\"\"\n",
        "        # Show the graph without blocking the rest of the program\n",
        "        plt.show(block=False)\n",
        "        # Necessary to view frames before they are unrendered\n",
        "        plt.pause(0.001)\n",
        "\n",
        "        \"\"\"Display image with OpenCV - no interruption\"\"\"\n",
        "        \"\"\"\n",
        "        # redraw the canvas\n",
        "        self.fig.canvas.draw()\n",
        "        # convert canvas to image\n",
        "        img = np.fromstring(self.fig.canvas.tostring_rgb(), dtype=np.uint8, sep='')\n",
        "        img  = img.reshape(self.fig.canvas.get_width_height()[::-1] + (3,))\n",
        "\n",
        "        # img is rgb, convert to opencv's default bgr\n",
        "        image = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "        # display image with OpenCV or any operation you like\n",
        "        cv2_imshow(image)\n",
        "\n",
        "        if cv2.waitKey(25) & 0xFF == ord(\"q\"):\n",
        "            cv2.destroyAllWindows()\n",
        "            return\n",
        "        \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Flatten, Conv1D, MaxPooling1D, LSTM\n",
        "from tensorflow.keras import backend as K\n",
        "#tf.config.experimental_run_functions_eagerly(True) # used for debuging and development\n",
        "tf.compat.v1.disable_eager_execution() # usually using this for fastest performance\n",
        "\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if len(gpus) > 0:\n",
        "    print(f'GPUs {gpus}')\n",
        "    try: tf.config.experimental.set_memory_growth(gpus[0], True)\n",
        "    except RuntimeError: pass\n",
        "\n",
        "class Shared_Model:\n",
        "    def __init__(self, input_shape, action_space, lr, optimizer, model=\"Dense\"):\n",
        "        X_input = Input(input_shape)\n",
        "        self.action_space = action_space\n",
        "\n",
        "        # Shared CNN layers:\n",
        "        if model==\"CNN\":\n",
        "            X = Conv1D(filters=64, kernel_size=6, padding=\"same\", activation=\"tanh\")(X_input)\n",
        "            X = MaxPooling1D(pool_size=2)(X)\n",
        "            X = Conv1D(filters=32, kernel_size=3, padding=\"same\", activation=\"tanh\")(X)\n",
        "            X = MaxPooling1D(pool_size=2)(X)\n",
        "            X = Flatten()(X)\n",
        "\n",
        "        # Shared LSTM layers:\n",
        "        elif model==\"LSTM\":\n",
        "            X = LSTM(512, return_sequences=True)(X_input)\n",
        "            X = LSTM(256)(X)\n",
        "\n",
        "        # Shared Dense layers:\n",
        "        else:\n",
        "            X = Flatten()(X_input)\n",
        "            X = Dense(512, activation=\"relu\")(X)\n",
        "        \n",
        "        # Critic model\n",
        "        V = Dense(512, activation=\"relu\")(X)\n",
        "        V = Dense(256, activation=\"relu\")(V)\n",
        "        V = Dense(64, activation=\"relu\")(V)\n",
        "        value = Dense(1, activation=None)(V)\n",
        "\n",
        "        self.Critic = Model(inputs=X_input, outputs = value)\n",
        "        self.Critic.compile(loss=self.critic_PPO2_loss, optimizer=optimizer(lr=lr))\n",
        "\n",
        "        # Actor model\n",
        "        A = Dense(512, activation=\"relu\")(X)\n",
        "        A = Dense(256, activation=\"relu\")(A)\n",
        "        A = Dense(64, activation=\"relu\")(A)\n",
        "        output = Dense(self.action_space, activation=\"softmax\")(A)\n",
        "\n",
        "        self.Actor = Model(inputs = X_input, outputs = output)\n",
        "        self.Actor.compile(loss=self.ppo_loss, optimizer=optimizer(lr=lr))\n",
        "        print(self.Actor.summary())\n",
        "\n",
        "    def ppo_loss(self, y_true, y_pred):\n",
        "        # Defined in https://arxiv.org/abs/1707.06347\n",
        "        advantages, prediction_picks, actions = y_true[:, :1], y_true[:, 1:1+self.action_space], y_true[:, 1+self.action_space:]\n",
        "        LOSS_CLIPPING = 0.2\n",
        "        ENTROPY_LOSS = 0.001\n",
        "        \n",
        "        prob = actions * y_pred\n",
        "        old_prob = actions * prediction_picks\n",
        "\n",
        "        prob = K.clip(prob, 1e-10, 1.0)\n",
        "        old_prob = K.clip(old_prob, 1e-10, 1.0)\n",
        "\n",
        "        ratio = K.exp(K.log(prob) - K.log(old_prob))\n",
        "        \n",
        "        p1 = ratio * advantages\n",
        "        p2 = K.clip(ratio, min_value=1 - LOSS_CLIPPING, max_value=1 + LOSS_CLIPPING) * advantages\n",
        "\n",
        "        actor_loss = -K.mean(K.minimum(p1, p2))\n",
        "\n",
        "        entropy = -(y_pred * K.log(y_pred + 1e-10))\n",
        "        entropy = ENTROPY_LOSS * K.mean(entropy)\n",
        "        \n",
        "        total_loss = actor_loss - entropy\n",
        "\n",
        "        return total_loss\n",
        "\n",
        "    def actor_predict(self, state):\n",
        "        return self.Actor.predict(state)\n",
        "\n",
        "    def critic_PPO2_loss(self, y_true, y_pred):\n",
        "        value_loss = K.mean((y_true - y_pred) ** 2) # standard PPO loss\n",
        "        return value_loss\n",
        "\n",
        "    def critic_predict(self, state):\n",
        "        return self.Critic.predict([state, np.zeros((state.shape[0], 1))])\n",
        "\n",
        "        \n",
        "class Actor_Model:\n",
        "    def __init__(self, input_shape, action_space, lr, optimizer):\n",
        "        X_input = Input(input_shape)\n",
        "        self.action_space = action_space\n",
        "\n",
        "        X = Flatten(input_shape=input_shape)(X_input)\n",
        "        X = Dense(512, activation=\"relu\")(X)\n",
        "        X = Dense(256, activation=\"relu\")(X)\n",
        "        X = Dense(64, activation=\"relu\")(X)\n",
        "        output = Dense(self.action_space, activation=\"softmax\")(X)\n",
        "\n",
        "        self.Actor = Model(inputs = X_input, outputs = output)\n",
        "        self.Actor.compile(loss=self.ppo_loss, optimizer=optimizer(lr=lr))\n",
        "        #print(self.Actor.summary)\n",
        "\n",
        "    def ppo_loss(self, y_true, y_pred):\n",
        "        # Defined in https://arxiv.org/abs/1707.06347\n",
        "        advantages, prediction_picks, actions = y_true[:, :1], y_true[:, 1:1+self.action_space], y_true[:, 1+self.action_space:]\n",
        "        LOSS_CLIPPING = 0.2\n",
        "        ENTROPY_LOSS = 0.001\n",
        "        \n",
        "        prob = actions * y_pred\n",
        "        old_prob = actions * prediction_picks\n",
        "\n",
        "        prob = K.clip(prob, 1e-10, 1.0)\n",
        "        old_prob = K.clip(old_prob, 1e-10, 1.0)\n",
        "\n",
        "        ratio = K.exp(K.log(prob) - K.log(old_prob))\n",
        "        \n",
        "        p1 = ratio * advantages\n",
        "        p2 = K.clip(ratio, min_value=1 - LOSS_CLIPPING, max_value=1 + LOSS_CLIPPING) * advantages\n",
        "\n",
        "        actor_loss = -K.mean(K.minimum(p1, p2))\n",
        "\n",
        "        entropy = -(y_pred * K.log(y_pred + 1e-10))\n",
        "        entropy = ENTROPY_LOSS * K.mean(entropy)\n",
        "        \n",
        "        total_loss = actor_loss - entropy\n",
        "\n",
        "        return total_loss\n",
        "\n",
        "    def actor_predict(self, state):\n",
        "        return self.Actor.predict(state)\n",
        "\n",
        "class Critic_Model:\n",
        "    def __init__(self, input_shape, action_space, lr, optimizer):\n",
        "        X_input = Input(input_shape)\n",
        "\n",
        "        V = Flatten(input_shape=input_shape)(X_input)\n",
        "        V = Dense(512, activation=\"relu\")(V)\n",
        "        V = Dense(256, activation=\"relu\")(V)\n",
        "        V = Dense(64, activation=\"relu\")(V)\n",
        "        value = Dense(1, activation=None)(V)\n",
        "\n",
        "        self.Critic = Model(inputs=X_input, outputs = value)\n",
        "        self.Critic.compile(loss=self.critic_PPO2_loss, optimizer=optimizer(lr=lr))\n",
        "\n",
        "    def critic_PPO2_loss(self, y_true, y_pred):\n",
        "        value_loss = K.mean((y_true - y_pred) ** 2) # standard PPO loss\n",
        "        return value_loss\n",
        "\n",
        "    def critic_predict(self, state):\n",
        "        return self.Critic.predict([state, np.zeros((state.shape[0], 1))])"
      ],
      "metadata": {
        "id": "YQWtbFjmxUcF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
        "import copy\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from collections import deque\n",
        "from tensorboardX import SummaryWriter\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "\n",
        "class CustomAgent:\n",
        "    # A custom Bitcoin trading agent\n",
        "    def __init__(self, lookback_window_size=50, lr=0.00005, epochs=1, optimizer=Adam, batch_size=32, model=\"\"):\n",
        "        self.lookback_window_size = lookback_window_size\n",
        "        self.model = model\n",
        "        \n",
        "        # Action space from 0 to 3, 0 is hold, 1 is buy, 2 is sell\n",
        "        self.action_space = np.array([0, 1, 2])\n",
        "\n",
        "        # folder to save models\n",
        "        self.log_name = datetime.now().strftime(\"%Y_%m_%d_%H_%M\")+\"_Crypto_trader\"\n",
        "        \n",
        "        # State size contains Market+Orders history for the last lookback_window_size steps\n",
        "        self.state_size = (lookback_window_size, 10)\n",
        "\n",
        "        # Neural Networks part bellow\n",
        "        self.lr = lr\n",
        "        self.epochs = epochs\n",
        "        self.optimizer = optimizer\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        # Create shared Actor-Critic network model\n",
        "        self.Actor = self.Critic = Shared_Model(input_shape=self.state_size, action_space = self.action_space.shape[0], lr=self.lr, optimizer = self.optimizer, model=self.model)\n",
        "        # Create Actor-Critic network model\n",
        "        #self.Actor = Actor_Model(input_shape=self.state_size, action_space = self.action_space.shape[0], lr=self.lr, optimizer = self.optimizer)\n",
        "        #self.Critic = Critic_Model(input_shape=self.state_size, action_space = self.action_space.shape[0], lr=self.lr, optimizer = self.optimizer)\n",
        "        \n",
        "    # create tensorboard writer\n",
        "    def create_writer(self, initial_balance, normalize_value, train_episodes):\n",
        "        self.replay_count = 0\n",
        "        self.writer = SummaryWriter('runs/'+self.log_name)\n",
        "\n",
        "        # Create folder to save models\n",
        "        if not os.path.exists(self.log_name):\n",
        "            os.makedirs(self.log_name)\n",
        "\n",
        "        self.start_training_log(initial_balance, normalize_value, train_episodes)\n",
        "            \n",
        "    def start_training_log(self, initial_balance, normalize_value, train_episodes):      \n",
        "        # save training parameters to Parameters.txt file for future\n",
        "        with open(self.log_name+\"/Parameters.txt\", \"w\") as params:\n",
        "            current_date = datetime.now().strftime('%Y-%m-%d %H:%M')\n",
        "            params.write(f\"training start: {current_date}\\n\")\n",
        "            params.write(f\"initial_balance: {initial_balance}\\n\")\n",
        "            params.write(f\"training episodes: {train_episodes}\\n\")\n",
        "            params.write(f\"lookback_window_size: {self.lookback_window_size}\\n\")\n",
        "            params.write(f\"lr: {self.lr}\\n\")\n",
        "            params.write(f\"epochs: {self.epochs}\\n\")\n",
        "            params.write(f\"batch size: {self.batch_size}\\n\")\n",
        "            params.write(f\"normalize_value: {normalize_value}\\n\")\n",
        "            params.write(f\"model: {self.model}\\n\")\n",
        "            \n",
        "    def end_training_log(self):\n",
        "        with open(self.log_name+\"/Parameters.txt\", \"a+\") as params:\n",
        "            current_date = datetime.now().strftime('%Y-%m-%d %H:%M')\n",
        "            params.write(f\"training end: {current_date}\\n\")\n",
        "\n",
        "    def get_gaes(self, rewards, dones, values, next_values, gamma = 0.99, lamda = 0.95, normalize=True):\n",
        "        deltas = [r + gamma * (1 - d) * nv - v for r, d, nv, v in zip(rewards, dones, next_values, values)]\n",
        "        deltas = np.stack(deltas)\n",
        "        gaes = copy.deepcopy(deltas)\n",
        "        for t in reversed(range(len(deltas) - 1)):\n",
        "            gaes[t] = gaes[t] + (1 - dones[t]) * gamma * lamda * gaes[t + 1]\n",
        "\n",
        "        target = gaes + values\n",
        "        if normalize:\n",
        "            gaes = (gaes - gaes.mean()) / (gaes.std() + 1e-8)\n",
        "        return np.vstack(gaes), np.vstack(target)\n",
        "\n",
        "    def replay(self, states, actions, rewards, predictions, dones, next_states):\n",
        "        # reshape memory to appropriate shape for training\n",
        "        states = np.vstack(states)\n",
        "        next_states = np.vstack(next_states)\n",
        "        actions = np.vstack(actions)\n",
        "        predictions = np.vstack(predictions)\n",
        "\n",
        "        # Get Critic network predictions \n",
        "        values = self.Critic.critic_predict(states)\n",
        "        next_values = self.Critic.critic_predict(next_states)\n",
        "        \n",
        "        # Compute advantages\n",
        "        advantages, target = self.get_gaes(rewards, dones, np.squeeze(values), np.squeeze(next_values))\n",
        "        '''\n",
        "        plt.plot(target,'-')\n",
        "        plt.plot(advantages,'.')\n",
        "        ax=plt.gca()\n",
        "        ax.grid(True)\n",
        "        plt.show()\n",
        "        '''\n",
        "        # stack everything to numpy array\n",
        "        y_true = np.hstack([advantages, predictions, actions])\n",
        "        \n",
        "        # training Actor and Critic networks\n",
        "        a_loss = self.Actor.Actor.fit(states, y_true, epochs=self.epochs, verbose=0, shuffle=True, batch_size=self.batch_size)\n",
        "        c_loss = self.Critic.Critic.fit(states, target, epochs=self.epochs, verbose=0, shuffle=True, batch_size=self.batch_size)\n",
        "\n",
        "        self.writer.add_scalar('Data/actor_loss_per_replay', np.sum(a_loss.history['loss']), self.replay_count)\n",
        "        self.writer.add_scalar('Data/critic_loss_per_replay', np.sum(c_loss.history['loss']), self.replay_count)\n",
        "        self.replay_count += 1\n",
        "\n",
        "        return np.sum(a_loss.history['loss']), np.sum(c_loss.history['loss'])\n",
        "\n",
        "    def act(self, state):\n",
        "        # Use the network to predict the next action to take, using the model\n",
        "        prediction = self.Actor.actor_predict(np.expand_dims(state, axis=0))[0]\n",
        "        action = np.random.choice(self.action_space, p=prediction)\n",
        "        return action, prediction\n",
        "        \n",
        "    def save(self, name=\"Crypto_trader\", score=\"\", args=[]):\n",
        "        # save keras model weights\n",
        "        self.Actor.Actor.save_weights(f\"{self.log_name}/{score}_{name}_Actor.h5\")\n",
        "        self.Critic.Critic.save_weights(f\"{self.log_name}/{score}_{name}_Critic.h5\")\n",
        "\n",
        "        # log saved model arguments to file\n",
        "        if len(args) > 0:\n",
        "            with open(f\"{self.log_name}/log.txt\", \"a+\") as log:\n",
        "                current_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "                log.write(f\"{current_time}, {args[0]}, {args[1]}, {args[2]}, {args[3]}, {args[4]}\\n\")\n",
        "\n",
        "    def load(self, folder, name):\n",
        "        # load keras model weights\n",
        "        self.Actor.Actor.load_weights(os.path.join(folder, f\"{name}_Actor.h5\"))\n",
        "        self.Critic.Critic.load_weights(os.path.join(folder, f\"{name}_Critic.h5\"))\n",
        "\n",
        "        \n",
        "class CustomEnv:\n",
        "    # A custom Bitcoin trading environment\n",
        "    def __init__(self, df, initial_balance=1000, lookback_window_size=50, Render_range=100, Show_reward=False, normalize_value=40000):\n",
        "        # Define action space and state size and other custom parameters\n",
        "        self.df = df.dropna().reset_index()\n",
        "        self.df_total_steps = len(self.df)-1\n",
        "        self.initial_balance = initial_balance\n",
        "        self.lookback_window_size = lookback_window_size\n",
        "        self.Render_range = Render_range # render range in visualization\n",
        "        self.Show_reward = Show_reward # show order reward in rendered visualization\n",
        "\n",
        "        # Orders history contains the balance, net_worth, crypto_bought, crypto_sold, crypto_held values for the last lookback_window_size steps\n",
        "        self.orders_history = deque(maxlen=self.lookback_window_size)\n",
        "        \n",
        "        # Market history contains the OHCL values for the last lookback_window_size prices\n",
        "        self.market_history = deque(maxlen=self.lookback_window_size)\n",
        "\n",
        "        self.normalize_value = normalize_value\n",
        "\n",
        "    # Reset the state of the environment to an initial state\n",
        "    def reset(self, env_steps_size = 0):\n",
        "        self.visualization = TradingGraph(Render_range=self.Render_range, Show_reward=self.Show_reward) # init visualization\n",
        "        self.trades = deque(maxlen=self.Render_range) # limited orders memory for visualization\n",
        "        \n",
        "        self.balance = self.initial_balance\n",
        "        self.net_worth = self.initial_balance\n",
        "        self.prev_net_worth = self.initial_balance\n",
        "        self.crypto_held = 0\n",
        "        self.crypto_sold = 0\n",
        "        self.crypto_bought = 0\n",
        "        self.episode_orders = 0 # track episode orders count\n",
        "        self.prev_episode_orders = 0 # track previous episode orders count\n",
        "        self.rewards = deque(maxlen=self.Render_range)\n",
        "        self.env_steps_size = env_steps_size\n",
        "        self.punish_value = 0\n",
        "        if env_steps_size > 0: # used for training dataset\n",
        "            self.start_step = random.randint(self.lookback_window_size, self.df_total_steps - env_steps_size)\n",
        "            self.end_step = self.start_step + env_steps_size\n",
        "        else: # used for testing dataset\n",
        "            self.start_step = self.lookback_window_size\n",
        "            self.end_step = self.df_total_steps\n",
        "            \n",
        "        self.current_step = self.start_step\n",
        "\n",
        "        for i in reversed(range(self.lookback_window_size)):\n",
        "            current_step = self.current_step - i\n",
        "            self.orders_history.append([self.balance, self.net_worth, self.crypto_bought, self.crypto_sold, self.crypto_held])\n",
        "            self.market_history.append([self.df.loc[current_step, 'Open'],\n",
        "                                        self.df.loc[current_step, 'High'],\n",
        "                                        self.df.loc[current_step, 'Low'],\n",
        "                                        self.df.loc[current_step, 'Close'],\n",
        "                                        self.df.loc[current_step, 'Volume']\n",
        "                                        ])\n",
        "\n",
        "        state = np.concatenate((self.market_history, self.orders_history), axis=1)\n",
        "        return state\n",
        "\n",
        "    # Get the data points for the given current_step\n",
        "    def _next_observation(self):\n",
        "        self.market_history.append([self.df.loc[self.current_step, 'Open'],\n",
        "                                    self.df.loc[self.current_step, 'High'],\n",
        "                                    self.df.loc[self.current_step, 'Low'],\n",
        "                                    self.df.loc[self.current_step, 'Close'],\n",
        "                                    self.df.loc[self.current_step, 'Volume']\n",
        "                                    ])\n",
        "        obs = np.concatenate((self.market_history, self.orders_history), axis=1)\n",
        "        return obs\n",
        "\n",
        "    # Execute one time step within the environment\n",
        "    def step(self, action):\n",
        "        self.crypto_bought = 0\n",
        "        self.crypto_sold = 0\n",
        "        self.current_step += 1\n",
        "\n",
        "        # Set the current price to a random price between open and close\n",
        "        #current_price = random.uniform(\n",
        "        #    self.df.loc[self.current_step, 'Open'],\n",
        "        #    self.df.loc[self.current_step, 'Close'])\n",
        "        current_price = self.df.loc[self.current_step, 'Open']\n",
        "        Date = self.df.loc[self.current_step, 'Date'] # for visualization\n",
        "        High = self.df.loc[self.current_step, 'High'] # for visualization\n",
        "        Low = self.df.loc[self.current_step, 'Low'] # for visualization\n",
        "\n",
        "        if action == 0: # Hold\n",
        "            pass\n",
        "\n",
        "        elif action == 1 and self.balance > self.initial_balance/100:\n",
        "            # Buy with 100% of current balance\n",
        "            self.crypto_bought = self.balance / current_price\n",
        "            self.balance -= self.crypto_bought * current_price\n",
        "            self.crypto_held += self.crypto_bought\n",
        "            self.trades.append({'Date' : Date, 'High' : High, 'Low' : Low, 'total': self.crypto_bought, 'type': \"buy\", 'current_price': current_price})\n",
        "            self.episode_orders += 1\n",
        "\n",
        "        elif action == 2 and self.crypto_held>0:\n",
        "            # Sell 100% of current crypto held\n",
        "            self.crypto_sold = self.crypto_held\n",
        "            self.balance += self.crypto_sold * current_price\n",
        "            self.crypto_held -= self.crypto_sold\n",
        "            self.trades.append({'Date' : Date, 'High' : High, 'Low' : Low, 'total': self.crypto_sold, 'type': \"sell\", 'current_price': current_price})\n",
        "            self.episode_orders += 1\n",
        "\n",
        "        self.prev_net_worth = self.net_worth\n",
        "        self.net_worth = self.balance + self.crypto_held * current_price\n",
        "\n",
        "        self.orders_history.append([self.balance, self.net_worth, self.crypto_bought, self.crypto_sold, self.crypto_held])\n",
        "\n",
        "        # Receive calculated reward\n",
        "        reward = self.get_reward()\n",
        "\n",
        "        if self.net_worth <= self.initial_balance/2:\n",
        "            done = True\n",
        "        else:\n",
        "            done = False\n",
        "\n",
        "        obs = self._next_observation() / self.normalize_value\n",
        "        \n",
        "        return obs, reward, done\n",
        "\n",
        "    # Calculate reward\n",
        "    def get_reward(self):\n",
        "        self.punish_value += self.net_worth * 0.00001\n",
        "        if self.episode_orders > 1 and self.episode_orders > self.prev_episode_orders:\n",
        "            self.prev_episode_orders = self.episode_orders\n",
        "            if self.trades[-1]['type'] == \"buy\" and self.trades[-2]['type'] == \"sell\":\n",
        "                reward = self.trades[-2]['total']*self.trades[-2]['current_price'] - self.trades[-2]['total']*self.trades[-1]['current_price']\n",
        "                reward -= self.punish_value\n",
        "                self.punish_value = 0\n",
        "                self.trades[-1][\"Reward\"] = reward\n",
        "                return reward\n",
        "            elif self.trades[-1]['type'] == \"sell\" and self.trades[-2]['type'] == \"buy\":\n",
        "                reward = self.trades[-1]['total']*self.trades[-1]['current_price'] - self.trades[-2]['total']*self.trades[-2]['current_price']\n",
        "                reward -= self.punish_value\n",
        "                self.punish_value = 0\n",
        "                self.trades[-1][\"Reward\"] = reward\n",
        "                return reward\n",
        "        else:\n",
        "            return 0 - self.punish_value\n",
        "\n",
        "    # render environment\n",
        "    def render(self, visualize = False):\n",
        "        #print(f'Step: {self.current_step}, Net Worth: {self.net_worth}')\n",
        "        if visualize:\n",
        "            Date = self.df.loc[self.current_step, 'Date']\n",
        "            Open = self.df.loc[self.current_step, 'Open']\n",
        "            Close = self.df.loc[self.current_step, 'Close']\n",
        "            High = self.df.loc[self.current_step, 'High']\n",
        "            Low = self.df.loc[self.current_step, 'Low']\n",
        "            Volume = self.df.loc[self.current_step, 'Volume']\n",
        "\n",
        "            # Render the environment to the screen\n",
        "            self.visualization.render(Date, Open, High, Low, Close, Volume, self.net_worth, self.trades)\n",
        "\n",
        "        \n",
        "def Random_games(env, visualize, test_episodes = 50, comment=\"\"):\n",
        "    average_net_worth = 0\n",
        "    average_orders = 0\n",
        "    no_profit_episodes = 0\n",
        "    for episode in range(test_episodes):\n",
        "        state = env.reset()\n",
        "        while True:\n",
        "            env.render(visualize)\n",
        "            action = np.random.randint(3, size=1)[0]\n",
        "            state, reward, done = env.step(action)\n",
        "            if env.current_step == env.end_step:\n",
        "                average_net_worth += env.net_worth\n",
        "                average_orders += env.episode_orders\n",
        "                if env.net_worth < env.initial_balance: no_profit_episodes += 1 # calculate episode count where we had negative profit through episode\n",
        "                print(\"episode: {}, net_worth: {}, average_net_worth: {}, orders: {}\".format(episode, env.net_worth, average_net_worth/(episode+1), env.episode_orders))\n",
        "                break\n",
        "\n",
        "    print(\"average {} episodes random net_worth: {}, orders: {}\".format(test_episodes, average_net_worth/test_episodes, average_orders/test_episodes))\n",
        "    # save test results to test_results.txt file\n",
        "    with open(\"test_results.txt\", \"a+\") as results:\n",
        "        current_date = datetime.now().strftime('%Y-%m-%d %H:%M')\n",
        "        results.write(f'{current_date}, {\"Random games\"}, test episodes:{test_episodes}')\n",
        "        results.write(f', net worth:{average_net_worth/(episode+1)}, orders per episode:{average_orders/test_episodes}')\n",
        "        results.write(f', no profit episodes:{no_profit_episodes}, comment: {comment}\\n')\n",
        "\n",
        "def train_agent(env, agent, visualize=False, train_episodes = 50, training_batch_size=500):\n",
        "    agent.create_writer(env.initial_balance, env.normalize_value, train_episodes) # create TensorBoard writer\n",
        "    total_average = deque(maxlen=100) # save recent 100 episodes net worth\n",
        "    best_average = 0 # used to track best average net worth\n",
        "    for episode in range(train_episodes):\n",
        "        state = env.reset(env_steps_size = training_batch_size)\n",
        "\n",
        "        states, actions, rewards, predictions, dones, next_states = [], [], [], [], [], []\n",
        "        for t in range(training_batch_size):\n",
        "            env.render(visualize)\n",
        "            action, prediction = agent.act(state)\n",
        "            next_state, reward, done = env.step(action)\n",
        "            states.append(np.expand_dims(state, axis=0))\n",
        "            next_states.append(np.expand_dims(next_state, axis=0))\n",
        "            action_onehot = np.zeros(3)\n",
        "            action_onehot[action] = 1\n",
        "            actions.append(action_onehot)\n",
        "            rewards.append(reward)\n",
        "            dones.append(done)\n",
        "            predictions.append(prediction)\n",
        "            state = next_state\n",
        "\n",
        "        a_loss, c_loss = agent.replay(states, actions, rewards, predictions, dones, next_states)\n",
        "        total_average.append(env.net_worth)\n",
        "        average = np.average(total_average)\n",
        "        \n",
        "        agent.writer.add_scalar('Data/average net_worth', average, episode)\n",
        "        agent.writer.add_scalar('Data/episode_orders', env.episode_orders, episode)\n",
        "        \n",
        "        print(\"episode: {:<5} net worth {:<7.2f} average: {:<7.2f} orders: {}\".format(episode, env.net_worth, average, env.episode_orders))\n",
        "        if episode > len(total_average):\n",
        "            if best_average < average:\n",
        "                best_average = average\n",
        "                print(\"Saving model\")\n",
        "                agent.save(score=\"{:.2f}\".format(best_average), args=[episode, average, env.episode_orders, a_loss, c_loss])\n",
        "            agent.save()\n",
        "            \n",
        "    agent.end_training_log()\n",
        "\n",
        "def test_agent(env, agent, visualize=True, test_episodes=10, folder=\"\", name=\"Crypto_trader\", comment=\"\"):\n",
        "    agent.load(folder, name)\n",
        "    average_net_worth = 0\n",
        "    average_orders = 0\n",
        "    no_profit_episodes = 0\n",
        "    for episode in range(test_episodes):\n",
        "        state = env.reset()\n",
        "        while True:\n",
        "            env.render(visualize)\n",
        "            action, prediction = agent.act(state)\n",
        "            state, reward, done = env.step(action)\n",
        "            if env.current_step == env.end_step:\n",
        "                average_net_worth += env.net_worth\n",
        "                average_orders += env.episode_orders\n",
        "                if env.net_worth < env.initial_balance: no_profit_episodes += 1 # calculate episode count where we had negative profit through episode\n",
        "                print(\"episode: {:<5}, net_worth: {:<7.2f}, average_net_worth: {:<7.2f}, orders: {}\".format(episode, env.net_worth, average_net_worth/(episode+1), env.episode_orders))\n",
        "                break\n",
        "            \n",
        "    print(\"average {} episodes agent net_worth: {}, orders: {}\".format(test_episodes, average_net_worth/test_episodes, average_orders/test_episodes))\n",
        "    print(\"No profit episodes: {}\".format(no_profit_episodes))\n",
        "    # save test results to test_results.txt file\n",
        "    with open(\"test_results.txt\", \"a+\") as results:\n",
        "        current_date = datetime.now().strftime('%Y-%m-%d %H:%M')\n",
        "        results.write(f'{current_date}, {name}, test episodes:{test_episodes}')\n",
        "        results.write(f', net worth:{average_net_worth/(episode+1)}, orders per episode:{average_orders/test_episodes}')\n",
        "        results.write(f', no profit episodes:{no_profit_episodes}, model: {agent.model}, comment: {comment}\\n')"
      ],
      "metadata": {
        "id": "5ofG9QYeiwkM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('./pricedata.csv')\n",
        "df = df.sort_values('Date')\n",
        "\n",
        "lookback_window_size = 50\n",
        "test_window = 720 # 30 days \n",
        "train_df = df[:-test_window-lookback_window_size]\n",
        "test_df = df[-test_window-lookback_window_size:]\n",
        "\n",
        "agent = CustomAgent(lookback_window_size=lookback_window_size, lr=0.00001, epochs=1, optimizer=Adam, batch_size = 32, model=\"Dense\")\n",
        "test_env = CustomEnv(test_df, lookback_window_size=lookback_window_size, Show_reward=False)\n",
        "test_agent(test_env, agent, visualize=True, test_episodes=10, folder=\"2021_01_11_13_32_Crypto_trader\", name=\"1277.39_Crypto_trader\", comment=\"\")\n",
        "'''\n",
        "agent = CustomAgent(lookback_window_size=lookback_window_size, lr=0.00001, epochs=1, optimizer=Adam, batch_size = 32, model=\"Dense\")\n",
        "#train_env = CustomEnv(train_df, lookback_window_size=lookback_window_size)\n",
        "#train_agent(train_env, agent, visualize=False, train_episodes=50000, training_batch_size=500)\n",
        "test_env = CustomEnv(test_df, lookback_window_size=lookback_window_size, Show_reward=False)\n",
        "test_agent(test_env, agent, visualize=False, test_episodes=10, folder=\"2021_01_11_13_32_Crypto_trader\", name=\"1277.39_Crypto_trader\", comment=\"\")\n",
        "\n",
        "agent = CustomAgent(lookback_window_size=lookback_window_size, lr=0.00001, epochs=1, optimizer=Adam, batch_size = 32, model=\"CNN\")\n",
        "test_env = CustomEnv(test_df, lookback_window_size=lookback_window_size, Show_reward=False)\n",
        "test_agent(test_env, agent, visualize=False, test_episodes=10, folder=\"2021_01_11_23_48_Crypto_trader\", name=\"1772.66_Crypto_trader\", comment=\"\")\n",
        "test_agent(test_env, agent, visualize=False, test_episodes=10, folder=\"2021_01_11_23_48_Crypto_trader\", name=\"1377.86_Crypto_trader\", comment=\"\")\n",
        "\n",
        "agent = CustomAgent(lookback_window_size=lookback_window_size, lr=0.00001, epochs=1, optimizer=Adam, batch_size = 128, model=\"LSTM\")\n",
        "test_env = CustomEnv(test_df, lookback_window_size=lookback_window_size, Show_reward=False)\n",
        "test_agent(test_env, agent, visualize=False, test_episodes=10, folder=\"2021_01_11_23_43_Crypto_trader\", name=\"1076.27_Crypto_trader\", comment=\"\")\n",
        "'''\n",
        "    "
      ],
      "metadata": {
        "id": "m_jtogSVk3jD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "2na8AydB_RpQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}