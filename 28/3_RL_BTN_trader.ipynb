{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3_RL_BTN_trader.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNLszyT2mz4DYf4WeDcjz67",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/decoderkurt/HUF_RL_2022/blob/main/28/3_RL_BTN_trader.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/decoderkurt/RL-Bitcoin-trading-bot"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ucK8tb3yzJSG",
        "outputId": "70ad6ee4-71c0-425a-8917-ff893c5ae865"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'RL-Bitcoin-trading-bot'...\n",
            "remote: Enumerating objects: 285, done.\u001b[K\n",
            "remote: Counting objects: 100% (15/15), done.\u001b[K\n",
            "remote: Compressing objects: 100% (7/7), done.\u001b[K\n",
            "remote: Total 285 (delta 8), reused 15 (delta 8), pack-reused 270\u001b[K\n",
            "Receiving objects: 100% (285/285), 226.50 MiB | 18.27 MiB/s, done.\n",
            "Resolving deltas: 100% (88/88), done.\n",
            "Checking out files: 100% (160/160), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd RL-Bitcoin-trading-bot/RL-Bitcoin-trading-bot_3/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A0_--fc0zWH_",
        "outputId": "7dd1cef2-4618-40cf-d154-535c5c6494f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/RL-Bitcoin-trading-bot/RL-Bitcoin-trading-bot_3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dcR5wU_wjENc",
        "outputId": "9489851f-df1c-4bfc-ffd5-12aa8ac49130"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 1)) (1.18.5)\n",
            "Requirement already satisfied: tensorflow==2.3.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 2)) (2.3.1)\n",
            "Requirement already satisfied: tensorflow-gpu==2.3.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 3)) (2.3.1)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 4)) (4.1.2.30)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 5)) (3.2.2)\n",
            "Requirement already satisfied: tensorboardx in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 6)) (2.4.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 7)) (1.1.5)\n",
            "Requirement already satisfied: mplfinance in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 8)) (0.12.8b6)\n",
            "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.1->-r requirements.txt (line 2)) (2.3.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.1->-r requirements.txt (line 2)) (0.2.0)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.1->-r requirements.txt (line 2)) (1.6.3)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.1->-r requirements.txt (line 2)) (1.1.0)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.1->-r requirements.txt (line 2)) (0.3.3)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.1->-r requirements.txt (line 2)) (3.3.0)\n",
            "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.1->-r requirements.txt (line 2)) (1.1.2)\n",
            "Requirement already satisfied: tensorboard<3,>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.1->-r requirements.txt (line 2)) (2.7.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.1->-r requirements.txt (line 2)) (0.37.1)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.1->-r requirements.txt (line 2)) (1.15.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.1->-r requirements.txt (line 2)) (1.0.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.1->-r requirements.txt (line 2)) (1.43.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.1->-r requirements.txt (line 2)) (1.13.3)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.1->-r requirements.txt (line 2)) (3.17.3)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.1->-r requirements.txt (line 2)) (2.10.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.1->-r requirements.txt (line 2)) (1.8.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.1->-r requirements.txt (line 2)) (1.35.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.1->-r requirements.txt (line 2)) (57.4.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.1->-r requirements.txt (line 2)) (0.6.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.1->-r requirements.txt (line 2)) (1.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.1->-r requirements.txt (line 2)) (2.23.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.1->-r requirements.txt (line 2)) (3.3.6)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.1->-r requirements.txt (line 2)) (0.4.6)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.1->-r requirements.txt (line 2)) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.1->-r requirements.txt (line 2)) (4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.1->-r requirements.txt (line 2)) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow==2.3.1->-r requirements.txt (line 2)) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow==2.3.1->-r requirements.txt (line 2)) (4.10.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow==2.3.1->-r requirements.txt (line 2)) (3.7.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow==2.3.1->-r requirements.txt (line 2)) (3.10.0.2)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.1->-r requirements.txt (line 2)) (0.4.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.1->-r requirements.txt (line 2)) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.1->-r requirements.txt (line 2)) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.1->-r requirements.txt (line 2)) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.1->-r requirements.txt (line 2)) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow==2.3.1->-r requirements.txt (line 2)) (3.1.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r requirements.txt (line 5)) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r requirements.txt (line 5)) (1.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r requirements.txt (line 5)) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r requirements.txt (line 5)) (3.0.7)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->-r requirements.txt (line 7)) (2018.9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd RL-Bitcoin-trading-bot/RL-Bitcoin-trading-bot_3/"
      ],
      "metadata": {
        "id": "NUPwg0TY3ivc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z6uJgLBSgt9h"
      },
      "outputs": [],
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "import pandas as pd\n",
        "from collections import deque\n",
        "import matplotlib.pyplot as plt\n",
        "from mplfinance.original_flavor import candlestick_ohlc\n",
        "import matplotlib.dates as mpl_dates\n",
        "from datetime import datetime\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "def Write_to_file(Date, net_worth, filename='{}.txt'.format(datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))):\n",
        "    for i in net_worth: \n",
        "        Date += \" {}\".format(i)\n",
        "    #print(Date)\n",
        "    if not os.path.exists('logs'):\n",
        "        os.makedirs('logs')\n",
        "    file = open(\"logs/\"+filename, 'a+')\n",
        "    file.write(Date+\"\\n\")\n",
        "    file.close()\n",
        "\n",
        "class TradingGraph:\n",
        "    # A crypto trading visualization using matplotlib made to render custom prices which come in following way:\n",
        "    # Date, Open, High, Low, Close, Volume, net_worth, trades\n",
        "    # call render every step\n",
        "    def __init__(self, Render_range):\n",
        "        self.Volume = deque(maxlen=Render_range)\n",
        "        self.net_worth = deque(maxlen=Render_range)\n",
        "        self.render_data = deque(maxlen=Render_range)\n",
        "        self.Render_range = Render_range\n",
        "\n",
        "        # We are using the style ‘ggplot’\n",
        "        plt.style.use('ggplot')\n",
        "        # close all plots if there are open\n",
        "        plt.close('all')\n",
        "        # figsize attribute allows us to specify the width and height of a figure in unit inches\n",
        "        self.fig = plt.figure(figsize=(16,8)) \n",
        "\n",
        "        # Create top subplot for price axis\n",
        "        self.ax1 = plt.subplot2grid((6,1), (0,0), rowspan=5, colspan=1)\n",
        "        \n",
        "        # Create bottom subplot for volume which shares its x-axis\n",
        "        self.ax2 = plt.subplot2grid((6,1), (5,0), rowspan=1, colspan=1, sharex=self.ax1)\n",
        "        \n",
        "        # Create a new axis for net worth which shares its x-axis with price\n",
        "        self.ax3 = self.ax1.twinx()\n",
        "\n",
        "        # Formatting Date\n",
        "        self.date_format = mpl_dates.DateFormatter('%d-%m-%Y')\n",
        "        #self.date_format = mpl_dates.DateFormatter('%d-%m-%Y')\n",
        "        \n",
        "        # Add paddings to make graph easier to view\n",
        "        #plt.subplots_adjust(left=0.07, bottom=-0.1, right=0.93, top=0.97, wspace=0, hspace=0)\n",
        "\n",
        "    # Render the environment to the screen\n",
        "    def render(self, Date, Open, High, Low, Close, Volume, net_worth, trades):\n",
        "        # append volume and net_worth to deque list\n",
        "        self.Volume.append(Volume)\n",
        "        self.net_worth.append(net_worth)\n",
        "\n",
        "        # before appending to deque list, need to convert Date to special format\n",
        "        Date = mpl_dates.date2num([pd.to_datetime(Date)])[0]\n",
        "        self.render_data.append([Date, Open, High, Low, Close])\n",
        "        \n",
        "        # Clear the frame rendered last step\n",
        "        self.ax1.clear()\n",
        "        candlestick_ohlc(self.ax1, self.render_data, width=0.8/24, colorup='green', colordown='red', alpha=0.8)\n",
        "\n",
        "        # Put all dates to one list and fill ax2 sublot with volume\n",
        "        Date_Render_range = [i[0] for i in self.render_data]\n",
        "        self.ax2.clear()\n",
        "        self.ax2.fill_between(Date_Render_range, self.Volume, 0)\n",
        "\n",
        "        # draw our net_worth graph on ax3 (shared with ax1) subplot\n",
        "        self.ax3.clear()\n",
        "        self.ax3.plot(Date_Render_range, self.net_worth, color=\"blue\")\n",
        "        \n",
        "        # beautify the x-labels (Our Date format)\n",
        "        self.ax1.xaxis.set_major_formatter(self.date_format)\n",
        "        self.fig.autofmt_xdate()\n",
        "\n",
        "        # sort sell and buy orders, put arrows in appropiate order positions\n",
        "        for trade in trades:\n",
        "            trade_date = mpl_dates.date2num([pd.to_datetime(trade['Date'])])[0]\n",
        "            if trade_date in Date_Render_range:\n",
        "                if trade['type'] == 'buy':\n",
        "                    high_low = trade['Low']-10\n",
        "                    self.ax1.scatter(trade_date, high_low, c='green', label='green', s = 120, edgecolors='none', marker=\"^\")\n",
        "                else:\n",
        "                    high_low = trade['High']+10\n",
        "                    self.ax1.scatter(trade_date, high_low, c='red', label='red', s = 120, edgecolors='none', marker=\"v\")\n",
        "\n",
        "        # we need to set layers every step, because we are clearing subplots every step\n",
        "        self.ax2.set_xlabel('Date')\n",
        "        self.ax1.set_ylabel('Price')\n",
        "        self.ax3.set_ylabel('Balance')\n",
        "\n",
        "        # I use tight_layout to replace plt.subplots_adjust\n",
        "        self.fig.tight_layout()\n",
        "\n",
        "        \"\"\"Display image with matplotlib - interrupting other tasks\"\"\"\n",
        "        # Show the graph without blocking the rest of the program\n",
        "        #plt.show(block=False)\n",
        "        # Necessary to view frames before they are unrendered\n",
        "        #plt.pause(0.001)\n",
        "\n",
        "        \"\"\"Display image with OpenCV - no interruption\"\"\"\n",
        "        # redraw the canvas\n",
        "        self.fig.canvas.draw()\n",
        "        # convert canvas to image\n",
        "        img = np.fromstring(self.fig.canvas.tostring_rgb(), dtype=np.uint8, sep='')\n",
        "        img  = img.reshape(self.fig.canvas.get_width_height()[::-1] + (3,))\n",
        "\n",
        "        # img is rgb, convert to opencv's default bgr\n",
        "        image = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "        # display image with OpenCV or any operation you like\n",
        "        cv2_imshow(image)\n",
        "\n",
        "        if cv2.waitKey(25) & 0xFF == ord(\"q\"):\n",
        "            cv2.destroyAllWindows()\n",
        "            return"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Flatten\n",
        "from tensorflow.keras import backend as K\n",
        "#tf.config.experimental_run_functions_eagerly(True) # used for debuging and development\n",
        "tf.compat.v1.disable_eager_execution() # usually using this for fastest performance\n",
        "\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if len(gpus) > 0:\n",
        "    print(f'GPUs {gpus}')\n",
        "    try: tf.config.experimental.set_memory_growth(gpus[0], True)\n",
        "    except RuntimeError: pass\n",
        "\n",
        "# https://github.com/decoderkurt/HUF_RL_2022/blob/main/24/simple_ppo.ipynb\n",
        "\n",
        "class Actor_Model:\n",
        "    def __init__(self, input_shape, action_space, lr, optimizer):\n",
        "        X_input = Input(input_shape)\n",
        "        self.action_space = action_space\n",
        "\n",
        "        X = Flatten(input_shape=input_shape)(X_input)\n",
        "        X = Dense(512, activation=\"relu\")(X)\n",
        "        X = Dense(256, activation=\"relu\")(X)\n",
        "        X = Dense(64, activation=\"relu\")(X)\n",
        "        output = Dense(self.action_space, activation=\"softmax\")(X)\n",
        "\n",
        "        self.Actor = Model(inputs = X_input, outputs = output)\n",
        "        self.Actor.compile(loss=self.ppo_loss, optimizer=optimizer(lr=lr))\n",
        "\n",
        "    def ppo_loss(self, y_true, y_pred):\n",
        "        # Defined in https://arxiv.org/abs/1707.06347\n",
        "        advantages, prediction_picks, actions = y_true[:, :1], y_true[:, 1:1+self.action_space], y_true[:, 1+self.action_space:]\n",
        "        LOSS_CLIPPING = 0.2\n",
        "        ENTROPY_LOSS = 0.001\n",
        "        \n",
        "        prob = actions * y_pred\n",
        "        old_prob = actions * prediction_picks\n",
        "\n",
        "        prob = K.clip(prob, 1e-10, 1.0)\n",
        "        old_prob = K.clip(old_prob, 1e-10, 1.0)\n",
        "\n",
        "        ratio = K.exp(K.log(prob) - K.log(old_prob))\n",
        "        \n",
        "        p1 = ratio * advantages\n",
        "        p2 = K.clip(ratio, min_value=1 - LOSS_CLIPPING, max_value=1 + LOSS_CLIPPING) * advantages\n",
        "\n",
        "        actor_loss = -K.mean(K.minimum(p1, p2))\n",
        "\n",
        "        entropy = -(y_pred * K.log(y_pred + 1e-10))\n",
        "        entropy = ENTROPY_LOSS * K.mean(entropy)\n",
        "        \n",
        "        total_loss = actor_loss - entropy\n",
        "\n",
        "        return total_loss\n",
        "\n",
        "    def predict(self, state):\n",
        "        return self.Actor.predict(state)\n",
        "\n",
        "class Critic_Model:\n",
        "    def __init__(self, input_shape, action_space, lr, optimizer):\n",
        "        X_input = Input(input_shape)\n",
        "\n",
        "        V = Flatten(input_shape=input_shape)(X_input)\n",
        "        V = Dense(512, activation=\"relu\")(V)\n",
        "        V = Dense(256, activation=\"relu\")(V)\n",
        "        V = Dense(64, activation=\"relu\")(V)\n",
        "        value = Dense(1, activation=None)(V)\n",
        "\n",
        "        self.Critic = Model(inputs=X_input, outputs = value)\n",
        "        self.Critic.compile(loss=self.critic_PPO2_loss, optimizer=optimizer(lr=lr))\n",
        "\n",
        "    def critic_PPO2_loss(self, y_true, y_pred):\n",
        "        value_loss = K.mean((y_true - y_pred) ** 2) # standard PPO loss\n",
        "        return value_loss\n",
        "\n",
        "    def predict(self, state):\n",
        "        return self.Critic.predict([state, np.zeros((state.shape[0], 1))])"
      ],
      "metadata": {
        "id": "YQWtbFjmxUcF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
        "import copy\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from collections import deque\n",
        "from tensorboardX import SummaryWriter\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop\n",
        "\n",
        "class CustomEnv:\n",
        "    # A custom Bitcoin trading environment\n",
        "    def __init__(self, df, initial_balance=1000, lookback_window_size=50, Render_range = 100):\n",
        "        # Define action space and state size and other custom parameters\n",
        "        self.df = df.dropna().reset_index()\n",
        "        self.df_total_steps = len(self.df)-1\n",
        "        self.initial_balance = initial_balance\n",
        "        self.lookback_window_size = lookback_window_size\n",
        "        self.Render_range = Render_range # render range in visualization\n",
        "\n",
        "        # Action space from 0 to 3, 0 is hold, 1 is buy, 2 is sell\n",
        "        self.action_space = np.array([0, 1, 2])\n",
        "\n",
        "        # Orders history contains the balance, net_worth, crypto_bought, crypto_sold, crypto_held values for the last lookback_window_size steps\n",
        "        self.orders_history = deque(maxlen=self.lookback_window_size)\n",
        "        \n",
        "        # Market history contains the OHCL values for the last lookback_window_size prices\n",
        "        self.market_history = deque(maxlen=self.lookback_window_size)\n",
        "\n",
        "        # State size contains Market+Orders history for the last lookback_window_size steps\n",
        "        self.state_size = (self.lookback_window_size, 10)\n",
        "\n",
        "        # Neural Networks part bellow\n",
        "        self.lr = 0.00001\n",
        "        self.epochs = 1\n",
        "        self.normalize_value = 100000\n",
        "        self.optimizer = Adam\n",
        "\n",
        "        # Create Actor-Critic network model\n",
        "        self.Actor = Actor_Model(input_shape=self.state_size, action_space = self.action_space.shape[0], lr=self.lr, optimizer = self.optimizer)\n",
        "        self.Critic = Critic_Model(input_shape=self.state_size, action_space = self.action_space.shape[0], lr=self.lr, optimizer = self.optimizer)\n",
        "\n",
        "    # create tensorboard writer\n",
        "    def create_writer(self):\n",
        "        self.replay_count = 0\n",
        "        self.writer = SummaryWriter(comment=\"Crypto_trader\")\n",
        "\n",
        "    # Reset the state of the environment to an initial state\n",
        "    def reset(self, env_steps_size = 0):\n",
        "        self.visualization = TradingGraph(Render_range=self.Render_range) # init visualization\n",
        "        self.trades = deque(maxlen=self.Render_range) # limited orders memory for visualization\n",
        "        \n",
        "        self.balance = self.initial_balance\n",
        "        self.net_worth = self.initial_balance\n",
        "        self.prev_net_worth = self.initial_balance\n",
        "        self.crypto_held = 0\n",
        "        self.crypto_sold = 0\n",
        "        self.crypto_bought = 0\n",
        "        self.episode_orders = 0 # test\n",
        "        self.env_steps_size = env_steps_size\n",
        "        if env_steps_size > 0: # used for training dataset\n",
        "            self.start_step = random.randint(self.lookback_window_size, self.df_total_steps - env_steps_size)\n",
        "            self.end_step = self.start_step + env_steps_size\n",
        "        else: # used for testing dataset\n",
        "            self.start_step = self.lookback_window_size\n",
        "            self.end_step = self.df_total_steps\n",
        "            \n",
        "        self.current_step = self.start_step\n",
        "\n",
        "        for i in reversed(range(self.lookback_window_size)):\n",
        "            current_step = self.current_step - i\n",
        "            self.orders_history.append([self.balance, self.net_worth, self.crypto_bought, self.crypto_sold, self.crypto_held])\n",
        "            self.market_history.append([self.df.loc[current_step, 'Open'],\n",
        "                                        self.df.loc[current_step, 'High'],\n",
        "                                        self.df.loc[current_step, 'Low'],\n",
        "                                        self.df.loc[current_step, 'Close'],\n",
        "                                        self.df.loc[current_step, 'Volume']\n",
        "                                        ])\n",
        "\n",
        "        state = np.concatenate((self.market_history, self.orders_history), axis=1)\n",
        "        return state\n",
        "\n",
        "    # Get the data points for the given current_step\n",
        "    def _next_observation(self):\n",
        "        self.market_history.append([self.df.loc[self.current_step, 'Open'],\n",
        "                                    self.df.loc[self.current_step, 'High'],\n",
        "                                    self.df.loc[self.current_step, 'Low'],\n",
        "                                    self.df.loc[self.current_step, 'Close'],\n",
        "                                    self.df.loc[self.current_step, 'Volume']\n",
        "                                    ])\n",
        "        obs = np.concatenate((self.market_history, self.orders_history), axis=1)\n",
        "        return obs\n",
        "\n",
        "    # Execute one time step within the environment\n",
        "    def step(self, action):\n",
        "        self.crypto_bought = 0\n",
        "        self.crypto_sold = 0\n",
        "        self.current_step += 1\n",
        "\n",
        "        # Set the current price to a random price between open and close\n",
        "        current_price = random.uniform(\n",
        "            self.df.loc[self.current_step, 'Open'],\n",
        "            self.df.loc[self.current_step, 'Close'])\n",
        "        Date = self.df.loc[self.current_step, 'Date'] # for visualization\n",
        "        High = self.df.loc[self.current_step, 'High'] # for visualization\n",
        "        Low = self.df.loc[self.current_step, 'Low'] # for visualization\n",
        "        \n",
        "        if action == 0: # Hold\n",
        "            pass\n",
        "\n",
        "        elif action == 1 and self.balance > self.initial_balance/100:\n",
        "            # Buy with 100% of current balance\n",
        "            self.crypto_bought = self.balance / current_price\n",
        "            self.balance -= self.crypto_bought * current_price\n",
        "            self.crypto_held += self.crypto_bought\n",
        "            self.trades.append({'Date' : Date, 'High' : High, 'Low' : Low, 'total': self.crypto_bought, 'type': \"buy\"})\n",
        "            self.episode_orders += 1\n",
        "\n",
        "        elif action == 2 and self.crypto_held>0:\n",
        "            # Sell 100% of current crypto held\n",
        "            self.crypto_sold = self.crypto_held\n",
        "            self.balance += self.crypto_sold * current_price\n",
        "            self.crypto_held -= self.crypto_sold\n",
        "            self.trades.append({'Date' : Date, 'High' : High, 'Low' : Low, 'total': self.crypto_sold, 'type': \"sell\"})\n",
        "            self.episode_orders += 1\n",
        "\n",
        "        self.prev_net_worth = self.net_worth\n",
        "        self.net_worth = self.balance + self.crypto_held * current_price\n",
        "\n",
        "        self.orders_history.append([self.balance, self.net_worth, self.crypto_bought, self.crypto_sold, self.crypto_held])\n",
        "        #Write_to_file(Date, self.orders_history[-1])\n",
        "\n",
        "        # Calculate reward\n",
        "        reward = self.net_worth - self.prev_net_worth\n",
        "\n",
        "        if self.net_worth <= self.initial_balance/2:\n",
        "            done = True\n",
        "        else:\n",
        "            done = False\n",
        "\n",
        "        obs = self._next_observation() / self.normalize_value\n",
        "        \n",
        "        return obs, reward, done\n",
        "\n",
        "    # render environment\n",
        "    def render(self, visualize = False):\n",
        "        #print(f'Step: {self.current_step}, Net Worth: {self.net_worth}')\n",
        "        if visualize:\n",
        "            Date = self.df.loc[self.current_step, 'Date']\n",
        "            Open = self.df.loc[self.current_step, 'Open']\n",
        "            Close = self.df.loc[self.current_step, 'Close']\n",
        "            High = self.df.loc[self.current_step, 'High']\n",
        "            Low = self.df.loc[self.current_step, 'Low']\n",
        "            Volume = self.df.loc[self.current_step, 'Volume']\n",
        "\n",
        "            # Render the environment to the screen\n",
        "            self.visualization.render(Date, Open, High, Low, Close, Volume, self.net_worth, self.trades)\n",
        "\n",
        "    def get_gaes(self, rewards, dones, values, next_values, gamma = 0.99, lamda = 0.95, normalize=True):\n",
        "        deltas = [r + gamma * (1 - d) * nv - v for r, d, nv, v in zip(rewards, dones, next_values, values)]\n",
        "        deltas = np.stack(deltas)\n",
        "        gaes = copy.deepcopy(deltas)\n",
        "        for t in reversed(range(len(deltas) - 1)):\n",
        "            gaes[t] = gaes[t] + (1 - dones[t]) * gamma * lamda * gaes[t + 1]\n",
        "\n",
        "        target = gaes + values\n",
        "        if normalize:\n",
        "            gaes = (gaes - gaes.mean()) / (gaes.std() + 1e-8)\n",
        "        return np.vstack(gaes), np.vstack(target)\n",
        "\n",
        "    def replay(self, states, actions, rewards, predictions, dones, next_states):\n",
        "        # reshape memory to appropriate shape for training\n",
        "        states = np.vstack(states)\n",
        "        next_states = np.vstack(next_states)\n",
        "        actions = np.vstack(actions)\n",
        "        predictions = np.vstack(predictions)\n",
        "\n",
        "        # Compute discounted rewards\n",
        "        #discounted_r = np.vstack(self.discount_rewards(rewards))\n",
        "\n",
        "        # Get Critic network predictions \n",
        "        values = self.Critic.predict(states)\n",
        "        next_values = self.Critic.predict(next_states)\n",
        "        # Compute advantages\n",
        "        #advantages = discounted_r - values\n",
        "        advantages, target = self.get_gaes(rewards, dones, np.squeeze(values), np.squeeze(next_values))\n",
        "        '''\n",
        "        pylab.plot(target,'-')\n",
        "        pylab.plot(advantages,'.')\n",
        "        ax=pylab.gca()\n",
        "        ax.grid(True)\n",
        "        pylab.show()\n",
        "        '''\n",
        "        # stack everything to numpy array\n",
        "        y_true = np.hstack([advantages, predictions, actions])\n",
        "        \n",
        "        # training Actor and Critic networks\n",
        "        a_loss = self.Actor.Actor.fit(states, y_true, epochs=self.epochs, verbose=0, shuffle=True)\n",
        "        c_loss = self.Critic.Critic.fit(states, target, epochs=self.epochs, verbose=0, shuffle=True)\n",
        "\n",
        "        self.writer.add_scalar('Data/actor_loss_per_replay', np.sum(a_loss.history['loss']), self.replay_count)\n",
        "        self.writer.add_scalar('Data/critic_loss_per_replay', np.sum(c_loss.history['loss']), self.replay_count)\n",
        "        self.replay_count += 1\n",
        "        \n",
        "    def act(self, state):\n",
        "        # Use the network to predict the next action to take, using the model\n",
        "        prediction = self.Actor.predict(np.expand_dims(state, axis=0))[0]\n",
        "        action = np.random.choice(self.action_space, p=prediction)\n",
        "        return action, prediction\n",
        "\n",
        "    def save(self, name=\"Crypto_trader\"):\n",
        "        # save keras model weights\n",
        "        self.Actor.Actor.save_weights(f\"{name}_Actor.h5\")\n",
        "        self.Critic.Critic.save_weights(f\"{name}_Critic.h5\")\n",
        "\n",
        "    def load(self, name=\"Crypto_trader\"):\n",
        "        # load keras model weights\n",
        "        self.Actor.Actor.load_weights(f\"{name}_Actor.h5\")\n",
        "        self.Critic.Critic.load_weights(f\"{name}_Critic.h5\")\n",
        "        \n",
        "def Random_games(env, visualize, train_episodes = 50):\n",
        "    average_net_worth = 0\n",
        "    for episode in range(train_episodes):\n",
        "        state = env.reset()\n",
        "        while True:\n",
        "            env.render(visualize)\n",
        "            action = np.random.randint(3, size=1)[0]\n",
        "            state, reward, done = env.step(action)\n",
        "            if env.current_step == env.end_step:\n",
        "                average_net_worth += env.net_worth\n",
        "                print(\"net_worth:\", episode, env.net_worth)\n",
        "                break\n",
        "\n",
        "    print(\"average {} episodes random net_worth: {}\".format(train_episodes, average_net_worth/train_episodes))\n",
        "\n",
        "def train_agent(env, visualize=False, train_episodes = 50, training_batch_size=500):\n",
        "    env.create_writer() # create TensorBoard writer\n",
        "    total_average = deque(maxlen=100) # save recent 100 episodes net worth\n",
        "    best_average = 0 # used to track best average net worth\n",
        "    for episode in range(train_episodes):\n",
        "        state = env.reset(env_steps_size = training_batch_size)\n",
        "\n",
        "        states, actions, rewards, predictions, dones, next_states = [], [], [], [], [], []\n",
        "        for t in range(training_batch_size):\n",
        "            env.render(visualize)\n",
        "            action, prediction = env.act(state)\n",
        "            next_state, reward, done = env.step(action)\n",
        "            states.append(np.expand_dims(state, axis=0))\n",
        "            next_states.append(np.expand_dims(next_state, axis=0))\n",
        "            action_onehot = np.zeros(3)\n",
        "            action_onehot[action] = 1\n",
        "            actions.append(action_onehot)\n",
        "            rewards.append(reward)\n",
        "            dones.append(done)\n",
        "            predictions.append(prediction)\n",
        "            state = next_state\n",
        "            \n",
        "        env.replay(states, actions, rewards, predictions, dones, next_states)\n",
        "        total_average.append(env.net_worth)\n",
        "        average = np.average(total_average)\n",
        "        \n",
        "        env.writer.add_scalar('Data/average net_worth', average, episode)\n",
        "        env.writer.add_scalar('Data/episode_orders', env.episode_orders, episode)\n",
        "        \n",
        "        print(\"net worth {} {:.2f} {:.2f} {}\".format(episode, env.net_worth, average, env.episode_orders))\n",
        "        if episode > len(total_average):\n",
        "            if best_average < average:\n",
        "                best_average = average\n",
        "                print(\"Saving model\")\n",
        "                env.save()\n",
        "\n",
        "def test_agent(env, visualize=True, test_episodes=10):\n",
        "    env.load() # load the model\n",
        "    average_net_worth = 0\n",
        "    for episode in range(test_episodes):\n",
        "        state = env.reset()\n",
        "        while True:\n",
        "            env.render(visualize)\n",
        "            action, prediction = env.act(state)\n",
        "            state, reward, done = env.step(action)\n",
        "            if env.current_step == env.end_step:\n",
        "                average_net_worth += env.net_worth\n",
        "                print(\"net_worth:\", episode, env.net_worth, env.episode_orders)\n",
        "                break\n",
        "            \n",
        "    print(\"average {} episodes agent net_worth: {}\".format(test_episodes, average_net_worth/test_episodes))"
      ],
      "metadata": {
        "id": "5ofG9QYeiwkM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('./pricedata.csv')\n",
        "df = df.sort_values('Date')\n",
        "\n",
        "lookback_window_size = 50\n",
        "train_df = df[:-720-lookback_window_size]\n",
        "test_df = df[-720-lookback_window_size:] # 30 days\n",
        "\n",
        "train_env = CustomEnv(train_df, lookback_window_size=lookback_window_size)\n",
        "test_env = CustomEnv(test_df, lookback_window_size=lookback_window_size)\n",
        "\n",
        "#train_agent(train_env, visualize=False, train_episodes=20000, training_batch_size=500)\n",
        "test_agent(test_env, visualize=True, test_episodes=1000)\n",
        "Random_games(test_env, visualize=False, train_episodes = 1000)"
      ],
      "metadata": {
        "id": "m_jtogSVk3jD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}