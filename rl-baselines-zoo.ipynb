{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "rl-baselines-zoo.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/decoderkurt/HUF_RL_2022/blob/main/rl-baselines-zoo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJy9QoDC7XA7"
      },
      "source": [
        "# RL Baselines3 Zoo: Training in Colab\n",
        "\n",
        "\n",
        "\n",
        "Github Repo: [https://github.com/DLR-RM/rl-baselines3-zoo](https://github.com/DLR-RM/rl-baselines3-zoo)\n",
        "\n",
        "Stable-Baselines3 Repo: [https://github.com/DLR-RM/rl-baselines3-zoo](https://github.com/DLR-RM/stable-baselines3)\n",
        "\n",
        "\n",
        "# Install Dependencies\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXVDDlTn02M9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb20603e-49de-4b73-ec74-08714e4c74c2"
      },
      "source": [
        "!apt-get install swig cmake ffmpeg freeglut3-dev xvfb"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "freeglut3-dev is already the newest version (2.8.1-3).\n",
            "freeglut3-dev set to manually installed.\n",
            "cmake is already the newest version (3.10.2-1ubuntu2.18.04.2).\n",
            "ffmpeg is already the newest version (7:3.4.8-0ubuntu0.2).\n",
            "The following additional packages will be installed:\n",
            "  swig3.0\n",
            "Suggested packages:\n",
            "  swig-doc swig-examples swig3.0-examples swig3.0-doc\n",
            "The following NEW packages will be installed:\n",
            "  swig swig3.0 xvfb\n",
            "0 upgraded, 3 newly installed, 0 to remove and 37 not upgraded.\n",
            "Need to get 1,884 kB of archives.\n",
            "After this operation, 8,094 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 swig3.0 amd64 3.0.12-1 [1,094 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 swig amd64 3.0.12-1 [6,460 B]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 xvfb amd64 2:1.19.6-1ubuntu4.10 [784 kB]\n",
            "Fetched 1,884 kB in 1s (1,955 kB/s)\n",
            "Selecting previously unselected package swig3.0.\n",
            "(Reading database ... 155229 files and directories currently installed.)\n",
            "Preparing to unpack .../swig3.0_3.0.12-1_amd64.deb ...\n",
            "Unpacking swig3.0 (3.0.12-1) ...\n",
            "Selecting previously unselected package swig.\n",
            "Preparing to unpack .../swig_3.0.12-1_amd64.deb ...\n",
            "Unpacking swig (3.0.12-1) ...\n",
            "Selecting previously unselected package xvfb.\n",
            "Preparing to unpack .../xvfb_2%3a1.19.6-1ubuntu4.10_amd64.deb ...\n",
            "Unpacking xvfb (2:1.19.6-1ubuntu4.10) ...\n",
            "Setting up swig3.0 (3.0.12-1) ...\n",
            "Setting up xvfb (2:1.19.6-1ubuntu4.10) ...\n",
            "Setting up swig (3.0.12-1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDjF3qRg7oGH"
      },
      "source": [
        "## Clone RL Baselines3 Zoo Repo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SCjGikdT1DFy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a510dd5-dd5a-4483-addc-940edfbdc77d"
      },
      "source": [
        "!git clone --recursive https://github.com/DLR-RM/rl-baselines3-zoo"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'rl-baselines3-zoo'...\n",
            "remote: Enumerating objects: 3578, done.\u001b[K\n",
            "remote: Counting objects: 100% (290/290), done.\u001b[K\n",
            "remote: Compressing objects: 100% (180/180), done.\u001b[K\n",
            "remote: Total 3578 (delta 145), reused 203 (delta 75), pack-reused 3288\u001b[K\n",
            "Receiving objects: 100% (3578/3578), 2.54 MiB | 9.62 MiB/s, done.\n",
            "Resolving deltas: 100% (2286/2286), done.\n",
            "Submodule 'rl-trained-agents' (https://github.com/DLR-RM/rl-trained-agents) registered for path 'rl-trained-agents'\n",
            "Cloning into '/content/rl-baselines3-zoo/rl-trained-agents'...\n",
            "remote: Enumerating objects: 2071, done.        \n",
            "remote: Counting objects: 100% (40/40), done.        \n",
            "remote: Compressing objects: 100% (21/21), done.        \n",
            "warning: Clone succeeded, but checkout failed.\n",
            "You can inspect what was checked out with 'git status'\n",
            "and retry the checkout with 'git checkout -f HEAD'\n",
            "\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "REMQlh-ezyVt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb541803-4932-4474-ecfc-00d846d8931a"
      },
      "source": [
        "%cd /content/rl-baselines3-zoo/"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/rl-baselines3-zoo\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tmD_QTBqTMb"
      },
      "source": [
        "### Install pip dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWIDzgJTqShY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ece1a91-ae1e-4a70-9127-3b5881d7e474"
      },
      "source": [
        "!pip install -r requirements.txt"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gym<0.20,>=0.17 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 1)) (0.17.3)\n",
            "Requirement already satisfied: stable-baselines3[docs,extra,tests]>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 2)) (1.4.0)\n",
            "Requirement already satisfied: sb3-contrib>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 3)) (1.4.0)\n",
            "Requirement already satisfied: box2d-py==2.3.8 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 4)) (2.3.8)\n",
            "Requirement already satisfied: pybullet in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 5)) (3.2.1)\n",
            "Requirement already satisfied: gym-minigrid in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 6)) (1.0.2)\n",
            "Requirement already satisfied: scikit-optimize in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 7)) (0.9.0)\n",
            "Requirement already satisfied: optuna in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 8)) (2.10.0)\n",
            "Requirement already satisfied: pytablewriter in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 9)) (0.64.1)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 10)) (0.11.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 11)) (6.0)\n",
            "Requirement already satisfied: cloudpickle>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 12)) (1.6.0)\n",
            "Requirement already satisfied: atari-py==0.2.6 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 14)) (0.2.6)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 15)) (4.4.1)\n",
            "Requirement already satisfied: panda-gym==1.1.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 16)) (1.1.1)\n",
            "Requirement already satisfied: rliable>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 17)) (1.0.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from atari-py==0.2.6->-r requirements.txt (line 14)) (1.19.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from atari-py==0.2.6->-r requirements.txt (line 14)) (1.15.0)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym<0.20,>=0.17->-r requirements.txt (line 1)) (1.5.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym<0.20,>=0.17->-r requirements.txt (line 1)) (1.7.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[docs,extra,tests]>=1.4.0->-r requirements.txt (line 2)) (3.2.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[docs,extra,tests]>=1.4.0->-r requirements.txt (line 2)) (1.1.5)\n",
            "Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[docs,extra,tests]>=1.4.0->-r requirements.txt (line 2)) (1.10.0+cu111)\n",
            "Requirement already satisfied: arch>=4.19 in /usr/local/lib/python3.7/dist-packages (from rliable>=1.0.5->-r requirements.txt (line 17)) (5.1.0)\n",
            "Requirement already satisfied: absl-py>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from rliable>=1.0.5->-r requirements.txt (line 17)) (0.12.0)\n",
            "Requirement already satisfied: statsmodels>=0.11 in /usr/local/lib/python3.7/dist-packages (from arch>=4.19->rliable>=1.0.5->-r requirements.txt (line 17)) (0.13.1)\n",
            "Requirement already satisfied: property-cached>=1.6.4 in /usr/local/lib/python3.7/dist-packages (from arch>=4.19->rliable>=1.0.5->-r requirements.txt (line 17)) (1.6.4)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines3[docs,extra,tests]>=1.4.0->-r requirements.txt (line 2)) (1.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines3[docs,extra,tests]>=1.4.0->-r requirements.txt (line 2)) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines3[docs,extra,tests]>=1.4.0->-r requirements.txt (line 2)) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines3[docs,extra,tests]>=1.4.0->-r requirements.txt (line 2)) (3.0.6)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->stable-baselines3[docs,extra,tests]>=1.4.0->-r requirements.txt (line 2)) (2018.9)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym<0.20,>=0.17->-r requirements.txt (line 1)) (0.16.0)\n",
            "Requirement already satisfied: sphinxcontrib.spelling in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[docs,extra,tests]>=1.4.0->-r requirements.txt (line 2)) (7.3.2)\n",
            "Requirement already satisfied: sphinx in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[docs,extra,tests]>=1.4.0->-r requirements.txt (line 2)) (4.3.2)\n",
            "Requirement already satisfied: sphinx-autodoc-typehints in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[docs,extra,tests]>=1.4.0->-r requirements.txt (line 2)) (1.15.3)\n",
            "Requirement already satisfied: sphinx-autobuild in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[docs,extra,tests]>=1.4.0->-r requirements.txt (line 2)) (2021.3.14)\n",
            "Requirement already satisfied: sphinx-rtd-theme in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[docs,extra,tests]>=1.4.0->-r requirements.txt (line 2)) (1.0.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[docs,extra,tests]>=1.4.0->-r requirements.txt (line 2)) (4.1.2.30)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[docs,extra,tests]>=1.4.0->-r requirements.txt (line 2)) (7.1.2)\n",
            "Requirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[docs,extra,tests]>=1.4.0->-r requirements.txt (line 2)) (2.7.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[docs,extra,tests]>=1.4.0->-r requirements.txt (line 2)) (5.4.8)\n",
            "Requirement already satisfied: black in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[docs,extra,tests]>=1.4.0->-r requirements.txt (line 2)) (21.12b0)\n",
            "Requirement already satisfied: pytype in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[docs,extra,tests]>=1.4.0->-r requirements.txt (line 2)) (2022.1.13)\n",
            "Requirement already satisfied: pytest-xdist in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[docs,extra,tests]>=1.4.0->-r requirements.txt (line 2)) (1.27.0)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[docs,extra,tests]>=1.4.0->-r requirements.txt (line 2)) (4.3.1)\n",
            "Requirement already satisfied: flake8>=3.8 in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[docs,extra,tests]>=1.4.0->-r requirements.txt (line 2)) (4.0.1)\n",
            "Requirement already satisfied: pytest-cov in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[docs,extra,tests]>=1.4.0->-r requirements.txt (line 2)) (2.9.0)\n",
            "Requirement already satisfied: pytest-env in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[docs,extra,tests]>=1.4.0->-r requirements.txt (line 2)) (0.6.2)\n",
            "Requirement already satisfied: isort>=5.0 in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[docs,extra,tests]>=1.4.0->-r requirements.txt (line 2)) (5.10.1)\n",
            "Requirement already satisfied: flake8-bugbear in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[docs,extra,tests]>=1.4.0->-r requirements.txt (line 2)) (22.1.11)\n",
            "Requirement already satisfied: pycodestyle<2.9.0,>=2.8.0 in /usr/local/lib/python3.7/dist-packages (from flake8>=3.8->stable-baselines3[docs,extra,tests]>=1.4.0->-r requirements.txt (line 2)) (2.8.0)\n",
            "Requirement already satisfied: pyflakes<2.5.0,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from flake8>=3.8->stable-baselines3[docs,extra,tests]>=1.4.0->-r requirements.txt (line 2)) (2.4.0)\n",
            "Requirement already satisfied: importlib-metadata<4.3 in /usr/local/lib/python3.7/dist-packages (from flake8>=3.8->stable-baselines3[docs,extra,tests]>=1.4.0->-r requirements.txt (line 2)) (4.2.0)\n",
            "Requirement already satisfied: mccabe<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from flake8>=3.8->stable-baselines3[docs,extra,tests]>=1.4.0->-r requirements.txt (line 2)) (0.6.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata<4.3->flake8>=3.8->stable-baselines3[docs,extra,tests]>=1.4.0->-r requirements.txt (line 2)) (3.7.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata<4.3->flake8>=3.8->stable-baselines3[docs,extra,tests]>=1.4.0->-r requirements.txt (line 2)) (3.10.0.2)\n",
            "Requirement already satisfied: patsy>=0.5.2 in /usr/local/lib/python3.7/dist-packages (from statsmodels>=0.11->arch>=4.19->rliable>=1.0.5->-r requirements.txt (line 17)) (0.5.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[docs,extra,tests]>=1.4.0->-r requirements.txt (line 2)) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[docs,extra,tests]>=1.4.0->-r requirements.txt (line 2)) (1.8.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[docs,extra,tests]>=1.4.0->-r requirements.txt (line 2)) (0.6.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[docs,extra,tests]>=1.4.0->-r requirements.txt (line 2)) (1.0.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[docs,extra,tests]>=1.4.0->-r requirements.txt (line 2)) (1.35.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[docs,extra,tests]>=1.4.0->-r requirements.txt (line 2)) (3.3.4)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[docs,extra,tests]>=1.4.0->-r requirements.txt (line 2)) (0.37.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[docs,extra,tests]>=1.4.0->-r requirements.txt (line 2)) (0.4.6)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[docs,extra,tests]>=1.4.0->-r requirements.txt (line 2)) (57.4.0)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[docs,extra,tests]>=1.4.0->-r requirements.txt (line 2)) (3.17.3)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[docs,extra,tests]>=1.4.0->-r requirements.txt (line 2)) (1.43.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->stable-baselines3[docs,extra,tests]>=1.4.0->-r requirements.txt (line 2)) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->stable-baselines3[docs,extra,tests]>=1.4.0->-r requirements.txt (line 2)) (4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->stable-baselines3[docs,extra,tests]>=1.4.0->-r requirements.txt (line 2)) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->stable-baselines3[docs,extra,tests]>=1.4.0->-r requirements.txt (line 2)) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->stable-baselines3[docs,extra,tests]>=1.4.0->-r requirements.txt (line 2)) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->stable-baselines3[docs,extra,tests]>=1.4.0->-r requirements.txt (line 2)) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->stable-baselines3[docs,extra,tests]>=1.4.0->-r requirements.txt (line 2)) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->stable-baselines3[docs,extra,tests]>=1.4.0->-r requirements.txt (line 2)) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->stable-baselines3[docs,extra,tests]>=1.4.0->-r requirements.txt (line 2)) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->stable-baselines3[docs,extra,tests]>=1.4.0->-r requirements.txt (line 2)) (3.1.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize->-r requirements.txt (line 7)) (1.1.0)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize->-r requirements.txt (line 7)) (1.0.2)\n",
            "Requirement already satisfied: pyaml>=16.9 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize->-r requirements.txt (line 7)) (21.10.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->scikit-optimize->-r requirements.txt (line 7)) (3.0.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from optuna->-r requirements.txt (line 8)) (1.4.29)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.7/dist-packages (from optuna->-r requirements.txt (line 8)) (6.6.0)\n",
            "Requirement already satisfied: cmaes>=0.8.2 in /usr/local/lib/python3.7/dist-packages (from optuna->-r requirements.txt (line 8)) (0.8.2)\n",
            "Requirement already satisfied: cliff in /usr/local/lib/python3.7/dist-packages (from optuna->-r requirements.txt (line 8)) (3.10.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from optuna->-r requirements.txt (line 8)) (4.62.3)\n",
            "Requirement already satisfied: alembic in /usr/local/lib/python3.7/dist-packages (from optuna->-r requirements.txt (line 8)) (1.7.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from optuna->-r requirements.txt (line 8)) (21.3)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=1.1.0->optuna->-r requirements.txt (line 8)) (1.1.2)\n",
            "Requirement already satisfied: pathvalidate<3,>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from pytablewriter->-r requirements.txt (line 9)) (2.5.0)\n",
            "Requirement already satisfied: typepy[datetime]<2,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from pytablewriter->-r requirements.txt (line 9)) (1.3.0)\n",
            "Requirement already satisfied: tabledata<2,>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from pytablewriter->-r requirements.txt (line 9)) (1.3.0)\n",
            "Requirement already satisfied: tcolorpy<1,>=0.0.5 in /usr/local/lib/python3.7/dist-packages (from pytablewriter->-r requirements.txt (line 9)) (0.1.1)\n",
            "Requirement already satisfied: mbstrdecoder<2,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from pytablewriter->-r requirements.txt (line 9)) (1.1.0)\n",
            "Requirement already satisfied: DataProperty<2,>=0.54.2 in /usr/local/lib/python3.7/dist-packages (from pytablewriter->-r requirements.txt (line 9)) (0.54.2)\n",
            "Requirement already satisfied: retrying>=1.3.3 in /usr/local/lib/python3.7/dist-packages (from plotly->-r requirements.txt (line 15)) (1.3.3)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from alembic->optuna->-r requirements.txt (line 8)) (5.4.0)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.7/dist-packages (from alembic->optuna->-r requirements.txt (line 8)) (1.1.6)\n",
            "Requirement already satisfied: tomli<2.0.0,>=0.2.6 in /usr/local/lib/python3.7/dist-packages (from black->stable-baselines3[docs,extra,tests]>=1.4.0->-r requirements.txt (line 2)) (1.2.3)\n",
            "Requirement already satisfied: typed-ast>=1.4.2 in /usr/local/lib/python3.7/dist-packages (from black->stable-baselines3[docs,extra,tests]>=1.4.0->-r requirements.txt (line 2)) (1.5.1)\n",
            "Requirement already satisfied: platformdirs>=2 in /usr/local/lib/python3.7/dist-packages (from black->stable-baselines3[docs,extra,tests]>=1.4.0->-r requirements.txt (line 2)) (2.4.1)\n",
            "Requirement already satisfied: pathspec<1,>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from black->stable-baselines3[docs,extra,tests]>=1.4.0->-r requirements.txt (line 2)) (0.9.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.4.3 in /usr/local/lib/python3.7/dist-packages (from black->stable-baselines3[docs,extra,tests]>=1.4.0->-r requirements.txt (line 2)) (0.4.3)\n",
            "Requirement already satisfied: click>=7.1.2 in /usr/local/lib/python3.7/dist-packages (from black->stable-baselines3[docs,extra,tests]>=1.4.0->-r requirements.txt (line 2)) (7.1.2)\n",
            "Requirement already satisfied: pbr!=2.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna->-r requirements.txt (line 8)) (5.8.0)\n",
            "Requirement already satisfied: PrettyTable>=0.7.2 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna->-r requirements.txt (line 8)) (3.0.0)\n",
            "Requirement already satisfied: autopage>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna->-r requirements.txt (line 8)) (0.5.0)\n",
            "Requirement already satisfied: cmd2>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna->-r requirements.txt (line 8)) (2.3.3)\n",
            "Requirement already satisfied: stevedore>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna->-r requirements.txt (line 8)) (3.5.0)\n",
            "Requirement already satisfied: wcwidth>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna->-r requirements.txt (line 8)) (0.2.5)\n",
            "Requirement already satisfied: pyperclip>=1.6 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna->-r requirements.txt (line 8)) (1.8.2)\n",
            "Requirement already satisfied: attrs>=16.3.0 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna->-r requirements.txt (line 8)) (21.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from Mako->alembic->optuna->-r requirements.txt (line 8)) (2.0.1)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest->stable-baselines3[docs,extra,tests]>=1.4.0->-r requirements.txt (line 2)) (1.4.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest->stable-baselines3[docs,extra,tests]>=1.4.0->-r requirements.txt (line 2)) (1.11.0)\n",
            "Requirement already satisfied: pluggy>=0.7 in /usr/local/lib/python3.7/dist-packages (from pytest->stable-baselines3[docs,extra,tests]>=1.4.0->-r requirements.txt (line 2)) (0.7.1)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest->stable-baselines3[docs,extra,tests]>=1.4.0->-r requirements.txt (line 2)) (8.12.0)\n",
            "Requirement already satisfied: coverage>=4.4 in /usr/local/lib/python3.7/dist-packages (from pytest-cov->stable-baselines3[docs,extra,tests]>=1.4.0->-r requirements.txt (line 2)) (6.2)\n",
            "Requirement already satisfied: execnet>=1.1 in /usr/local/lib/python3.7/dist-packages (from pytest-xdist->stable-baselines3[docs,extra,tests]>=1.4.0->-r requirements.txt (line 2)) (1.9.0)\n",
            "Requirement already satisfied: pytest-forked in /usr/local/lib/python3.7/dist-packages (from pytest-xdist->stable-baselines3[docs,extra,tests]>=1.4.0->-r requirements.txt (line 2)) (1.4.0)\n",
            "Requirement already satisfied: importlab>=0.7 in /usr/local/lib/python3.7/dist-packages (from pytype->stable-baselines3[docs,extra,tests]>=1.4.0->-r requirements.txt (line 2)) (0.7)\n",
            "Requirement already satisfied: libcst in /usr/local/lib/python3.7/dist-packages (from pytype->stable-baselines3[docs,extra,tests]>=1.4.0->-r requirements.txt (line 2)) (0.4.0)\n",
            "Requirement already satisfied: ninja>=1.10.0.post2 in /usr/local/lib/python3.7/dist-packages (from pytype->stable-baselines3[docs,extra,tests]>=1.4.0->-r requirements.txt (line 2)) (1.10.2.3)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.7/dist-packages (from pytype->stable-baselines3[docs,extra,tests]>=1.4.0->-r requirements.txt (line 2)) (0.10.2)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from pytype->stable-baselines3[docs,extra,tests]>=1.4.0->-r requirements.txt (line 2)) (0.8.9)\n",
            "Requirement already satisfied: networkx>=2 in /usr/local/lib/python3.7/dist-packages (from importlab>=0.7->pytype->stable-baselines3[docs,extra,tests]>=1.4.0->-r requirements.txt (line 2)) (2.6.3)\n",
            "Requirement already satisfied: typing-inspect>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from libcst->pytype->stable-baselines3[docs,extra,tests]>=1.4.0->-r requirements.txt (line 2)) (0.7.1)\n",
            "Requirement already satisfied: sphinxcontrib-applehelp in /usr/local/lib/python3.7/dist-packages (from sphinx->stable-baselines3[docs,extra,tests]>=1.4.0->-r requirements.txt (line 2)) (1.0.2)\n",
            "Requirement already satisfied: sphinxcontrib-devhelp in /usr/local/lib/python3.7/dist-packages (from sphinx->stable-baselines3[docs,extra,tests]>=1.4.0->-r requirements.txt (line 2)) (1.0.2)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.5 in /usr/local/lib/python3.7/dist-packages (from sphinx->stable-baselines3[docs,extra,tests]>=1.4.0->-r requirements.txt (line 2)) (1.1.5)\n",
            "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.7/dist-packages (from sphinx->stable-baselines3[docs,extra,tests]>=1.4.0->-r requirements.txt (line 2)) (2.6.1)\n",
            "Requirement already satisfied: sphinxcontrib-qthelp in /usr/local/lib/python3.7/dist-packages (from sphinx->stable-baselines3[docs,extra,tests]>=1.4.0->-r requirements.txt (line 2)) (1.0.3)\n",
            "Requirement already satisfied: docutils<0.18,>=0.14 in /usr/local/lib/python3.7/dist-packages (from sphinx->stable-baselines3[docs,extra,tests]>=1.4.0->-r requirements.txt (line 2)) (0.17.1)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.7/dist-packages (from sphinx->stable-baselines3[docs,extra,tests]>=1.4.0->-r requirements.txt (line 2)) (2.2.0)\n",
            "Requirement already satisfied: babel>=1.3 in /usr/local/lib/python3.7/dist-packages (from sphinx->stable-baselines3[docs,extra,tests]>=1.4.0->-r requirements.txt (line 2)) (2.9.1)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.7/dist-packages (from sphinx->stable-baselines3[docs,extra,tests]>=1.4.0->-r requirements.txt (line 2)) (1.3.0)\n",
            "Requirement already satisfied: sphinxcontrib-jsmath in /usr/local/lib/python3.7/dist-packages (from sphinx->stable-baselines3[docs,extra,tests]>=1.4.0->-r requirements.txt (line 2)) (1.0.1)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.7/dist-packages (from sphinx->stable-baselines3[docs,extra,tests]>=1.4.0->-r requirements.txt (line 2)) (0.7.12)\n",
            "Requirement already satisfied: Jinja2>=2.3 in /usr/local/lib/python3.7/dist-packages (from sphinx->stable-baselines3[docs,extra,tests]>=1.4.0->-r requirements.txt (line 2)) (2.11.3)\n",
            "Requirement already satisfied: sphinxcontrib-htmlhelp>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from sphinx->stable-baselines3[docs,extra,tests]>=1.4.0->-r requirements.txt (line 2)) (2.0.0)\n",
            "Requirement already satisfied: livereload in /usr/local/lib/python3.7/dist-packages (from sphinx-autobuild->stable-baselines3[docs,extra,tests]>=1.4.0->-r requirements.txt (line 2)) (2.6.3)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.7/dist-packages (from sphinx-autobuild->stable-baselines3[docs,extra,tests]>=1.4.0->-r requirements.txt (line 2)) (0.4.4)\n",
            "Requirement already satisfied: tornado in /usr/local/lib/python3.7/dist-packages (from livereload->sphinx-autobuild->stable-baselines3[docs,extra,tests]>=1.4.0->-r requirements.txt (line 2)) (5.1.1)\n",
            "Requirement already satisfied: PyEnchant>=3.1.1 in /usr/local/lib/python3.7/dist-packages (from sphinxcontrib.spelling->stable-baselines3[docs,extra,tests]>=1.4.0->-r requirements.txt (line 2)) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gJ-pAbF7zRZ"
      },
      "source": [
        "## Train an RL Agent\n",
        "\n",
        "\n",
        "The train agent can be found in the `logs/` folder.\n",
        "\n",
        "Here we will train A2C on CartPole-v1 environment for 100 000 steps. \n",
        "\n",
        "\n",
        "To train it on Pong (Atari), you just have to pass `--env PongNoFrameskip-v4`\n",
        "\n",
        "Note: You need to update `hyperparams/algo.yml` to support new environments. You can access it in the side panel of Google Colab. (see https://stackoverflow.com/questions/46986398/import-data-into-google-colaboratory)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9bIR_N7R11XI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82b745c1-518d-48fa-b251-2068ceee8c55"
      },
      "source": [
        "!python train.py --algo a2c --env CartPole-v1 --n-timesteps 100000"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "========== CartPole-v1 ==========\n",
            "Seed: 417449917\n",
            "Default hyperparameters for environment (ones being tuned will be overridden):\n",
            "OrderedDict([('ent_coef', 0.0),\n",
            "             ('n_envs', 8),\n",
            "             ('n_timesteps', 500000.0),\n",
            "             ('policy', 'MlpPolicy')])\n",
            "Using 8 environments\n",
            "Overwriting n_timesteps with n=100000\n",
            "Creating test environment\n",
            "Using cpu device\n",
            "Log path: logs/a2c/CartPole-v1_1\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 19.4     |\n",
            "|    ep_rew_mean        | 19.4     |\n",
            "| time/                 |          |\n",
            "|    fps                | 4413     |\n",
            "|    iterations         | 100      |\n",
            "|    time_elapsed       | 0        |\n",
            "|    total_timesteps    | 4000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.591   |\n",
            "|    explained_variance | 0.0579   |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 99       |\n",
            "|    policy_loss        | -0.54    |\n",
            "|    value_loss         | 29.3     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 43       |\n",
            "|    ep_rew_mean        | 43       |\n",
            "| time/                 |          |\n",
            "|    fps                | 4731     |\n",
            "|    iterations         | 200      |\n",
            "|    time_elapsed       | 1        |\n",
            "|    total_timesteps    | 8000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.539   |\n",
            "|    explained_variance | 0.105    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 199      |\n",
            "|    policy_loss        | 1.42     |\n",
            "|    value_loss         | 7.01     |\n",
            "------------------------------------\n",
            "Eval num_timesteps=10000, episode_reward=330.60 +/- 30.55\n",
            "Episode length: 330.60 +/- 30.55\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 331      |\n",
            "|    mean_reward        | 331      |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 10000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.553   |\n",
            "|    explained_variance | -0.0208  |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 249      |\n",
            "|    policy_loss        | 0.136    |\n",
            "|    value_loss         | 48.1     |\n",
            "------------------------------------\n",
            "New best mean reward!\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 68.9     |\n",
            "|    ep_rew_mean        | 68.9     |\n",
            "| time/                 |          |\n",
            "|    fps                | 3513     |\n",
            "|    iterations         | 300      |\n",
            "|    time_elapsed       | 3        |\n",
            "|    total_timesteps    | 12000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.506   |\n",
            "|    explained_variance | 0.0074   |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 299      |\n",
            "|    policy_loss        | 1.43     |\n",
            "|    value_loss         | 5.97     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 101      |\n",
            "|    ep_rew_mean        | 101      |\n",
            "| time/                 |          |\n",
            "|    fps                | 3774     |\n",
            "|    iterations         | 400      |\n",
            "|    time_elapsed       | 4        |\n",
            "|    total_timesteps    | 16000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.565   |\n",
            "|    explained_variance | 0.0162   |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 399      |\n",
            "|    policy_loss        | 1.02     |\n",
            "|    value_loss         | 5.26     |\n",
            "------------------------------------\n",
            "Eval num_timesteps=20000, episode_reward=500.00 +/- 0.00\n",
            "Episode length: 500.00 +/- 0.00\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 500      |\n",
            "|    mean_reward        | 500      |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 20000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.539   |\n",
            "|    explained_variance | 0.0634   |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 499      |\n",
            "|    policy_loss        | 0.847    |\n",
            "|    value_loss         | 4.53     |\n",
            "------------------------------------\n",
            "New best mean reward!\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 136      |\n",
            "|    ep_rew_mean     | 136      |\n",
            "| time/              |          |\n",
            "|    fps             | 3128     |\n",
            "|    iterations      | 500      |\n",
            "|    time_elapsed    | 6        |\n",
            "|    total_timesteps | 20000    |\n",
            "---------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 185      |\n",
            "|    ep_rew_mean        | 185      |\n",
            "| time/                 |          |\n",
            "|    fps                | 3340     |\n",
            "|    iterations         | 600      |\n",
            "|    time_elapsed       | 7        |\n",
            "|    total_timesteps    | 24000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.524   |\n",
            "|    explained_variance | 0.034    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 599      |\n",
            "|    policy_loss        | 0.851    |\n",
            "|    value_loss         | 3.96     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 213      |\n",
            "|    ep_rew_mean        | 213      |\n",
            "| time/                 |          |\n",
            "|    fps                | 3504     |\n",
            "|    iterations         | 700      |\n",
            "|    time_elapsed       | 7        |\n",
            "|    total_timesteps    | 28000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.485   |\n",
            "|    explained_variance | 0.0485   |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 699      |\n",
            "|    policy_loss        | 0.85     |\n",
            "|    value_loss         | 3.4      |\n",
            "------------------------------------\n",
            "Eval num_timesteps=30000, episode_reward=500.00 +/- 0.00\n",
            "Episode length: 500.00 +/- 0.00\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 500      |\n",
            "|    mean_reward        | 500      |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 30000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.527   |\n",
            "|    explained_variance | 0.000158 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 749      |\n",
            "|    policy_loss        | 0.568    |\n",
            "|    value_loss         | 3.2      |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| rollout/              |           |\n",
            "|    ep_len_mean        | 240       |\n",
            "|    ep_rew_mean        | 240       |\n",
            "| time/                 |           |\n",
            "|    fps                | 3147      |\n",
            "|    iterations         | 800       |\n",
            "|    time_elapsed       | 10        |\n",
            "|    total_timesteps    | 32000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -0.492    |\n",
            "|    explained_variance | -0.000764 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 799       |\n",
            "|    policy_loss        | 0.837     |\n",
            "|    value_loss         | 2.93      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 289      |\n",
            "|    ep_rew_mean        | 289      |\n",
            "| time/                 |          |\n",
            "|    fps                | 3282     |\n",
            "|    iterations         | 900      |\n",
            "|    time_elapsed       | 10       |\n",
            "|    total_timesteps    | 36000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.533   |\n",
            "|    explained_variance | 0.0268   |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 899      |\n",
            "|    policy_loss        | -1.64    |\n",
            "|    value_loss         | 183      |\n",
            "------------------------------------\n",
            "Eval num_timesteps=40000, episode_reward=353.80 +/- 79.06\n",
            "Episode length: 353.80 +/- 79.06\n",
            "------------------------------------\n",
            "| eval/                 |          |\n",
            "|    mean_ep_length     | 354      |\n",
            "|    mean_reward        | 354      |\n",
            "| time/                 |          |\n",
            "|    total_timesteps    | 40000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.536   |\n",
            "|    explained_variance | 1.41e-05 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 999      |\n",
            "|    policy_loss        | 0.681    |\n",
            "|    value_loss         | 2.14     |\n",
            "------------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 313      |\n",
            "|    ep_rew_mean     | 313      |\n",
            "| time/              |          |\n",
            "|    fps             | 3140     |\n",
            "|    iterations      | 1000     |\n",
            "|    time_elapsed    | 12       |\n",
            "|    total_timesteps | 40000    |\n",
            "---------------------------------\n",
            "-------------------------------------\n",
            "| rollout/              |           |\n",
            "|    ep_len_mean        | 333       |\n",
            "|    ep_rew_mean        | 333       |\n",
            "| time/                 |           |\n",
            "|    fps                | 3247      |\n",
            "|    iterations         | 1100      |\n",
            "|    time_elapsed       | 13        |\n",
            "|    total_timesteps    | 44000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -0.514    |\n",
            "|    explained_variance | -0.000169 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1099      |\n",
            "|    policy_loss        | 0.619     |\n",
            "|    value_loss         | 1.81      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| rollout/              |           |\n",
            "|    ep_len_mean        | 341       |\n",
            "|    ep_rew_mean        | 341       |\n",
            "| time/                 |           |\n",
            "|    fps                | 3339      |\n",
            "|    iterations         | 1200      |\n",
            "|    time_elapsed       | 14        |\n",
            "|    total_timesteps    | 48000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -0.545    |\n",
            "|    explained_variance | -0.000122 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1199      |\n",
            "|    policy_loss        | 0.6       |\n",
            "|    value_loss         | 1.52      |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=50000, episode_reward=139.80 +/- 13.24\n",
            "Episode length: 139.80 +/- 13.24\n",
            "-------------------------------------\n",
            "| eval/                 |           |\n",
            "|    mean_ep_length     | 140       |\n",
            "|    mean_reward        | 140       |\n",
            "| time/                 |           |\n",
            "|    total_timesteps    | 50000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -0.508    |\n",
            "|    explained_variance | -0.000109 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1249      |\n",
            "|    policy_loss        | -5.83     |\n",
            "|    value_loss         | 833       |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| rollout/              |           |\n",
            "|    ep_len_mean        | 272       |\n",
            "|    ep_rew_mean        | 272       |\n",
            "| time/                 |           |\n",
            "|    fps                | 3342      |\n",
            "|    iterations         | 1300      |\n",
            "|    time_elapsed       | 15        |\n",
            "|    total_timesteps    | 52000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -0.458    |\n",
            "|    explained_variance | -2.81e-05 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1299      |\n",
            "|    policy_loss        | 0.603     |\n",
            "|    value_loss         | 1.3       |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| rollout/              |           |\n",
            "|    ep_len_mean        | 214       |\n",
            "|    ep_rew_mean        | 214       |\n",
            "| time/                 |           |\n",
            "|    fps                | 3430      |\n",
            "|    iterations         | 1400      |\n",
            "|    time_elapsed       | 16        |\n",
            "|    total_timesteps    | 56000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -0.516    |\n",
            "|    explained_variance | -3.45e-05 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1399      |\n",
            "|    policy_loss        | -3.59     |\n",
            "|    value_loss         | 426       |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=60000, episode_reward=143.20 +/- 14.27\n",
            "Episode length: 143.20 +/- 14.27\n",
            "-------------------------------------\n",
            "| eval/                 |           |\n",
            "|    mean_ep_length     | 143       |\n",
            "|    mean_reward        | 143       |\n",
            "| time/                 |           |\n",
            "|    total_timesteps    | 60000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -0.469    |\n",
            "|    explained_variance | -2.49e-05 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1499      |\n",
            "|    policy_loss        | -1.48     |\n",
            "|    value_loss         | 349       |\n",
            "-------------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 176      |\n",
            "|    ep_rew_mean     | 176      |\n",
            "| time/              |          |\n",
            "|    fps             | 3425     |\n",
            "|    iterations      | 1500     |\n",
            "|    time_elapsed    | 17       |\n",
            "|    total_timesteps | 60000    |\n",
            "---------------------------------\n",
            "-------------------------------------\n",
            "| rollout/              |           |\n",
            "|    ep_len_mean        | 148       |\n",
            "|    ep_rew_mean        | 148       |\n",
            "| time/                 |           |\n",
            "|    fps                | 3499      |\n",
            "|    iterations         | 1600      |\n",
            "|    time_elapsed       | 18        |\n",
            "|    total_timesteps    | 64000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -0.521    |\n",
            "|    explained_variance | -0.000117 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1599      |\n",
            "|    policy_loss        | 0.417     |\n",
            "|    value_loss         | 0.805     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| rollout/              |           |\n",
            "|    ep_len_mean        | 150       |\n",
            "|    ep_rew_mean        | 150       |\n",
            "| time/                 |           |\n",
            "|    fps                | 3564      |\n",
            "|    iterations         | 1700      |\n",
            "|    time_elapsed       | 19        |\n",
            "|    total_timesteps    | 68000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -0.514    |\n",
            "|    explained_variance | -3.08e-05 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1699      |\n",
            "|    policy_loss        | 0.345     |\n",
            "|    value_loss         | 0.653     |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=70000, episode_reward=317.00 +/- 24.70\n",
            "Episode length: 317.00 +/- 24.70\n",
            "-------------------------------------\n",
            "| eval/                 |           |\n",
            "|    mean_ep_length     | 317       |\n",
            "|    mean_reward        | 317       |\n",
            "| time/                 |           |\n",
            "|    total_timesteps    | 70000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -0.549    |\n",
            "|    explained_variance | -4.32e-05 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1749      |\n",
            "|    policy_loss        | 0.417     |\n",
            "|    value_loss         | 0.576     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| rollout/              |           |\n",
            "|    ep_len_mean        | 163       |\n",
            "|    ep_rew_mean        | 163       |\n",
            "| time/                 |           |\n",
            "|    fps                | 3469      |\n",
            "|    iterations         | 1800      |\n",
            "|    time_elapsed       | 20        |\n",
            "|    total_timesteps    | 72000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -0.474    |\n",
            "|    explained_variance | -8.23e-05 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1799      |\n",
            "|    policy_loss        | 0.337     |\n",
            "|    value_loss         | 0.498     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| rollout/              |           |\n",
            "|    ep_len_mean        | 172       |\n",
            "|    ep_rew_mean        | 172       |\n",
            "| time/                 |           |\n",
            "|    fps                | 3527      |\n",
            "|    iterations         | 1900      |\n",
            "|    time_elapsed       | 21        |\n",
            "|    total_timesteps    | 76000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -0.488    |\n",
            "|    explained_variance | -3.05e-05 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1899      |\n",
            "|    policy_loss        | 0.353     |\n",
            "|    value_loss         | 0.375     |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=80000, episode_reward=215.20 +/- 21.54\n",
            "Episode length: 215.20 +/- 21.54\n",
            "-------------------------------------\n",
            "| eval/                 |           |\n",
            "|    mean_ep_length     | 215       |\n",
            "|    mean_reward        | 215       |\n",
            "| time/                 |           |\n",
            "|    total_timesteps    | 80000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -0.492    |\n",
            "|    explained_variance | -3.81e-06 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1999      |\n",
            "|    policy_loss        | -4.94     |\n",
            "|    value_loss         | 341       |\n",
            "-------------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 178      |\n",
            "|    ep_rew_mean     | 178      |\n",
            "| time/              |          |\n",
            "|    fps             | 3486     |\n",
            "|    iterations      | 2000     |\n",
            "|    time_elapsed    | 22       |\n",
            "|    total_timesteps | 80000    |\n",
            "---------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 189      |\n",
            "|    ep_rew_mean        | 189      |\n",
            "| time/                 |          |\n",
            "|    fps                | 3541     |\n",
            "|    iterations         | 2100     |\n",
            "|    time_elapsed       | 23       |\n",
            "|    total_timesteps    | 84000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.502   |\n",
            "|    explained_variance | 1.23e-05 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 2099     |\n",
            "|    policy_loss        | 0.22     |\n",
            "|    value_loss         | 0.186    |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| rollout/              |           |\n",
            "|    ep_len_mean        | 196       |\n",
            "|    ep_rew_mean        | 196       |\n",
            "| time/                 |           |\n",
            "|    fps                | 3591      |\n",
            "|    iterations         | 2200      |\n",
            "|    time_elapsed       | 24        |\n",
            "|    total_timesteps    | 88000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -0.506    |\n",
            "|    explained_variance | -2.65e-05 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 2199      |\n",
            "|    policy_loss        | 0.15      |\n",
            "|    value_loss         | 0.111     |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=90000, episode_reward=175.20 +/- 12.92\n",
            "Episode length: 175.20 +/- 12.92\n",
            "-------------------------------------\n",
            "| eval/                 |           |\n",
            "|    mean_ep_length     | 175       |\n",
            "|    mean_reward        | 175       |\n",
            "| time/                 |           |\n",
            "|    total_timesteps    | 90000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -0.5      |\n",
            "|    explained_variance | -1.67e-06 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 2249      |\n",
            "|    policy_loss        | 0.148     |\n",
            "|    value_loss         | 0.0859    |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| rollout/              |           |\n",
            "|    ep_len_mean        | 185       |\n",
            "|    ep_rew_mean        | 185       |\n",
            "| time/                 |           |\n",
            "|    fps                | 3564      |\n",
            "|    iterations         | 2300      |\n",
            "|    time_elapsed       | 25        |\n",
            "|    total_timesteps    | 92000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -0.497    |\n",
            "|    explained_variance | -6.16e-05 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 2299      |\n",
            "|    policy_loss        | 0.0948    |\n",
            "|    value_loss         | 0.0607    |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| rollout/              |           |\n",
            "|    ep_len_mean        | 188       |\n",
            "|    ep_rew_mean        | 188       |\n",
            "| time/                 |           |\n",
            "|    fps                | 3609      |\n",
            "|    iterations         | 2400      |\n",
            "|    time_elapsed       | 26        |\n",
            "|    total_timesteps    | 96000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -0.579    |\n",
            "|    explained_variance | -4.77e-05 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 2399      |\n",
            "|    policy_loss        | 0.0748    |\n",
            "|    value_loss         | 0.0255    |\n",
            "-------------------------------------\n",
            "Eval num_timesteps=100000, episode_reward=173.80 +/- 12.54\n",
            "Episode length: 173.80 +/- 12.54\n",
            "-------------------------------------\n",
            "| eval/                 |           |\n",
            "|    mean_ep_length     | 174       |\n",
            "|    mean_reward        | 174       |\n",
            "| time/                 |           |\n",
            "|    total_timesteps    | 100000    |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -0.498    |\n",
            "|    explained_variance | -7.51e-05 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 2499      |\n",
            "|    policy_loss        | 0.0352    |\n",
            "|    value_loss         | 0.00567   |\n",
            "-------------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 185      |\n",
            "|    ep_rew_mean     | 185      |\n",
            "| time/              |          |\n",
            "|    fps             | 3588     |\n",
            "|    iterations      | 2500     |\n",
            "|    time_elapsed    | 27       |\n",
            "|    total_timesteps | 100000   |\n",
            "---------------------------------\n",
            "Saving to logs/a2c/CartPole-v1_1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-fHBq73665yD"
      },
      "source": [
        "#### Evaluate trained agent\n",
        "\n",
        "\n",
        "You can remove the `--folder logs/` to evaluate pretrained agent."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bw8YuEgU6bT3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05532c6e-856a-49c9-f581-abeb44837fca"
      },
      "source": [
        "!python enjoy.py --algo a2c --env CartPole-v1 --no-render --n-timesteps 5000 --folder logs/"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading latest experiment, id=1\n",
            "Loading logs/a2c/CartPole-v1_1/CartPole-v1.zip\n",
            "Episode Reward: 166.00\n",
            "Episode Length 166\n",
            "Episode Reward: 156.00\n",
            "Episode Length 156\n",
            "Episode Reward: 148.00\n",
            "Episode Length 148\n",
            "Episode Reward: 182.00\n",
            "Episode Length 182\n",
            "Episode Reward: 157.00\n",
            "Episode Length 157\n",
            "Episode Reward: 170.00\n",
            "Episode Length 170\n",
            "Episode Reward: 187.00\n",
            "Episode Length 187\n",
            "Episode Reward: 154.00\n",
            "Episode Length 154\n",
            "Episode Reward: 163.00\n",
            "Episode Length 163\n",
            "Episode Reward: 172.00\n",
            "Episode Length 172\n",
            "Episode Reward: 148.00\n",
            "Episode Length 148\n",
            "Episode Reward: 189.00\n",
            "Episode Length 189\n",
            "Episode Reward: 154.00\n",
            "Episode Length 154\n",
            "Episode Reward: 166.00\n",
            "Episode Length 166\n",
            "Episode Reward: 192.00\n",
            "Episode Length 192\n",
            "Episode Reward: 158.00\n",
            "Episode Length 158\n",
            "Episode Reward: 182.00\n",
            "Episode Length 182\n",
            "Episode Reward: 163.00\n",
            "Episode Length 163\n",
            "Episode Reward: 156.00\n",
            "Episode Length 156\n",
            "Episode Reward: 178.00\n",
            "Episode Length 178\n",
            "Episode Reward: 146.00\n",
            "Episode Length 146\n",
            "Episode Reward: 173.00\n",
            "Episode Length 173\n",
            "Episode Reward: 156.00\n",
            "Episode Length 156\n",
            "Episode Reward: 165.00\n",
            "Episode Length 165\n",
            "Episode Reward: 168.00\n",
            "Episode Length 168\n",
            "Episode Reward: 158.00\n",
            "Episode Length 158\n",
            "Episode Reward: 171.00\n",
            "Episode Length 171\n",
            "Episode Reward: 152.00\n",
            "Episode Length 152\n",
            "Episode Reward: 158.00\n",
            "Episode Length 158\n",
            "Episode Reward: 174.00\n",
            "Episode Length 174\n",
            "30 Episodes\n",
            "Mean reward: 165.40 +/- 12.40\n",
            "Mean episode length: 165.40 +/- 12.40\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5Il2J0VHPLC"
      },
      "source": [
        "#### Tune Hyperparameters\n",
        "\n",
        "We use [Optuna](https://optuna.org/) for optimizing the hyperparameters.\n",
        "\n",
        "Tune the hyperparameters for PPO, using a tpe sampler and median pruner, 2 parallels jobs,\n",
        "with a budget of 1000 trials and a maximum of 50000 steps"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2sC22eGHTH-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfad04ca-07f4-4f39-f418-c0b959ec5f41"
      },
      "source": [
        "!python train.py --algo ppo --env MountainCar-v0 -n 5000 -optimize --n-trials 1000 --n-jobs 2 --sampler tpe --pruner median"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "========== MountainCar-v0 ==========\n",
            "Seed: 1368782301\n",
            "Default hyperparameters for environment (ones being tuned will be overridden):\n",
            "OrderedDict([('ent_coef', 0.0),\n",
            "             ('gae_lambda', 0.98),\n",
            "             ('gamma', 0.99),\n",
            "             ('n_envs', 16),\n",
            "             ('n_epochs', 4),\n",
            "             ('n_steps', 16),\n",
            "             ('n_timesteps', 1000000.0),\n",
            "             ('normalize', True),\n",
            "             ('policy', 'MlpPolicy')])\n",
            "Using 16 environments\n",
            "Overwriting n_timesteps with n=50000\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Optimizing hyperparameters\n",
            "/usr/local/lib/python3.7/dist-packages/optuna/samplers/_tpe/sampler.py:266: ExperimentalWarning:\n",
            "\n",
            "``multivariate`` option is an experimental feature. The interface can change in the future.\n",
            "\n",
            "Sampler: tpe - Pruner: median\n",
            "\u001b[32m[I 2022-01-22 21:09:00,682]\u001b[0m A new study created in memory with name: no-name-8198499d-f4e3-4002-a57e-7594694dabed\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/optuna/study/study.py:397: FutureWarning:\n",
            "\n",
            "`n_jobs` argument has been deprecated in v2.7.0. This feature will be removed in v4.0.0. See https://github.com/optuna/optuna/releases/tag/v2.7.0.\n",
            "\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "\u001b[32m[I 2022-01-22 21:14:42,415]\u001b[0m Trial 1 finished with value: -200.0 and parameters: {'batch_size': 8, 'n_steps': 2048, 'gamma': 0.9, 'learning_rate': 0.00010989955574165337, 'ent_coef': 1.3073741214438175e-06, 'clip_range': 0.4, 'n_epochs': 5, 'gae_lambda': 0.92, 'max_grad_norm': 0.9, 'vf_coef': 0.851435424335748, 'net_arch': 'small', 'activation_fn': 'tanh'}. Best is trial 1 with value: -200.0.\u001b[0m\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "\u001b[32m[I 2022-01-22 21:16:09,924]\u001b[0m Trial 2 finished with value: -200.0 and parameters: {'batch_size': 512, 'n_steps': 1024, 'gamma': 0.95, 'learning_rate': 0.007679775711923204, 'ent_coef': 6.504019790431808e-08, 'clip_range': 0.2, 'n_epochs': 10, 'gae_lambda': 0.92, 'max_grad_norm': 0.5, 'vf_coef': 0.5461290503078913, 'net_arch': 'medium', 'activation_fn': 'relu'}. Best is trial 1 with value: -200.0.\u001b[0m\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "\u001b[32m[I 2022-01-22 21:23:40,417]\u001b[0m Trial 3 finished with value: -200.0 and parameters: {'batch_size': 256, 'n_steps': 16, 'gamma': 0.9, 'learning_rate': 0.0004254137188356323, 'ent_coef': 1.7964767482440097e-08, 'clip_range': 0.3, 'n_epochs': 20, 'gae_lambda': 0.9, 'max_grad_norm': 0.9, 'vf_coef': 0.983020155669539, 'net_arch': 'small', 'activation_fn': 'relu'}. Best is trial 1 with value: -200.0.\u001b[0m\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "\u001b[32m[I 2022-01-22 21:25:22,309]\u001b[0m Trial 4 finished with value: -200.0 and parameters: {'batch_size': 512, 'n_steps': 256, 'gamma': 0.95, 'learning_rate': 0.00015047816906869205, 'ent_coef': 3.0447134114359636e-08, 'clip_range': 0.2, 'n_epochs': 20, 'gae_lambda': 0.99, 'max_grad_norm': 0.5, 'vf_coef': 0.29872698116964214, 'net_arch': 'medium', 'activation_fn': 'relu'}. Best is trial 1 with value: -200.0.\u001b[0m\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "\u001b[32m[I 2022-01-22 21:26:17,985]\u001b[0m Trial 5 finished with value: -123.0 and parameters: {'batch_size': 512, 'n_steps': 512, 'gamma': 0.98, 'learning_rate': 3.9580367666331446e-05, 'ent_coef': 0.00013057911405319636, 'clip_range': 0.3, 'n_epochs': 20, 'gae_lambda': 0.98, 'max_grad_norm': 0.8, 'vf_coef': 0.3046963365441169, 'net_arch': 'small', 'activation_fn': 'tanh'}. Best is trial 5 with value: -123.0.\u001b[0m\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "\u001b[32m[I 2022-01-22 21:27:10,932]\u001b[0m Trial 6 finished with value: -200.0 and parameters: {'batch_size': 512, 'n_steps': 64, 'gamma': 0.995, 'learning_rate': 0.026534582562668628, 'ent_coef': 1.935978699200409e-08, 'clip_range': 0.4, 'n_epochs': 1, 'gae_lambda': 0.92, 'max_grad_norm': 0.3, 'vf_coef': 0.6412078222759747, 'net_arch': 'medium', 'activation_fn': 'tanh'}. Best is trial 5 with value: -123.0.\u001b[0m\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "\u001b[32m[I 2022-01-22 21:28:29,255]\u001b[0m Trial 7 finished with value: -200.0 and parameters: {'batch_size': 64, 'n_steps': 256, 'gamma': 0.995, 'learning_rate': 0.02358516543257077, 'ent_coef': 0.006855340325748077, 'clip_range': 0.2, 'n_epochs': 5, 'gae_lambda': 0.98, 'max_grad_norm': 5, 'vf_coef': 0.6895384667359808, 'net_arch': 'medium', 'activation_fn': 'tanh'}. Best is trial 5 with value: -123.0.\u001b[0m\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "\u001b[32m[I 2022-01-22 21:30:40,171]\u001b[0m Trial 8 finished with value: -200.0 and parameters: {'batch_size': 256, 'n_steps': 64, 'gamma': 0.95, 'learning_rate': 0.08700107422057327, 'ent_coef': 0.039255750903115845, 'clip_range': 0.2, 'n_epochs': 10, 'gae_lambda': 0.8, 'max_grad_norm': 0.9, 'vf_coef': 0.26231325077200696, 'net_arch': 'medium', 'activation_fn': 'relu'}. Best is trial 5 with value: -123.0.\u001b[0m\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "\u001b[32m[I 2022-01-22 21:32:13,304]\u001b[0m Trial 9 finished with value: -200.0 and parameters: {'batch_size': 32, 'n_steps': 32, 'gamma': 0.98, 'learning_rate': 0.015243293833122356, 'ent_coef': 4.699814640218708e-06, 'clip_range': 0.2, 'n_epochs': 5, 'gae_lambda': 0.8, 'max_grad_norm': 0.8, 'vf_coef': 0.2642235587740167, 'net_arch': 'medium', 'activation_fn': 'tanh'}. Best is trial 5 with value: -123.0.\u001b[0m\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "\u001b[32m[I 2022-01-22 21:43:51,609]\u001b[0m Trial 10 finished with value: -200.0 and parameters: {'batch_size': 8, 'n_steps': 32, 'gamma': 0.99, 'learning_rate': 0.02608644497760704, 'ent_coef': 0.005435828026780079, 'clip_range': 0.2, 'n_epochs': 20, 'gae_lambda': 0.99, 'max_grad_norm': 0.5, 'vf_coef': 0.5390543084396818, 'net_arch': 'small', 'activation_fn': 'relu'}. Best is trial 5 with value: -123.0.\u001b[0m\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "\u001b[32m[I 2022-01-22 21:44:41,751]\u001b[0m Trial 11 finished with value: -122.4 and parameters: {'batch_size': 512, 'n_steps': 64, 'gamma': 0.98, 'learning_rate': 1.778856118882496e-05, 'ent_coef': 0.0014565038195096312, 'clip_range': 0.3, 'n_epochs': 5, 'gae_lambda': 0.99, 'max_grad_norm': 0.8, 'vf_coef': 0.42491092539394015, 'net_arch': 'small', 'activation_fn': 'tanh'}. Best is trial 11 with value: -122.4.\u001b[0m\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "\u001b[32m[I 2022-01-22 21:45:43,350]\u001b[0m Trial 12 finished with value: -122.4 and parameters: {'batch_size': 512, 'n_steps': 512, 'gamma': 0.98, 'learning_rate': 2.398862966318614e-05, 'ent_coef': 6.079916027833228e-05, 'clip_range': 0.4, 'n_epochs': 20, 'gae_lambda': 0.98, 'max_grad_norm': 0.8, 'vf_coef': 0.2876415702120783, 'net_arch': 'small', 'activation_fn': 'tanh'}. Best is trial 11 with value: -122.4.\u001b[0m\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "\u001b[32m[I 2022-01-22 21:46:33,751]\u001b[0m Trial 13 finished with value: -115.8 and parameters: {'batch_size': 64, 'n_steps': 64, 'gamma': 0.98, 'learning_rate': 8.856946324366379e-05, 'ent_coef': 0.03711676466451531, 'clip_range': 0.3, 'n_epochs': 5, 'gae_lambda': 0.95, 'max_grad_norm': 2, 'vf_coef': 0.07945655348048497, 'net_arch': 'small', 'activation_fn': 'tanh'}. Best is trial 13 with value: -115.8.\u001b[0m\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "\u001b[32m[I 2022-01-22 21:47:17,812]\u001b[0m Trial 14 pruned. \u001b[0m\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "\u001b[32m[I 2022-01-22 21:48:19,745]\u001b[0m Trial 15 finished with value: -112.0 and parameters: {'batch_size': 64, 'n_steps': 64, 'gamma': 0.98, 'learning_rate': 1.8112387205306157e-05, 'ent_coef': 0.0003331587903616849, 'clip_range': 0.3, 'n_epochs': 5, 'gae_lambda': 0.95, 'max_grad_norm': 1, 'vf_coef': 0.569026122416139, 'net_arch': 'medium', 'activation_fn': 'tanh'}. Best is trial 15 with value: -112.0.\u001b[0m\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "\u001b[32m[I 2022-01-22 21:49:04,720]\u001b[0m Trial 16 finished with value: -200.0 and parameters: {'batch_size': 64, 'n_steps': 64, 'gamma': 0.98, 'learning_rate': 4.8093419114490714e-05, 'ent_coef': 0.08924506777968393, 'clip_range': 0.3, 'n_epochs': 1, 'gae_lambda': 0.98, 'max_grad_norm': 2, 'vf_coef': 0.19467924819094645, 'net_arch': 'medium', 'activation_fn': 'relu'}. Best is trial 15 with value: -112.0.\u001b[0m\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "\u001b[32m[I 2022-01-22 21:49:47,966]\u001b[0m Trial 17 finished with value: -176.0 and parameters: {'batch_size': 64, 'n_steps': 64, 'gamma': 0.9, 'learning_rate': 9.089077073951944e-05, 'ent_coef': 8.458432613381915e-06, 'clip_range': 0.3, 'n_epochs': 1, 'gae_lambda': 0.9, 'max_grad_norm': 0.9, 'vf_coef': 0.4540354205919347, 'net_arch': 'medium', 'activation_fn': 'tanh'}. Best is trial 15 with value: -112.0.\u001b[0m\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "\u001b[32m[I 2022-01-22 21:53:18,673]\u001b[0m Trial 18 finished with value: -200.0 and parameters: {'batch_size': 64, 'n_steps': 1024, 'gamma': 0.98, 'learning_rate': 2.1143460183795828e-05, 'ent_coef': 0.017982141478172904, 'clip_range': 0.3, 'n_epochs': 20, 'gae_lambda': 0.95, 'max_grad_norm': 1, 'vf_coef': 0.44038329143823207, 'net_arch': 'medium', 'activation_fn': 'tanh'}. Best is trial 15 with value: -112.0.\u001b[0m\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "\u001b[32m[I 2022-01-22 21:54:28,934]\u001b[0m Trial 19 finished with value: -113.8 and parameters: {'batch_size': 64, 'n_steps': 64, 'gamma': 0.999, 'learning_rate': 1.3024544535129103e-05, 'ent_coef': 1.769247087661209e-06, 'clip_range': 0.3, 'n_epochs': 10, 'gae_lambda': 0.95, 'max_grad_norm': 1, 'vf_coef': 0.7867280187671176, 'net_arch': 'small', 'activation_fn': 'tanh'}. Best is trial 15 with value: -112.0.\u001b[0m\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "\u001b[32m[I 2022-01-22 21:59:49,491]\u001b[0m Trial 20 finished with value: -112.2 and parameters: {'batch_size': 8, 'n_steps': 64, 'gamma': 0.999, 'learning_rate': 1.1009184081654504e-05, 'ent_coef': 5.466191293149913e-08, 'clip_range': 0.3, 'n_epochs': 10, 'gae_lambda': 0.98, 'max_grad_norm': 0.6, 'vf_coef': 0.6139220243099272, 'net_arch': 'small', 'activation_fn': 'tanh'}. Best is trial 15 with value: -112.0.\u001b[0m\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "\u001b[32m[I 2022-01-22 22:00:52,807]\u001b[0m Trial 21 pruned. \u001b[0m\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "\u001b[32m[I 2022-01-22 22:02:16,900]\u001b[0m Trial 0 finished with value: -200.0 and parameters: {'batch_size': 8, 'n_steps': 1024, 'gamma': 0.99, 'learning_rate': 0.00797653853266894, 'ent_coef': 0.027358272480827978, 'clip_range': 0.2, 'n_epochs': 20, 'gae_lambda': 1.0, 'max_grad_norm': 0.3, 'vf_coef': 0.35332353682894846, 'net_arch': 'medium', 'activation_fn': 'tanh'}. Best is trial 15 with value: -112.0.\u001b[0m\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "\u001b[32m[I 2022-01-22 22:11:19,326]\u001b[0m Trial 23 finished with value: -112.4 and parameters: {'batch_size': 8, 'n_steps': 64, 'gamma': 0.9999, 'learning_rate': 1.1924836438533524e-05, 'ent_coef': 2.4839672772673792e-08, 'clip_range': 0.3, 'n_epochs': 10, 'gae_lambda': 0.99, 'max_grad_norm': 0.6, 'vf_coef': 0.7313553901601041, 'net_arch': 'small', 'activation_fn': 'tanh'}. Best is trial 15 with value: -112.0.\u001b[0m\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "\u001b[32m[I 2022-01-22 22:13:23,982]\u001b[0m Trial 24 finished with value: -116.6 and parameters: {'batch_size': 128, 'n_steps': 64, 'gamma': 0.9999, 'learning_rate': 0.00016607535278218604, 'ent_coef': 6.215759078329123e-08, 'clip_range': 0.4, 'n_epochs': 10, 'gae_lambda': 0.99, 'max_grad_norm': 0.6, 'vf_coef': 0.7777862427520351, 'net_arch': 'medium', 'activation_fn': 'tanh'}. Best is trial 15 with value: -112.0.\u001b[0m\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "\u001b[32m[I 2022-01-22 22:17:35,066]\u001b[0m Trial 22 finished with value: -143.8 and parameters: {'batch_size': 8, 'n_steps': 64, 'gamma': 0.999, 'learning_rate': 0.00015424904535045342, 'ent_coef': 3.4833922049952926e-06, 'clip_range': 0.3, 'n_epochs': 20, 'gae_lambda': 0.98, 'max_grad_norm': 0.6, 'vf_coef': 0.7001303206038149, 'net_arch': 'small', 'activation_fn': 'tanh'}. Best is trial 15 with value: -112.0.\u001b[0m\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "\u001b[32m[I 2022-01-22 22:19:14,002]\u001b[0m Trial 26 finished with value: -117.6 and parameters: {'batch_size': 512, 'n_steps': 64, 'gamma': 0.995, 'learning_rate': 1.4212042507239996e-05, 'ent_coef': 8.774179681242426e-08, 'clip_range': 0.3, 'n_epochs': 10, 'gae_lambda': 0.98, 'max_grad_norm': 0.6, 'vf_coef': 0.4633855582712587, 'net_arch': 'small', 'activation_fn': 'tanh'}. Best is trial 15 with value: -112.0.\u001b[0m\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "\u001b[32m[I 2022-01-22 22:20:36,893]\u001b[0m Trial 27 finished with value: -116.0 and parameters: {'batch_size': 64, 'n_steps': 64, 'gamma': 0.98, 'learning_rate': 4.2715824112696765e-05, 'ent_coef': 0.0018613493210639623, 'clip_range': 0.2, 'n_epochs': 5, 'gae_lambda': 0.9, 'max_grad_norm': 1, 'vf_coef': 0.41287652902562544, 'net_arch': 'medium', 'activation_fn': 'tanh'}. Best is trial 15 with value: -112.0.\u001b[0m\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "\u001b[32m[I 2022-01-22 22:22:54,565]\u001b[0m Trial 25 finished with value: -96.6 and parameters: {'batch_size': 8, 'n_steps': 64, 'gamma': 0.9999, 'learning_rate': 3.6568461617252886e-05, 'ent_coef': 1.1603253166028883e-06, 'clip_range': 0.3, 'n_epochs': 10, 'gae_lambda': 0.99, 'max_grad_norm': 0.6, 'vf_coef': 0.713671353121, 'net_arch': 'small', 'activation_fn': 'tanh'}. Best is trial 25 with value: -96.6.\u001b[0m\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "\u001b[32m[I 2022-01-22 22:29:42,463]\u001b[0m Trial 28 finished with value: -108.4 and parameters: {'batch_size': 8, 'n_steps': 64, 'gamma': 0.9999, 'learning_rate': 1.4021280087154904e-05, 'ent_coef': 1.0650775849238879e-07, 'clip_range': 0.3, 'n_epochs': 10, 'gae_lambda': 0.9, 'max_grad_norm': 0.5, 'vf_coef': 0.8443295149562576, 'net_arch': 'small', 'activation_fn': 'tanh'}. Best is trial 25 with value: -96.6.\u001b[0m\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "\u001b[32m[I 2022-01-22 22:31:06,753]\u001b[0m Trial 30 pruned. \u001b[0m\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "\u001b[32m[I 2022-01-22 22:32:01,291]\u001b[0m Trial 31 pruned. \u001b[0m\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "\u001b[32m[I 2022-01-22 22:32:23,398]\u001b[0m Trial 29 finished with value: -144.6 and parameters: {'batch_size': 64, 'n_steps': 8, 'gamma': 0.999, 'learning_rate': 0.00015368889742558406, 'ent_coef': 1.3102952600675722e-08, 'clip_range': 0.1, 'n_epochs': 10, 'gae_lambda': 0.98, 'max_grad_norm': 0.6, 'vf_coef': 0.8202260511407828, 'net_arch': 'small', 'activation_fn': 'tanh'}. Best is trial 25 with value: -96.6.\u001b[0m\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "\u001b[32m[I 2022-01-22 22:33:30,037]\u001b[0m Trial 32 pruned. \u001b[0m\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "\u001b[32m[I 2022-01-22 22:33:49,243]\u001b[0m Trial 33 finished with value: -120.2 and parameters: {'batch_size': 128, 'n_steps': 64, 'gamma': 0.98, 'learning_rate': 1.630401685481452e-05, 'ent_coef': 9.160913491153429e-05, 'clip_range': 0.3, 'n_epochs': 5, 'gae_lambda': 0.95, 'max_grad_norm': 1, 'vf_coef': 0.5702143503794705, 'net_arch': 'medium', 'activation_fn': 'tanh'}. Best is trial 25 with value: -96.6.\u001b[0m\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "\u001b[32m[I 2022-01-22 22:34:05,034]\u001b[0m Trial 35 pruned. \u001b[0m\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "\u001b[32m[I 2022-01-22 22:34:45,268]\u001b[0m Trial 36 pruned. \u001b[0m\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "\u001b[32m[I 2022-01-22 22:35:16,772]\u001b[0m Trial 37 pruned. \u001b[0m\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "\u001b[32m[I 2022-01-22 22:37:26,345]\u001b[0m Trial 38 finished with value: -96.6 and parameters: {'batch_size': 256, 'n_steps': 64, 'gamma': 0.999, 'learning_rate': 9.808278909553055e-05, 'ent_coef': 6.047144145080958e-08, 'clip_range': 0.2, 'n_epochs': 10, 'gae_lambda': 0.9, 'max_grad_norm': 0.5, 'vf_coef': 0.976587834487318, 'net_arch': 'medium', 'activation_fn': 'tanh'}. Best is trial 25 with value: -96.6.\u001b[0m\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "\u001b[32m[I 2022-01-22 22:39:35,483]\u001b[0m Trial 39 finished with value: -97.4 and parameters: {'batch_size': 256, 'n_steps': 64, 'gamma': 0.999, 'learning_rate': 0.0005245840507181016, 'ent_coef': 1.1903564769588207e-06, 'clip_range': 0.2, 'n_epochs': 10, 'gae_lambda': 0.92, 'max_grad_norm': 0.5, 'vf_coef': 0.7478721058716602, 'net_arch': 'medium', 'activation_fn': 'tanh'}. Best is trial 25 with value: -96.6.\u001b[0m\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "\u001b[32m[I 2022-01-22 22:39:57,375]\u001b[0m Trial 40 pruned. \u001b[0m\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "\u001b[32m[I 2022-01-22 22:40:38,520]\u001b[0m Trial 41 pruned. \u001b[0m\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "\u001b[32m[I 2022-01-22 22:41:37,086]\u001b[0m Trial 42 pruned. \u001b[0m\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "\u001b[32m[I 2022-01-22 22:43:19,564]\u001b[0m Trial 34 finished with value: -200.0 and parameters: {'batch_size': 8, 'n_steps': 32, 'gamma': 0.9, 'learning_rate': 2.0792723647718195e-05, 'ent_coef': 1.4815283311911572e-06, 'clip_range': 0.2, 'n_epochs': 10, 'gae_lambda': 0.98, 'max_grad_norm': 0.5, 'vf_coef': 0.6655200591100794, 'net_arch': 'small', 'activation_fn': 'tanh'}. Best is trial 25 with value: -96.6.\u001b[0m\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "\u001b[32m[I 2022-01-22 22:44:38,439]\u001b[0m Trial 44 pruned. \u001b[0m\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "\u001b[32m[I 2022-01-22 22:47:00,704]\u001b[0m Trial 43 pruned. \u001b[0m\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "\u001b[32m[I 2022-01-22 22:51:56,822]\u001b[0m Trial 46 finished with value: -112.6 and parameters: {'batch_size': 32, 'n_steps': 64, 'gamma': 0.9999, 'learning_rate': 1.9154760169088334e-05, 'ent_coef': 2.6865465132847988e-08, 'clip_range': 0.3, 'n_epochs': 20, 'gae_lambda': 0.9, 'max_grad_norm': 0.8, 'vf_coef': 0.7469847229883569, 'net_arch': 'small', 'activation_fn': 'tanh'}. Best is trial 25 with value: -96.6.\u001b[0m\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "\u001b[32m[I 2022-01-22 22:52:44,046]\u001b[0m Trial 47 pruned. \u001b[0m\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "\u001b[32m[I 2022-01-22 22:56:03,900]\u001b[0m Trial 48 finished with value: -111.8 and parameters: {'batch_size': 32, 'n_steps': 64, 'gamma': 0.999, 'learning_rate': 2.020234180631766e-05, 'ent_coef': 5.3136548639549576e-08, 'clip_range': 0.2, 'n_epochs': 10, 'gae_lambda': 0.9, 'max_grad_norm': 0.8, 'vf_coef': 0.9906480418089804, 'net_arch': 'medium', 'activation_fn': 'tanh'}. Best is trial 25 with value: -96.6.\u001b[0m\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "\u001b[32m[I 2022-01-22 22:56:39,896]\u001b[0m Trial 45 finished with value: -200.0 and parameters: {'batch_size': 8, 'n_steps': 2048, 'gamma': 0.9999, 'learning_rate': 6.123003955901895e-05, 'ent_coef': 3.5201716491639596e-05, 'clip_range': 0.3, 'n_epochs': 10, 'gae_lambda': 0.9, 'max_grad_norm': 0.5, 'vf_coef': 0.7114417361712233, 'net_arch': 'small', 'activation_fn': 'tanh'}. Best is trial 25 with value: -96.6.\u001b[0m\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "\u001b[32m[I 2022-01-22 22:57:12,721]\u001b[0m Trial 49 pruned. \u001b[0m\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "\u001b[32m[I 2022-01-22 22:57:41,412]\u001b[0m Trial 50 pruned. \u001b[0m\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "\u001b[32m[I 2022-01-22 23:01:05,659]\u001b[0m Trial 51 finished with value: -140.2 and parameters: {'batch_size': 256, 'n_steps': 64, 'gamma': 0.999, 'learning_rate': 1.127721390107111e-05, 'ent_coef': 1.3846802205972249e-07, 'clip_range': 0.4, 'n_epochs': 20, 'gae_lambda': 0.98, 'max_grad_norm': 0.5, 'vf_coef': 0.9692811020905454, 'net_arch': 'medium', 'activation_fn': 'tanh'}. Best is trial 25 with value: -96.6.\u001b[0m\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "\u001b[32m[I 2022-01-22 23:01:33,666]\u001b[0m Trial 52 finished with value: -96.0 and parameters: {'batch_size': 256, 'n_steps': 64, 'gamma': 0.999, 'learning_rate': 0.0001210512314627554, 'ent_coef': 1.1104325772131861e-07, 'clip_range': 0.2, 'n_epochs': 20, 'gae_lambda': 0.9, 'max_grad_norm': 0.5, 'vf_coef': 0.7593647702539166, 'net_arch': 'medium', 'activation_fn': 'tanh'}. Best is trial 52 with value: -96.0.\u001b[0m\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "\u001b[32m[I 2022-01-22 23:01:47,863]\u001b[0m Trial 54 pruned. \u001b[0m\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "\u001b[32m[I 2022-01-22 23:03:05,999]\u001b[0m Trial 55 pruned. \u001b[0m\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "\u001b[32m[I 2022-01-22 23:03:25,188]\u001b[0m Trial 53 finished with value: -95.8 and parameters: {'batch_size': 512, 'n_steps': 64, 'gamma': 0.999, 'learning_rate': 4.207855095736749e-05, 'ent_coef': 5.797108113862232e-08, 'clip_range': 0.2, 'n_epochs': 10, 'gae_lambda': 0.9, 'max_grad_norm': 0.8, 'vf_coef': 0.9843262950459849, 'net_arch': 'medium', 'activation_fn': 'tanh'}. Best is trial 53 with value: -95.8.\u001b[0m\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "\u001b[32m[I 2022-01-22 23:05:09,044]\u001b[0m Trial 56 finished with value: -113.0 and parameters: {'batch_size': 32, 'n_steps': 64, 'gamma': 0.999, 'learning_rate': 2.213040792603105e-05, 'ent_coef': 3.5047384271519514e-08, 'clip_range': 0.2, 'n_epochs': 5, 'gae_lambda': 0.8, 'max_grad_norm': 0.8, 'vf_coef': 0.8800389227335172, 'net_arch': 'medium', 'activation_fn': 'tanh'}. Best is trial 53 with value: -95.8.\u001b[0m\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "\u001b[32m[I 2022-01-22 23:05:24,707]\u001b[0m Trial 58 pruned. \u001b[0m\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "\u001b[32m[I 2022-01-22 23:06:05,346]\u001b[0m Trial 59 pruned. \u001b[0m\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "\u001b[32m[I 2022-01-22 23:07:04,009]\u001b[0m Trial 57 finished with value: -142.4 and parameters: {'batch_size': 16, 'n_steps': 32, 'gamma': 0.999, 'learning_rate': 5.285346298235567e-05, 'ent_coef': 6.098386999092934e-07, 'clip_range': 0.2, 'n_epochs': 5, 'gae_lambda': 0.9, 'max_grad_norm': 0.5, 'vf_coef': 0.8606374623390984, 'net_arch': 'medium', 'activation_fn': 'tanh'}. Best is trial 53 with value: -95.8.\u001b[0m\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "\u001b[32m[I 2022-01-22 23:08:44,504]\u001b[0m Trial 61 finished with value: -113.4 and parameters: {'batch_size': 128, 'n_steps': 64, 'gamma': 0.9999, 'learning_rate': 3.860984058976674e-05, 'ent_coef': 3.898156151039305e-07, 'clip_range': 0.3, 'n_epochs': 10, 'gae_lambda': 0.9, 'max_grad_norm': 0.5, 'vf_coef': 0.9565791049723991, 'net_arch': 'small', 'activation_fn': 'tanh'}. Best is trial 53 with value: -95.8.\u001b[0m\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "\u001b[32m[I 2022-01-22 23:09:03,334]\u001b[0m Trial 60 pruned. \u001b[0m\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "\u001b[32m[I 2022-01-22 23:10:46,630]\u001b[0m Trial 63 finished with value: -111.6 and parameters: {'batch_size': 512, 'n_steps': 64, 'gamma': 0.999, 'learning_rate': 6.201028917344681e-05, 'ent_coef': 1.1963473836697962e-08, 'clip_range': 0.2, 'n_epochs': 10, 'gae_lambda': 0.9, 'max_grad_norm': 0.8, 'vf_coef': 0.8074625164447831, 'net_arch': 'small', 'activation_fn': 'tanh'}. Best is trial 53 with value: -95.8.\u001b[0m\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "\u001b[32m[I 2022-01-22 23:12:28,280]\u001b[0m Trial 64 finished with value: -119.4 and parameters: {'batch_size': 512, 'n_steps': 64, 'gamma': 0.999, 'learning_rate': 0.00034094502958661327, 'ent_coef': 4.3968848092957e-08, 'clip_range': 0.2, 'n_epochs': 10, 'gae_lambda': 0.9, 'max_grad_norm': 0.8, 'vf_coef': 0.8160050270180695, 'net_arch': 'small', 'activation_fn': 'tanh'}. Best is trial 53 with value: -95.8.\u001b[0m\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "\u001b[32m[I 2022-01-22 23:14:09,773]\u001b[0m Trial 62 finished with value: -109.0 and parameters: {'batch_size': 16, 'n_steps': 64, 'gamma': 0.999, 'learning_rate': 3.286180211035073e-05, 'ent_coef': 1.3374920355412886e-07, 'clip_range': 0.2, 'n_epochs': 10, 'gae_lambda': 0.9, 'max_grad_norm': 0.8, 'vf_coef': 0.9578549092372269, 'net_arch': 'small', 'activation_fn': 'tanh'}. Best is trial 53 with value: -95.8.\u001b[0m\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "\u001b[32m[I 2022-01-22 23:23:27,070]\u001b[0m Trial 66 finished with value: -129.6 and parameters: {'batch_size': 16, 'n_steps': 256, 'gamma': 0.999, 'learning_rate': 3.0447088228172152e-05, 'ent_coef': 1.0178956492408455e-07, 'clip_range': 0.2, 'n_epochs': 20, 'gae_lambda': 0.9, 'max_grad_norm': 5, 'vf_coef': 0.8971502706280986, 'net_arch': 'small', 'activation_fn': 'tanh'}. Best is trial 53 with value: -95.8.\u001b[0m\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "\u001b[32m[I 2022-01-22 23:23:45,533]\u001b[0m Trial 65 finished with value: -132.8 and parameters: {'batch_size': 8, 'n_steps': 8, 'gamma': 0.9999, 'learning_rate': 2.624535006812603e-05, 'ent_coef': 1.079417396600241e-08, 'clip_range': 0.3, 'n_epochs': 10, 'gae_lambda': 0.9, 'max_grad_norm': 0.5, 'vf_coef': 0.6651433403681005, 'net_arch': 'medium', 'activation_fn': 'tanh'}. Best is trial 53 with value: -95.8.\u001b[0m\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "\u001b[32m[I 2022-01-22 23:29:28,069]\u001b[0m Trial 68 pruned. \u001b[0m\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "\u001b[32m[I 2022-01-22 23:29:42,334]\u001b[0m Trial 69 pruned. \u001b[0m\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "\u001b[32m[I 2022-01-22 23:30:25,092]\u001b[0m Trial 70 finished with value: -114.4 and parameters: {'batch_size': 128, 'n_steps': 64, 'gamma': 0.999, 'learning_rate': 0.00021654298980574698, 'ent_coef': 3.8927599441055334e-08, 'clip_range': 0.2, 'n_epochs': 1, 'gae_lambda': 0.9, 'max_grad_norm': 0.5, 'vf_coef': 0.3119465594669711, 'net_arch': 'medium', 'activation_fn': 'tanh'}. Best is trial 53 with value: -95.8.\u001b[0m\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "\u001b[32m[I 2022-01-22 23:31:09,585]\u001b[0m Trial 71 pruned. \u001b[0m\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "\u001b[32m[I 2022-01-22 23:31:22,378]\u001b[0m Trial 72 pruned. \u001b[0m\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "\u001b[32m[I 2022-01-22 23:33:51,305]\u001b[0m Trial 73 pruned. \u001b[0m\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "\u001b[32m[I 2022-01-22 23:34:04,599]\u001b[0m Trial 74 pruned. \u001b[0m\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "\u001b[32m[I 2022-01-22 23:35:36,308]\u001b[0m Trial 67 finished with value: -156.0 and parameters: {'batch_size': 16, 'n_steps': 64, 'gamma': 0.999, 'learning_rate': 0.0008606592554321498, 'ent_coef': 1.023162058572397e-06, 'clip_range': 0.2, 'n_epochs': 20, 'gae_lambda': 0.95, 'max_grad_norm': 0.9, 'vf_coef': 0.8317173671321643, 'net_arch': 'medium', 'activation_fn': 'tanh'}. Best is trial 53 with value: -95.8.\u001b[0m\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "\u001b[32m[I 2022-01-22 23:36:09,523]\u001b[0m Trial 75 finished with value: -97.0 and parameters: {'batch_size': 512, 'n_steps': 64, 'gamma': 0.98, 'learning_rate': 8.126055938005065e-05, 'ent_coef': 5.153646060630491e-07, 'clip_range': 0.3, 'n_epochs': 10, 'gae_lambda': 1.0, 'max_grad_norm': 0.8, 'vf_coef': 0.866874695909517, 'net_arch': 'medium', 'activation_fn': 'tanh'}. Best is trial 53 with value: -95.8.\u001b[0m\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "\u001b[32m[I 2022-01-22 23:36:43,632]\u001b[0m Trial 76 pruned. \u001b[0m\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "\u001b[32m[I 2022-01-22 23:38:37,857]\u001b[0m Trial 78 pruned. \u001b[0m\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "\u001b[32m[I 2022-01-22 23:40:11,156]\u001b[0m Trial 79 finished with value: -115.8 and parameters: {'batch_size': 512, 'n_steps': 128, 'gamma': 0.9, 'learning_rate': 6.175147572482963e-05, 'ent_coef': 6.499071656786693e-08, 'clip_range': 0.4, 'n_epochs': 10, 'gae_lambda': 0.9, 'max_grad_norm': 0.8, 'vf_coef': 0.8378996640924938, 'net_arch': 'medium', 'activation_fn': 'tanh'}. Best is trial 53 with value: -95.8.\u001b[0m\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "\u001b[32m[I 2022-01-22 23:40:24,639]\u001b[0m Trial 80 pruned. \u001b[0m\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "\u001b[32m[I 2022-01-22 23:40:50,104]\u001b[0m Trial 81 pruned. \u001b[0m\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "\u001b[32m[I 2022-01-22 23:42:23,602]\u001b[0m Trial 82 pruned. \u001b[0m\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "\u001b[32m[I 2022-01-22 23:42:41,628]\u001b[0m Trial 77 finished with value: -172.8 and parameters: {'batch_size': 16, 'n_steps': 64, 'gamma': 0.995, 'learning_rate': 0.0012220873360870817, 'ent_coef': 2.4285564593875956e-06, 'clip_range': 0.3, 'n_epochs': 10, 'gae_lambda': 1.0, 'max_grad_norm': 0.6, 'vf_coef': 0.9595430044895116, 'net_arch': 'medium', 'activation_fn': 'tanh'}. Best is trial 53 with value: -95.8.\u001b[0m\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "\u001b[32m[I 2022-01-22 23:44:34,616]\u001b[0m Trial 83 pruned. \u001b[0m\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "\u001b[32m[I 2022-01-22 23:46:08,515]\u001b[0m Trial 85 pruned. \u001b[0m\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "\u001b[32m[I 2022-01-22 23:47:17,492]\u001b[0m Trial 86 pruned. \u001b[0m\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "\u001b[32m[I 2022-01-22 23:47:31,299]\u001b[0m Trial 87 pruned. \u001b[0m\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "\u001b[32m[I 2022-01-22 23:47:50,544]\u001b[0m Trial 84 finished with value: -119.0 and parameters: {'batch_size': 16, 'n_steps': 64, 'gamma': 0.9, 'learning_rate': 7.064802574939127e-05, 'ent_coef': 1.8725965800860312e-06, 'clip_range': 0.2, 'n_epochs': 10, 'gae_lambda': 0.8, 'max_grad_norm': 2, 'vf_coef': 0.9539244715067614, 'net_arch': 'small', 'activation_fn': 'tanh'}. Best is trial 53 with value: -95.8.\u001b[0m\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "\u001b[32m[I 2022-01-22 23:50:05,502]\u001b[0m Trial 89 finished with value: -111.8 and parameters: {'batch_size': 512, 'n_steps': 64, 'gamma': 0.999, 'learning_rate': 4.059026464353312e-05, 'ent_coef': 5.033058748205191e-06, 'clip_range': 0.2, 'n_epochs': 10, 'gae_lambda': 0.92, 'max_grad_norm': 0.6, 'vf_coef': 0.9375399821163984, 'net_arch': 'medium', 'activation_fn': 'tanh'}. Best is trial 53 with value: -95.8.\u001b[0m\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "\u001b[32m[I 2022-01-22 23:50:19,055]\u001b[0m Trial 90 pruned. \u001b[0m\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "\u001b[32m[I 2022-01-22 23:50:31,968]\u001b[0m Trial 91 pruned. \u001b[0m\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "\u001b[32m[I 2022-01-22 23:51:54,845]\u001b[0m Trial 88 finished with value: -163.2 and parameters: {'batch_size': 512, 'n_steps': 64, 'gamma': 0.999, 'learning_rate': 0.0007901753690274843, 'ent_coef': 6.859395185489468e-05, 'clip_range': 0.3, 'n_epochs': 20, 'gae_lambda': 0.92, 'max_grad_norm': 0.5, 'vf_coef': 0.8498525996954598, 'net_arch': 'medium', 'activation_fn': 'tanh'}. Best is trial 53 with value: -95.8.\u001b[0m\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "\u001b[32m[I 2022-01-22 23:52:35,845]\u001b[0m Trial 93 pruned. \u001b[0m\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "\u001b[32m[I 2022-01-22 23:52:51,209]\u001b[0m Trial 92 finished with value: -96.4 and parameters: {'batch_size': 512, 'n_steps': 64, 'gamma': 0.999, 'learning_rate': 0.00017812994932499488, 'ent_coef': 0.0003990488742544669, 'clip_range': 0.2, 'n_epochs': 10, 'gae_lambda': 0.99, 'max_grad_norm': 0.6, 'vf_coef': 0.9909274427475141, 'net_arch': 'medium', 'activation_fn': 'tanh'}. Best is trial 53 with value: -95.8.\u001b[0m\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "\u001b[32m[I 2022-01-22 23:53:46,380]\u001b[0m Trial 94 finished with value: -167.6 and parameters: {'batch_size': 128, 'n_steps': 128, 'gamma': 0.999, 'learning_rate': 1.5117170584205754e-05, 'ent_coef': 9.717955117337326e-07, 'clip_range': 0.2, 'n_epochs': 10, 'gae_lambda': 0.9, 'max_grad_norm': 0.3, 'vf_coef': 0.6531654649459502, 'net_arch': 'small', 'activation_fn': 'tanh'}. Best is trial 53 with value: -95.8.\u001b[0m\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "\u001b[32m[I 2022-01-22 23:54:26,596]\u001b[0m Trial 95 pruned. \u001b[0m\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "\u001b[32m[I 2022-01-22 23:54:40,858]\u001b[0m Trial 96 finished with value: -113.2 and parameters: {'batch_size': 512, 'n_steps': 64, 'gamma': 0.999, 'learning_rate': 6.385338367945095e-05, 'ent_coef': 5.9185825975712e-06, 'clip_range': 0.2, 'n_epochs': 1, 'gae_lambda': 0.99, 'max_grad_norm': 0.8, 'vf_coef': 0.9012785950556638, 'net_arch': 'medium', 'activation_fn': 'tanh'}. Best is trial 53 with value: -95.8.\u001b[0m\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "\u001b[32m[I 2022-01-22 23:54:44,035]\u001b[0m Trial 97 pruned. \u001b[0m\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "\u001b[32m[I 2022-01-22 23:56:55,897]\u001b[0m Trial 98 finished with value: -109.6 and parameters: {'batch_size': 512, 'n_steps': 64, 'gamma': 0.9999, 'learning_rate': 1.491390446902406e-05, 'ent_coef': 3.203680804062279e-07, 'clip_range': 0.2, 'n_epochs': 10, 'gae_lambda': 0.9, 'max_grad_norm': 0.3, 'vf_coef': 0.8236576874958009, 'net_arch': 'medium', 'activation_fn': 'tanh'}. Best is trial 53 with value: -95.8.\u001b[0m\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "\u001b[32m[I 2022-01-22 23:57:43,502]\u001b[0m Trial 100 pruned. \u001b[0m\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "\u001b[32m[I 2022-01-22 23:59:47,258]\u001b[0m Trial 99 finished with value: -108.8 and parameters: {'batch_size': 16, 'n_steps': 64, 'gamma': 0.999, 'learning_rate': 3.4527917020231814e-05, 'ent_coef': 4.778190628869614e-05, 'clip_range': 0.1, 'n_epochs': 10, 'gae_lambda': 0.9, 'max_grad_norm': 0.8, 'vf_coef': 0.9519641606816669, 'net_arch': 'small', 'activation_fn': 'tanh'}. Best is trial 53 with value: -95.8.\u001b[0m\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "\u001b[32m[I 2022-01-23 00:01:18,230]\u001b[0m Trial 102 pruned. \u001b[0m\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "\u001b[32m[I 2022-01-23 00:01:33,371]\u001b[0m Trial 103 pruned. \u001b[0m\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "\u001b[32m[I 2022-01-23 00:01:33,747]\u001b[0m Trial 101 finished with value: -124.0 and parameters: {'batch_size': 256, 'n_steps': 32, 'gamma': 0.9999, 'learning_rate': 2.3916362213830814e-05, 'ent_coef': 1.0086762979121652e-07, 'clip_range': 0.2, 'n_epochs': 10, 'gae_lambda': 0.98, 'max_grad_norm': 0.3, 'vf_coef': 0.9442221600042662, 'net_arch': 'medium', 'activation_fn': 'tanh'}. Best is trial 53 with value: -95.8.\u001b[0m\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "\u001b[32m[I 2022-01-23 00:03:01,269]\u001b[0m Trial 104 finished with value: -120.0 and parameters: {'batch_size': 256, 'n_steps': 64, 'gamma': 0.999, 'learning_rate': 0.0002315553577256951, 'ent_coef': 0.0039791772724351695, 'clip_range': 0.2, 'n_epochs': 5, 'gae_lambda': 0.99, 'max_grad_norm': 0.6, 'vf_coef': 0.9852524991161087, 'net_arch': 'medium', 'activation_fn': 'tanh'}. Best is trial 53 with value: -95.8.\u001b[0m\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "\u001b[32m[I 2022-01-23 00:08:10,763]\u001b[0m Trial 105 finished with value: -114.6 and parameters: {'batch_size': 512, 'n_steps': 16, 'gamma': 0.9999, 'learning_rate': 1.3159243427911799e-05, 'ent_coef': 9.231857096939181e-07, 'clip_range': 0.2, 'n_epochs': 10, 'gae_lambda': 0.9, 'max_grad_norm': 0.3, 'vf_coef': 0.7223274506104079, 'net_arch': 'medium', 'activation_fn': 'tanh'}. Best is trial 53 with value: -95.8.\u001b[0m\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "\u001b[32m[I 2022-01-23 00:08:52,173]\u001b[0m Trial 107 pruned. \u001b[0m\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "\u001b[32m[I 2022-01-23 00:12:31,915]\u001b[0m Trial 106 finished with value: -108.4 and parameters: {'batch_size': 8, 'n_steps': 64, 'gamma': 0.999, 'learning_rate': 1.207028249546154e-05, 'ent_coef': 0.005077181223808435, 'clip_range': 0.1, 'n_epochs': 10, 'gae_lambda': 0.9, 'max_grad_norm': 0.6, 'vf_coef': 0.9447954951411048, 'net_arch': 'small', 'activation_fn': 'tanh'}. Best is trial 53 with value: -95.8.\u001b[0m\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "\u001b[32m[I 2022-01-23 00:14:12,335]\u001b[0m Trial 108 finished with value: -117.4 and parameters: {'batch_size': 8, 'n_steps': 64, 'gamma': 0.999, 'learning_rate': 0.00012541476607126284, 'ent_coef': 3.439670264759347e-08, 'clip_range': 0.3, 'n_epochs': 5, 'gae_lambda': 0.9, 'max_grad_norm': 0.5, 'vf_coef': 0.6773840625258538, 'net_arch': 'small', 'activation_fn': 'tanh'}. Best is trial 53 with value: -95.8.\u001b[0m\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "\u001b[32m[I 2022-01-23 00:14:18,800]\u001b[0m Trial 109 finished with value: -117.8 and parameters: {'batch_size': 128, 'n_steps': 64, 'gamma': 0.999, 'learning_rate': 0.00029944653189479024, 'ent_coef': 2.3672624997137197e-05, 'clip_range': 0.1, 'n_epochs': 10, 'gae_lambda': 0.9, 'max_grad_norm': 0.8, 'vf_coef': 0.9824206581683228, 'net_arch': 'small', 'activation_fn': 'tanh'}. Best is trial 53 with value: -95.8.\u001b[0m\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "\u001b[32m[I 2022-01-23 00:15:54,817]\u001b[0m Trial 110 finished with value: -113.2 and parameters: {'batch_size': 128, 'n_steps': 64, 'gamma': 0.999, 'learning_rate': 4.827988019585715e-05, 'ent_coef': 0.08860993330367702, 'clip_range': 0.1, 'n_epochs': 10, 'gae_lambda': 0.9, 'max_grad_norm': 0.6, 'vf_coef': 0.9051244516713216, 'net_arch': 'small', 'activation_fn': 'tanh'}. Best is trial 53 with value: -95.8.\u001b[0m\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "\u001b[32m[I 2022-01-23 00:16:43,249]\u001b[0m Trial 112 pruned. \u001b[0m\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "\u001b[32m[I 2022-01-23 00:19:26,244]\u001b[0m Trial 113 pruned. \u001b[0m\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "\u001b[32m[I 2022-01-23 00:23:31,265]\u001b[0m Trial 111 finished with value: -108.4 and parameters: {'batch_size': 8, 'n_steps': 64, 'gamma': 0.999, 'learning_rate': 1.8947173227315762e-05, 'ent_coef': 0.0014279093923107863, 'clip_range': 0.1, 'n_epochs': 10, 'gae_lambda': 0.9, 'max_grad_norm': 0.6, 'vf_coef': 0.8776523489879455, 'net_arch': 'small', 'activation_fn': 'tanh'}. Best is trial 53 with value: -95.8.\u001b[0m\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "\u001b[32m[I 2022-01-23 00:26:03,891]\u001b[0m Trial 115 pruned. \u001b[0m\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "\u001b[32m[I 2022-01-23 00:28:32,610]\u001b[0m Trial 116 pruned. \u001b[0m\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "\u001b[32m[I 2022-01-23 00:29:24,365]\u001b[0m Trial 117 pruned. \u001b[0m\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "\u001b[32m[I 2022-01-23 00:34:14,270]\u001b[0m Trial 118 pruned. \u001b[0m\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "\u001b[32m[I 2022-01-23 00:34:53,216]\u001b[0m Trial 119 pruned. \u001b[0m\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "\u001b[32m[I 2022-01-23 00:35:13,056]\u001b[0m Trial 120 pruned. \u001b[0m\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "\u001b[32m[I 2022-01-23 00:35:26,198]\u001b[0m Trial 121 pruned. \u001b[0m\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "\u001b[32m[I 2022-01-23 00:37:29,657]\u001b[0m Trial 122 finished with value: -109.0 and parameters: {'batch_size': 256, 'n_steps': 64, 'gamma': 0.999, 'learning_rate': 0.0014965002748653603, 'ent_coef': 8.082739533155007e-08, 'clip_range': 0.2, 'n_epochs': 10, 'gae_lambda': 0.9, 'max_grad_norm': 0.5, 'vf_coef': 0.8506426375465775, 'net_arch': 'medium', 'activation_fn': 'tanh'}. Best is trial 53 with value: -95.8.\u001b[0m\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "\u001b[32m[I 2022-01-23 00:40:01,050]\u001b[0m Trial 123 pruned. \u001b[0m\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "\u001b[32m[I 2022-01-23 00:42:01,194]\u001b[0m Trial 114 finished with value: -133.0 and parameters: {'batch_size': 256, 'n_steps': 8, 'gamma': 0.999, 'learning_rate': 1.3641264475773342e-05, 'ent_coef': 1.0323864909403522e-06, 'clip_range': 0.2, 'n_epochs': 20, 'gae_lambda': 0.9, 'max_grad_norm': 0.5, 'vf_coef': 0.9450228775940481, 'net_arch': 'medium', 'activation_fn': 'tanh'}. Best is trial 53 with value: -95.8.\u001b[0m\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "\u001b[32m[I 2022-01-23 00:42:19,527]\u001b[0m Trial 125 pruned. \u001b[0m\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "\u001b[32m[I 2022-01-23 00:44:50,967]\u001b[0m Trial 126 pruned. \u001b[0m\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "\u001b[32m[I 2022-01-23 00:45:13,029]\u001b[0m Trial 127 pruned. \u001b[0m\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "\u001b[32m[I 2022-01-23 00:51:36,268]\u001b[0m Trial 124 finished with value: -115.0 and parameters: {'batch_size': 256, 'n_steps': 8, 'gamma': 0.999, 'learning_rate': 8.752447537442577e-05, 'ent_coef': 1.7626634023416175e-08, 'clip_range': 0.1, 'n_epochs': 10, 'gae_lambda': 0.9, 'max_grad_norm': 0.5, 'vf_coef': 0.8901176151992836, 'net_arch': 'medium', 'activation_fn': 'tanh'}. Best is trial 53 with value: -95.8.\u001b[0m\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "\u001b[32m[I 2022-01-23 00:51:49,539]\u001b[0m Trial 129 pruned. \u001b[0m\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "\u001b[32m[I 2022-01-23 00:53:13,872]\u001b[0m Trial 130 pruned. \u001b[0m\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "\u001b[32m[I 2022-01-23 00:53:50,329]\u001b[0m Trial 128 finished with value: -144.2 and parameters: {'batch_size': 8, 'n_steps': 8, 'gamma': 0.995, 'learning_rate': 5.089339806552488e-05, 'ent_coef': 1.3350858289896138e-05, 'clip_range': 0.1, 'n_epochs': 10, 'gae_lambda': 0.9, 'max_grad_norm': 0.6, 'vf_coef': 0.7586902415232997, 'net_arch': 'small', 'activation_fn': 'tanh'}. Best is trial 53 with value: -95.8.\u001b[0m\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "\u001b[32m[I 2022-01-23 00:54:37,667]\u001b[0m Trial 132 pruned. \u001b[0m\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "\u001b[32m[I 2022-01-23 00:55:34,511]\u001b[0m Trial 131 finished with value: -95.8 and parameters: {'batch_size': 256, 'n_steps': 64, 'gamma': 0.999, 'learning_rate': 8.861907632697757e-05, 'ent_coef': 8.651066140360053e-07, 'clip_range': 0.2, 'n_epochs': 10, 'gae_lambda': 0.9, 'max_grad_norm': 0.5, 'vf_coef': 0.6582485426911664, 'net_arch': 'medium', 'activation_fn': 'tanh'}. Best is trial 53 with value: -95.8.\u001b[0m\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "\u001b[32m[I 2022-01-23 00:56:53,542]\u001b[0m Trial 134 pruned. \u001b[0m\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "\u001b[32m[I 2022-01-23 00:57:08,156]\u001b[0m Trial 135 pruned. \u001b[0m\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "\u001b[32m[I 2022-01-23 00:57:29,826]\u001b[0m Trial 133 pruned. \u001b[0m\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "\u001b[32m[I 2022-01-23 01:00:19,455]\u001b[0m Trial 137 finished with value: -116.6 and parameters: {'batch_size': 16, 'n_steps': 64, 'gamma': 0.98, 'learning_rate': 1.804566976571904e-05, 'ent_coef': 3.253974294107134e-07, 'clip_range': 0.2, 'n_epochs': 5, 'gae_lambda': 0.9, 'max_grad_norm': 0.8, 'vf_coef': 0.905153828352556, 'net_arch': 'small', 'activation_fn': 'tanh'}. Best is trial 53 with value: -95.8.\u001b[0m\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "\u001b[32m[I 2022-01-23 01:00:41,769]\u001b[0m Trial 138 pruned. \u001b[0m\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "\u001b[32m[I 2022-01-23 01:06:55,570]\u001b[0m Trial 136 finished with value: -95.6 and parameters: {'batch_size': 8, 'n_steps': 64, 'gamma': 0.9999, 'learning_rate': 0.00010073854132892985, 'ent_coef': 3.4510904359284934e-06, 'clip_range': 0.3, 'n_epochs': 10, 'gae_lambda': 1.0, 'max_grad_norm': 5, 'vf_coef': 0.7012594079000993, 'net_arch': 'small', 'activation_fn': 'relu'}. Best is trial 136 with value: -95.6.\u001b[0m\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "\u001b[32m[I 2022-01-23 01:16:53,938]\u001b[0m Trial 140 finished with value: -98.0 and parameters: {'batch_size': 8, 'n_steps': 64, 'gamma': 0.9999, 'learning_rate': 8.175319387439064e-05, 'ent_coef': 5.5302618878813114e-08, 'clip_range': 0.3, 'n_epochs': 10, 'gae_lambda': 0.8, 'max_grad_norm': 0.5, 'vf_coef': 0.6217089364068644, 'net_arch': 'small', 'activation_fn': 'tanh'}. Best is trial 136 with value: -95.6.\u001b[0m\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "\u001b[32m[I 2022-01-23 01:19:22,023]\u001b[0m Trial 141 pruned. \u001b[0m\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "\u001b[32m[I 2022-01-23 01:19:51,196]\u001b[0m Trial 139 finished with value: -96.2 and parameters: {'batch_size': 8, 'n_steps': 8, 'gamma': 0.999, 'learning_rate': 7.936234405498473e-05, 'ent_coef': 0.012863319967865208, 'clip_range': 0.1, 'n_epochs': 20, 'gae_lambda': 0.9, 'max_grad_norm': 0.9, 'vf_coef': 0.921241150544183, 'net_arch': 'small', 'activation_fn': 'tanh'}. Best is trial 136 with value: -95.6.\u001b[0m\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n",
            "\u001b[32m[I 2022-01-23 01:22:11,911]\u001b[0m Trial 142 pruned. \u001b[0m\n",
            "Normalization activated: {'gamma': 0.99}\n",
            "Normalization activated: {'gamma': 0.99, 'norm_reward': False}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVm9QPNVwKXN"
      },
      "source": [
        "### Record  a Video"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPyfQxD5z26J"
      },
      "source": [
        "# Set up display; otherwise rendering will fail\n",
        "import os\n",
        "os.system(\"Xvfb :1 -screen 0 1024x768x24 &\")\n",
        "os.environ['DISPLAY'] = ':1'"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ip3AauLzwNGP"
      },
      "source": [
        "!python -m utils.record_video --algo a2c --env CartPole-v1 --exp-id 0 -f logs/ -n 1000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBuUfnzI8DN6"
      },
      "source": [
        "### Display the video"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZC3OTfpf8CXu"
      },
      "source": [
        "import base64\n",
        "from pathlib import Path\n",
        "\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "def show_videos(video_path='', prefix=''):\n",
        "  \"\"\"\n",
        "  Taken from https://github.com/eleurent/highway-env\n",
        "\n",
        "  :param video_path: (str) Path to the folder containing videos\n",
        "  :param prefix: (str) Filter the video, showing only the only starting with this prefix\n",
        "  \"\"\"\n",
        "  html = []\n",
        "  for mp4 in Path(video_path).glob(\"{}*.mp4\".format(prefix)):\n",
        "      video_b64 = base64.b64encode(mp4.read_bytes())\n",
        "      html.append('''<video alt=\"{}\" autoplay \n",
        "                    loop controls style=\"height: 400px;\">\n",
        "                    <source src=\"data:video/mp4;base64,{}\" type=\"video/mp4\" />\n",
        "                </video>'''.format(mp4, video_b64.decode('ascii')))\n",
        "  ipythondisplay.display(ipythondisplay.HTML(data=\"<br>\".join(html)))"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKOjFuwK9HI0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "24a3a473-5c89-4e6a-c527-80861fb8b421"
      },
      "source": [
        "show_videos(video_path='logs/videos/', prefix='a2c')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RjdpP0HE8D2p"
      },
      "source": [
        "### Continue Training\n",
        "\n",
        "Here, we will continue training of the previous model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgMZQJJF6u1C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de114459-4d02-44c5-ddab-4d32328c4533"
      },
      "source": [
        "!python train.py --algo a2c --env CartPole-v1 --n-timesteps 50000 -i logs/a2c/CartPole-v1_1/CartPole-v1.zip"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "python3: can't open file 'train.py': [Errno 2] No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GSaoyiAE8cVj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b747f029-1516-4852-a27d-1eebd4c2485e"
      },
      "source": [
        "!python enjoy.py --algo a2c --env CartPole-v1 --no-render --n-timesteps 1000 --folder logs/"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "python3: can't open file 'enjoy.py': [Errno 2] No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jL9u4I1H-48O"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}