{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PPO_colab.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/decoderkurt/HUF_RL_2022/blob/main/22/PPO_colab_robo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7JowRQEGGKQ"
      },
      "source": [
        "################################################################################\n",
        "> # **Clone GitHub repository**\n",
        "################################################################################"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IyGzuEMQF6sJ",
        "outputId": "2749874b-6274-4695-f2cc-f71187fe8a87",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "\n",
        "################# Clone repository from github to colab session ################\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "run this section if you want to clone all the preTrained networks, logs, graph figures, gifs \n",
        "from the GitHub repository to this colab session\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "!git clone https://github.com/nikhilbarhate99/PPO-PyTorch\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================================================\n",
            "Cloning into 'PPO-PyTorch'...\n",
            "remote: Enumerating objects: 350, done.\u001b[K\n",
            "remote: Counting objects: 100% (141/141), done.\u001b[K\n",
            "remote: Compressing objects: 100% (132/132), done.\u001b[K\n",
            "remote: Total 350 (delta 43), reused 91 (delta 9), pack-reused 209\u001b[K\n",
            "Receiving objects: 100% (350/350), 12.35 MiB | 16.51 MiB/s, done.\n",
            "Resolving deltas: 100% (134/134), done.\n",
            "============================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mrn6rpJpF8Sc",
        "outputId": "d991237c-4c6e-49c1-a38f-f6f2fa502496",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "\n",
        "\"\"\"\n",
        "\n",
        "run this section if you want to copy all files and folders from cloned folder (PPO-PyTorch)\n",
        "to current directory (/content/ or ./)\n",
        "\n",
        "So you can load preTrained networks and log files without changing any paths\n",
        "\n",
        "**  This will overwrite any saved networks, logs, graph figures, or gifs \n",
        "    that are created in this session before copying having the same name (or number)\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "!cp -rv ./PPO-PyTorch/* ./\n",
        "\n",
        "print(\"============================================================================================\")\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================================================\n",
            "cp: cannot stat './PPO-PyTorch/*': No such file or directory\n",
            "============================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-7AbGA2F8Ut",
        "outputId": "8cd494ef-2bd5-4407-95b1-a604558108cd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "\n",
        "\"\"\"\n",
        "\n",
        "run this section if you want to delete original cloned folder and the cloned ipynb file\n",
        "(after you have copied its contents to current directory)\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "# delete original cloned folder\n",
        "!rm -r ./PPO-PyTorch\n",
        "\n",
        "# delete cloned ipynb file\n",
        "!rm ./PPO_colab.ipynb\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================================================\n",
            "rm: cannot remove './PPO-PyTorch': No such file or directory\n",
            "rm: cannot remove './PPO_colab.ipynb': No such file or directory\n",
            "============================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4VJcUT2GlJz"
      },
      "source": [
        "################################################################################\n",
        "> # **Install Dependencies**\n",
        "################################################################################"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbpSQTflGlAr",
        "outputId": "15da886f-dc89-493b-ce7e-ac857b4a97a6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "\n",
        "\n",
        "############ install compatible version of OpenAI roboschool and gym ###########\n",
        "\n",
        "!pip install roboschool==1.0.48 gym==0.15.4\n",
        "\n",
        "!pip install box2d-py\n",
        "\n",
        "!pip install pybullet\n",
        "\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: roboschool==1.0.48 in /usr/local/lib/python3.7/dist-packages (1.0.48)\n",
            "Requirement already satisfied: gym==0.15.4 in /usr/local/lib/python3.7/dist-packages (0.15.4)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from gym==0.15.4) (4.1.2.30)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gym==0.15.4) (1.15.0)\n",
            "Requirement already satisfied: pyglet<=1.3.2,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym==0.15.4) (1.3.2)\n",
            "Requirement already satisfied: cloudpickle~=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym==0.15.4) (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym==0.15.4) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym==0.15.4) (1.4.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.3.2,>=1.2.0->gym==0.15.4) (0.16.0)\n",
            "Requirement already satisfied: box2d-py in /usr/local/lib/python3.7/dist-packages (2.3.8)\n",
            "Requirement already satisfied: pybullet in /usr/local/lib/python3.7/dist-packages (3.2.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzZairIiGQ11"
      },
      "source": [
        "################################################################################\n",
        "> # **Introduction**\n",
        "> The notebook is divided into 5 major parts : \n",
        "\n",
        "*   **Part I** : define actor-critic network and PPO algorithm\n",
        "*   **Part II** : train PPO algorithm and save network weights and log files\n",
        "*   **Part III** : load (preTrained) network weights and test PPO algorithm\n",
        "*   **Part IV** : load log files and plot graphs\n",
        "*   **Part V** : install xvbf, load (preTrained) network weights and save images for gif and then generate gif\n",
        "\n",
        "################################################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s37cJXAYGrTY"
      },
      "source": [
        "################################################################################\n",
        "> # **Part - I**\n",
        "\n",
        "*   define actor critic networks\n",
        "*   define PPO algorithm\n",
        "\n",
        "################################################################################"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UT6VUBg-F8Zm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e400b6f-df31-4827-d0b7-30d1f38f3f3a"
      },
      "source": [
        "\n",
        "\n",
        "############################### Import libraries ###############################\n",
        "\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.distributions import MultivariateNormal\n",
        "from torch.distributions import Categorical\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import gym\n",
        "import roboschool\n",
        "import pybullet_envs\n",
        "\n",
        "\n",
        "################################## set device ##################################\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "\n",
        "# set device to cpu or cuda\n",
        "device = torch.device('cpu')\n",
        "\n",
        "if(torch.cuda.is_available()): \n",
        "    device = torch.device('cuda:0') \n",
        "    torch.cuda.empty_cache()\n",
        "    print(\"Device set to : \" + str(torch.cuda.get_device_name(device)))\n",
        "else:\n",
        "    print(\"Device set to : cpu\")\n",
        "    \n",
        "print(\"============================================================================================\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "################################## PPO Policy ##################################\n",
        "\n",
        "\n",
        "class RolloutBuffer:\n",
        "    def __init__(self):\n",
        "        self.actions = []\n",
        "        self.states = []\n",
        "        self.logprobs = []\n",
        "        self.rewards = []\n",
        "        self.is_terminals = []\n",
        "    \n",
        "\n",
        "    def clear(self):\n",
        "        del self.actions[:]\n",
        "        del self.states[:]\n",
        "        del self.logprobs[:]\n",
        "        del self.rewards[:]\n",
        "        del self.is_terminals[:]\n",
        "\n",
        "\n",
        "class ActorCritic(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, has_continuous_action_space, action_std_init):\n",
        "        super(ActorCritic, self).__init__()\n",
        "\n",
        "        self.has_continuous_action_space = has_continuous_action_space\n",
        "\n",
        "        if has_continuous_action_space:\n",
        "            self.action_dim = action_dim\n",
        "            self.action_var = torch.full((action_dim,), action_std_init * action_std_init).to(device)\n",
        "\n",
        "        # actor\n",
        "        if has_continuous_action_space :\n",
        "            self.actor = nn.Sequential(\n",
        "                            nn.Linear(state_dim, 64),\n",
        "                            nn.Tanh(),\n",
        "                            nn.Linear(64, 64),\n",
        "                            nn.Tanh(),\n",
        "                            nn.Linear(64, action_dim),\n",
        "                            nn.Tanh()\n",
        "                        )\n",
        "        else:\n",
        "            self.actor = nn.Sequential(\n",
        "                            nn.Linear(state_dim, 64),\n",
        "                            nn.Tanh(),\n",
        "                            nn.Linear(64, 64),\n",
        "                            nn.Tanh(),\n",
        "                            nn.Linear(64, action_dim),\n",
        "                            nn.Softmax(dim=-1)\n",
        "                        )\n",
        "\n",
        "        \n",
        "        # critic\n",
        "        self.critic = nn.Sequential(\n",
        "                        nn.Linear(state_dim, 64),\n",
        "                        nn.Tanh(),\n",
        "                        nn.Linear(64, 64),\n",
        "                        nn.Tanh(),\n",
        "                        nn.Linear(64, 1)\n",
        "                    )\n",
        "        \n",
        "    def set_action_std(self, new_action_std):\n",
        "\n",
        "        if self.has_continuous_action_space:\n",
        "            self.action_var = torch.full((self.action_dim,), new_action_std * new_action_std).to(device)\n",
        "        else:\n",
        "            print(\"--------------------------------------------------------------------------------------------\")\n",
        "            print(\"WARNING : Calling ActorCritic::set_action_std() on discrete action space policy\")\n",
        "            print(\"--------------------------------------------------------------------------------------------\")\n",
        "\n",
        "\n",
        "    def forward(self):\n",
        "        raise NotImplementedError\n",
        "    \n",
        "\n",
        "    def act(self, state):\n",
        "\n",
        "        if self.has_continuous_action_space:\n",
        "            action_mean = self.actor(state)\n",
        "            cov_mat = torch.diag(self.action_var).unsqueeze(dim=0)\n",
        "            dist = MultivariateNormal(action_mean, cov_mat)\n",
        "        else:\n",
        "            action_probs = self.actor(state)\n",
        "            dist = Categorical(action_probs)\n",
        "\n",
        "        action = dist.sample()\n",
        "        action_logprob = dist.log_prob(action)\n",
        "        \n",
        "        return action.detach(), action_logprob.detach()\n",
        "    \n",
        "\n",
        "    def evaluate(self, state, action):\n",
        "\n",
        "        if self.has_continuous_action_space:\n",
        "            action_mean = self.actor(state)\n",
        "            action_var = self.action_var.expand_as(action_mean)\n",
        "            cov_mat = torch.diag_embed(action_var).to(device)\n",
        "            dist = MultivariateNormal(action_mean, cov_mat)\n",
        "            \n",
        "            # for single action continuous environments\n",
        "            if self.action_dim == 1:\n",
        "                action = action.reshape(-1, self.action_dim)\n",
        "\n",
        "        else:\n",
        "            action_probs = self.actor(state)\n",
        "            dist = Categorical(action_probs)\n",
        "\n",
        "        action_logprobs = dist.log_prob(action)\n",
        "        dist_entropy = dist.entropy()\n",
        "        state_values = self.critic(state)\n",
        "        \n",
        "        return action_logprobs, state_values, dist_entropy\n",
        "\n",
        "\n",
        "class PPO:\n",
        "    def __init__(self, state_dim, action_dim, lr_actor, lr_critic, gamma, K_epochs, eps_clip, has_continuous_action_space, action_std_init=0.6):\n",
        "\n",
        "        self.has_continuous_action_space = has_continuous_action_space\n",
        "\n",
        "        if has_continuous_action_space:\n",
        "            self.action_std = action_std_init\n",
        "\n",
        "        self.gamma = gamma\n",
        "        self.eps_clip = eps_clip\n",
        "        self.K_epochs = K_epochs\n",
        "        \n",
        "        self.buffer = RolloutBuffer()\n",
        "\n",
        "        self.policy = ActorCritic(state_dim, action_dim, has_continuous_action_space, action_std_init).to(device)\n",
        "        self.optimizer = torch.optim.Adam([\n",
        "                        {'params': self.policy.actor.parameters(), 'lr': lr_actor},\n",
        "                        {'params': self.policy.critic.parameters(), 'lr': lr_critic}\n",
        "                    ])\n",
        "\n",
        "        self.policy_old = ActorCritic(state_dim, action_dim, has_continuous_action_space, action_std_init).to(device)\n",
        "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
        "        \n",
        "        self.MseLoss = nn.MSELoss()\n",
        "\n",
        "\n",
        "    def set_action_std(self, new_action_std):\n",
        "        \n",
        "        if self.has_continuous_action_space:\n",
        "            self.action_std = new_action_std\n",
        "            self.policy.set_action_std(new_action_std)\n",
        "            self.policy_old.set_action_std(new_action_std)\n",
        "        \n",
        "        else:\n",
        "            print(\"--------------------------------------------------------------------------------------------\")\n",
        "            print(\"WARNING : Calling PPO::set_action_std() on discrete action space policy\")\n",
        "            print(\"--------------------------------------------------------------------------------------------\")\n",
        "\n",
        "\n",
        "    def decay_action_std(self, action_std_decay_rate, min_action_std):\n",
        "        print(\"--------------------------------------------------------------------------------------------\")\n",
        "\n",
        "        if self.has_continuous_action_space:\n",
        "            self.action_std = self.action_std - action_std_decay_rate\n",
        "            self.action_std = round(self.action_std, 4)\n",
        "            if (self.action_std <= min_action_std):\n",
        "                self.action_std = min_action_std\n",
        "                print(\"setting actor output action_std to min_action_std : \", self.action_std)\n",
        "            else:\n",
        "                print(\"setting actor output action_std to : \", self.action_std)\n",
        "            self.set_action_std(self.action_std)\n",
        "\n",
        "        else:\n",
        "            print(\"WARNING : Calling PPO::decay_action_std() on discrete action space policy\")\n",
        "\n",
        "        print(\"--------------------------------------------------------------------------------------------\")\n",
        "\n",
        "\n",
        "    def select_action(self, state):\n",
        "\n",
        "        if self.has_continuous_action_space:\n",
        "            with torch.no_grad():\n",
        "                state = torch.FloatTensor(state).to(device)\n",
        "                action, action_logprob = self.policy_old.act(state)\n",
        "\n",
        "            self.buffer.states.append(state)\n",
        "            self.buffer.actions.append(action)\n",
        "            self.buffer.logprobs.append(action_logprob)\n",
        "\n",
        "            return action.detach().cpu().numpy().flatten()\n",
        "\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                state = torch.FloatTensor(state).to(device)\n",
        "                action, action_logprob = self.policy_old.act(state)\n",
        "            \n",
        "            self.buffer.states.append(state)\n",
        "            self.buffer.actions.append(action)\n",
        "            self.buffer.logprobs.append(action_logprob)\n",
        "\n",
        "            return action.item()\n",
        "\n",
        "\n",
        "    def update(self):\n",
        "\n",
        "        # Monte Carlo estimate of returns\n",
        "        rewards = []\n",
        "        discounted_reward = 0\n",
        "        for reward, is_terminal in zip(reversed(self.buffer.rewards), reversed(self.buffer.is_terminals)):\n",
        "            if is_terminal:\n",
        "                discounted_reward = 0\n",
        "            discounted_reward = reward + (self.gamma * discounted_reward)\n",
        "            rewards.insert(0, discounted_reward)\n",
        "            \n",
        "        # Normalizing the rewards\n",
        "        rewards = torch.tensor(rewards, dtype=torch.float32).to(device)\n",
        "        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-7)\n",
        "\n",
        "        # convert list to tensor\n",
        "        old_states = torch.squeeze(torch.stack(self.buffer.states, dim=0)).detach().to(device)\n",
        "        old_actions = torch.squeeze(torch.stack(self.buffer.actions, dim=0)).detach().to(device)\n",
        "        old_logprobs = torch.squeeze(torch.stack(self.buffer.logprobs, dim=0)).detach().to(device)\n",
        "\n",
        "        \n",
        "        # Optimize policy for K epochs\n",
        "        for _ in range(self.K_epochs):\n",
        "\n",
        "            # Evaluating old actions and values\n",
        "            logprobs, state_values, dist_entropy = self.policy.evaluate(old_states, old_actions)\n",
        "\n",
        "            # match state_values tensor dimensions with rewards tensor\n",
        "            state_values = torch.squeeze(state_values)\n",
        "            \n",
        "            # Finding the ratio (pi_theta / pi_theta__old)\n",
        "            ratios = torch.exp(logprobs - old_logprobs.detach())\n",
        "\n",
        "            # Finding Surrogate Loss\n",
        "            advantages = rewards - state_values.detach()   \n",
        "            surr1 = ratios * advantages\n",
        "            surr2 = torch.clamp(ratios, 1-self.eps_clip, 1+self.eps_clip) * advantages\n",
        "\n",
        "            # final loss of clipped objective PPO\n",
        "            loss = -torch.min(surr1, surr2) + 0.5*self.MseLoss(state_values, rewards) - 0.01*dist_entropy\n",
        "            \n",
        "            # take gradient step\n",
        "            self.optimizer.zero_grad()\n",
        "            loss.mean().backward()\n",
        "            self.optimizer.step()\n",
        "            \n",
        "        # Copy new weights into old policy\n",
        "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
        "\n",
        "        # clear buffer\n",
        "        self.buffer.clear()\n",
        "    \n",
        "    \n",
        "    def save(self, checkpoint_path):\n",
        "        torch.save(self.policy_old.state_dict(), checkpoint_path)\n",
        "   \n",
        "\n",
        "    def load(self, checkpoint_path):\n",
        "        self.policy_old.load_state_dict(torch.load(checkpoint_path, map_location=lambda storage, loc: storage))\n",
        "        self.policy.load_state_dict(torch.load(checkpoint_path, map_location=lambda storage, loc: storage))\n",
        "        \n",
        "        \n",
        "       \n",
        "\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================================================\n",
            "Device set to : cpu\n",
            "============================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xCb_EyxF8cF"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "################################# End of Part I ################################\n",
        "\n",
        "\n"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yr-ZjT_CGyEi"
      },
      "source": [
        "################################################################################\n",
        "> # **Part - II**\n",
        "\n",
        "*   train PPO algorithm on environments\n",
        "*   save preTrained networks weights and log files\n",
        "\n",
        "################################################################################"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YY1-DzVCF8eh",
        "outputId": "83ecd0eb-05a1-4da5-ef0e-253d2a8cf70f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "\n",
        "################################### Training ###################################\n",
        "\n",
        "\n",
        "####### initialize environment hyperparameters ######\n",
        "\n",
        "env_name = \"RoboschoolWalker2d-v1\"\n",
        "has_continuous_action_space = True\n",
        "max_ep_len = 1000           # max timesteps in one episode\n",
        "action_std = 0.1            # set same std for action distribution which was used while saving\n",
        "\n",
        "max_ep_len = 400                    # max timesteps in one episode\n",
        "max_training_timesteps = int(1e5)   # break training loop if timeteps > max_training_timesteps\n",
        "\n",
        "print_freq = max_ep_len * 4     # print avg reward in the interval (in num timesteps)\n",
        "log_freq = max_ep_len * 2       # log avg reward in the interval (in num timesteps)\n",
        "save_model_freq = int(2e4)      # save model frequency (in num timesteps)\n",
        "\n",
        "action_std = 0.6                    # starting std for action distribution (Multivariate Normal)\n",
        "action_std_decay_rate = 0.05        # linearly decay action_std (action_std = action_std - action_std_decay_rate)\n",
        "min_action_std = 0.1                # minimum action_std (stop decay after action_std <= min_action_std)\n",
        "action_std_decay_freq = int(2.5e5)  # action_std decay frequency (in num timesteps)\n",
        "\n",
        "#####################################################\n",
        "\n",
        "\n",
        "## Note : print/log frequencies should be > than max_ep_len\n",
        "\n",
        "\n",
        "################ PPO hyperparameters ################\n",
        "\n",
        "\n",
        "update_timestep = max_ep_len * 4      # update policy every n timesteps\n",
        "K_epochs = 40               # update policy for K epochs\n",
        "eps_clip = 0.2              # clip parameter for PPO\n",
        "gamma = 0.99                # discount factor\n",
        "\n",
        "lr_actor = 0.0003       # learning rate for actor network\n",
        "lr_critic = 0.001       # learning rate for critic network\n",
        "\n",
        "random_seed = 0         # set random seed if required (0 = no random seed)\n",
        "\n",
        "#####################################################\n",
        "\n",
        "\n",
        "\n",
        "print(\"training environment name : \" + env_name)\n",
        "\n",
        "env = gym.make(env_name)\n",
        "\n",
        "# state space dimension\n",
        "state_dim = env.observation_space.shape[0]\n",
        "\n",
        "# action space dimension\n",
        "if has_continuous_action_space:\n",
        "    action_dim = env.action_space.shape[0]\n",
        "else:\n",
        "    action_dim = env.action_space.n\n",
        "\n",
        "\n",
        "\n",
        "###################### logging ######################\n",
        "\n",
        "#### log files for multiple runs are NOT overwritten\n",
        "\n",
        "log_dir = \"PPO_logs\"\n",
        "if not os.path.exists(log_dir):\n",
        "      os.makedirs(log_dir)\n",
        "\n",
        "log_dir = log_dir + '/' + env_name + '/'\n",
        "if not os.path.exists(log_dir):\n",
        "      os.makedirs(log_dir)\n",
        "\n",
        "\n",
        "#### get number of log files in log directory\n",
        "run_num = 0\n",
        "current_num_files = next(os.walk(log_dir))[2]\n",
        "run_num = len(current_num_files)\n",
        "\n",
        "\n",
        "#### create new log file for each run \n",
        "log_f_name = log_dir + '/PPO_' + env_name + \"_log_\" + str(run_num) + \".csv\"\n",
        "\n",
        "print(\"current logging run number for \" + env_name + \" : \", run_num)\n",
        "print(\"logging at : \" + log_f_name)\n",
        "\n",
        "#####################################################\n",
        "\n",
        "\n",
        "################### checkpointing ###################\n",
        "\n",
        "run_num_pretrained = 0      #### change this to prevent overwriting weights in same env_name folder\n",
        "\n",
        "directory = \"PPO_preTrained\"\n",
        "if not os.path.exists(directory):\n",
        "      os.makedirs(directory)\n",
        "\n",
        "directory = directory + '/' + env_name + '/'\n",
        "if not os.path.exists(directory):\n",
        "      os.makedirs(directory)\n",
        "\n",
        "\n",
        "checkpoint_path = directory + \"PPO_{}_{}_{}.pth\".format(env_name, random_seed, run_num_pretrained)\n",
        "print(\"save checkpoint path : \" + checkpoint_path)\n",
        "\n",
        "#####################################################\n",
        "\n",
        "\n",
        "############# print all hyperparameters #############\n",
        "\n",
        "print(\"--------------------------------------------------------------------------------------------\")\n",
        "\n",
        "print(\"max training timesteps : \", max_training_timesteps)\n",
        "print(\"max timesteps per episode : \", max_ep_len)\n",
        "\n",
        "print(\"model saving frequency : \" + str(save_model_freq) + \" timesteps\")\n",
        "print(\"log frequency : \" + str(log_freq) + \" timesteps\")\n",
        "print(\"printing average reward over episodes in last : \" + str(print_freq) + \" timesteps\")\n",
        "\n",
        "print(\"--------------------------------------------------------------------------------------------\")\n",
        "\n",
        "print(\"state space dimension : \", state_dim)\n",
        "print(\"action space dimension : \", action_dim)\n",
        "\n",
        "print(\"--------------------------------------------------------------------------------------------\")\n",
        "\n",
        "if has_continuous_action_space:\n",
        "    print(\"Initializing a continuous action space policy\")\n",
        "    print(\"--------------------------------------------------------------------------------------------\")\n",
        "    print(\"starting std of action distribution : \", action_std)\n",
        "    print(\"decay rate of std of action distribution : \", action_std_decay_rate)\n",
        "    print(\"minimum std of action distribution : \", min_action_std)\n",
        "    print(\"decay frequency of std of action distribution : \" + str(action_std_decay_freq) + \" timesteps\")\n",
        "\n",
        "else:\n",
        "    print(\"Initializing a discrete action space policy\")\n",
        "\n",
        "print(\"--------------------------------------------------------------------------------------------\")\n",
        "\n",
        "print(\"PPO update frequency : \" + str(update_timestep) + \" timesteps\") \n",
        "print(\"PPO K epochs : \", K_epochs)\n",
        "print(\"PPO epsilon clip : \", eps_clip)\n",
        "print(\"discount factor (gamma) : \", gamma)\n",
        "\n",
        "print(\"--------------------------------------------------------------------------------------------\")\n",
        "\n",
        "print(\"optimizer learning rate actor : \", lr_actor)\n",
        "print(\"optimizer learning rate critic : \", lr_critic)\n",
        "\n",
        "if random_seed:\n",
        "    print(\"--------------------------------------------------------------------------------------------\")\n",
        "    print(\"setting random seed to \", random_seed)\n",
        "    torch.manual_seed(random_seed)\n",
        "    env.seed(random_seed)\n",
        "    np.random.seed(random_seed)\n",
        "\n",
        "#####################################################\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "################# training procedure ################\n",
        "\n",
        "# initialize a PPO agent\n",
        "ppo_agent = PPO(state_dim, action_dim, lr_actor, lr_critic, gamma, K_epochs, eps_clip, has_continuous_action_space, action_std)\n",
        "\n",
        "\n",
        "# track total training time\n",
        "start_time = datetime.now().replace(microsecond=0)\n",
        "print(\"Started training at (GMT) : \", start_time)\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "\n",
        "# logging file\n",
        "log_f = open(log_f_name,\"w+\")\n",
        "log_f.write('episode,timestep,reward\\n')\n",
        "\n",
        "\n",
        "# printing and logging variables\n",
        "print_running_reward = 0\n",
        "print_running_episodes = 0\n",
        "\n",
        "log_running_reward = 0\n",
        "log_running_episodes = 0\n",
        "\n",
        "time_step = 0\n",
        "i_episode = 0\n",
        "\n",
        "\n",
        "# training loop\n",
        "while time_step <= max_training_timesteps:\n",
        "    \n",
        "    state = env.reset()\n",
        "    current_ep_reward = 0\n",
        "\n",
        "    for t in range(1, max_ep_len+1):\n",
        "        \n",
        "        # select action with policy\n",
        "        action = ppo_agent.select_action(state)\n",
        "        state, reward, done, _ = env.step(action)\n",
        "        \n",
        "        # saving reward and is_terminals\n",
        "        ppo_agent.buffer.rewards.append(reward)\n",
        "        ppo_agent.buffer.is_terminals.append(done)\n",
        "        \n",
        "        time_step +=1\n",
        "        current_ep_reward += reward\n",
        "\n",
        "        # update PPO agent\n",
        "        if time_step % update_timestep == 0:\n",
        "            ppo_agent.update()\n",
        "\n",
        "        # if continuous action space; then decay action std of ouput action distribution\n",
        "        if has_continuous_action_space and time_step % action_std_decay_freq == 0:\n",
        "            ppo_agent.decay_action_std(action_std_decay_rate, min_action_std)\n",
        "\n",
        "        # log in logging file\n",
        "        if time_step % log_freq == 0:\n",
        "\n",
        "            # log average reward till last episode\n",
        "            log_avg_reward = log_running_reward / log_running_episodes\n",
        "            log_avg_reward = round(log_avg_reward, 4)\n",
        "\n",
        "            log_f.write('{},{},{}\\n'.format(i_episode, time_step, log_avg_reward))\n",
        "            log_f.flush()\n",
        "\n",
        "            log_running_reward = 0\n",
        "            log_running_episodes = 0\n",
        "\n",
        "        # printing average reward\n",
        "        if time_step % print_freq == 0:\n",
        "\n",
        "            # print average reward till last episode\n",
        "            print_avg_reward = print_running_reward / print_running_episodes\n",
        "            print_avg_reward = round(print_avg_reward, 2)\n",
        "\n",
        "            print(\"Episode : {} \\t\\t Timestep : {} \\t\\t Average Reward : {}\".format(i_episode, time_step, print_avg_reward))\n",
        "\n",
        "            print_running_reward = 0\n",
        "            print_running_episodes = 0\n",
        "            \n",
        "        # save model weights\n",
        "        if time_step % save_model_freq == 0:\n",
        "            print(\"--------------------------------------------------------------------------------------------\")\n",
        "            print(\"saving model at : \" + checkpoint_path)\n",
        "            ppo_agent.save(checkpoint_path)\n",
        "            print(\"model saved\")\n",
        "            print(\"Elapsed Time  : \", datetime.now().replace(microsecond=0) - start_time)\n",
        "            print(\"--------------------------------------------------------------------------------------------\")\n",
        "            \n",
        "        # break; if the episode is over\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    print_running_reward += current_ep_reward\n",
        "    print_running_episodes += 1\n",
        "\n",
        "    log_running_reward += current_ep_reward\n",
        "    log_running_episodes += 1\n",
        "\n",
        "    i_episode += 1\n",
        "\n",
        "\n",
        "log_f.close()\n",
        "env.close()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# print total training time\n",
        "print(\"============================================================================================\")\n",
        "end_time = datetime.now().replace(microsecond=0)\n",
        "print(\"Started training at (GMT) : \", start_time)\n",
        "print(\"Finished training at (GMT) : \", end_time)\n",
        "print(\"Total training time  : \", end_time - start_time)\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================================================\n",
            "training environment name : RoboschoolWalker2d-v1\n",
            "current logging run number for RoboschoolWalker2d-v1 :  4\n",
            "logging at : PPO_logs/RoboschoolWalker2d-v1//PPO_RoboschoolWalker2d-v1_log_4.csv\n",
            "save checkpoint path : PPO_preTrained/RoboschoolWalker2d-v1/PPO_RoboschoolWalker2d-v1_0_0.pth\n",
            "--------------------------------------------------------------------------------------------\n",
            "max training timesteps :  100000\n",
            "max timesteps per episode :  400\n",
            "model saving frequency : 20000 timesteps\n",
            "log frequency : 800 timesteps\n",
            "printing average reward over episodes in last : 1600 timesteps\n",
            "--------------------------------------------------------------------------------------------\n",
            "state space dimension :  22\n",
            "action space dimension :  6\n",
            "--------------------------------------------------------------------------------------------\n",
            "Initializing a continuous action space policy\n",
            "--------------------------------------------------------------------------------------------\n",
            "starting std of action distribution :  0.6\n",
            "decay rate of std of action distribution :  0.05\n",
            "minimum std of action distribution :  0.1\n",
            "decay frequency of std of action distribution : 250000 timesteps\n",
            "--------------------------------------------------------------------------------------------\n",
            "PPO update frequency : 1600 timesteps\n",
            "PPO K epochs :  40\n",
            "PPO epsilon clip :  0.2\n",
            "discount factor (gamma) :  0.99\n",
            "--------------------------------------------------------------------------------------------\n",
            "optimizer learning rate actor :  0.0003\n",
            "optimizer learning rate critic :  0.001\n",
            "============================================================================================\n",
            "Started training at (GMT) :  2022-01-22 04:08:21\n",
            "============================================================================================\n",
            "Episode : 143 \t\t Timestep : 1600 \t\t Average Reward : 14.57\n",
            "Episode : 272 \t\t Timestep : 3200 \t\t Average Reward : 15.41\n",
            "Episode : 384 \t\t Timestep : 4800 \t\t Average Reward : 16.4\n",
            "Episode : 480 \t\t Timestep : 6400 \t\t Average Reward : 18.19\n",
            "Episode : 564 \t\t Timestep : 8000 \t\t Average Reward : 19.25\n",
            "Episode : 631 \t\t Timestep : 9600 \t\t Average Reward : 22.17\n",
            "Episode : 685 \t\t Timestep : 11200 \t\t Average Reward : 25.63\n",
            "Episode : 736 \t\t Timestep : 12800 \t\t Average Reward : 25.84\n",
            "Episode : 774 \t\t Timestep : 14400 \t\t Average Reward : 29.75\n",
            "Episode : 817 \t\t Timestep : 16000 \t\t Average Reward : 30.24\n",
            "Episode : 848 \t\t Timestep : 17600 \t\t Average Reward : 38.87\n",
            "Episode : 876 \t\t Timestep : 19200 \t\t Average Reward : 42.11\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/RoboschoolWalker2d-v1/PPO_RoboschoolWalker2d-v1_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:00:48\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 901 \t\t Timestep : 20800 \t\t Average Reward : 45.15\n",
            "Episode : 926 \t\t Timestep : 22400 \t\t Average Reward : 42.05\n",
            "Episode : 949 \t\t Timestep : 24000 \t\t Average Reward : 51.24\n",
            "Episode : 971 \t\t Timestep : 25600 \t\t Average Reward : 49.52\n",
            "Episode : 987 \t\t Timestep : 27200 \t\t Average Reward : 57.69\n",
            "Episode : 1007 \t\t Timestep : 28800 \t\t Average Reward : 55.94\n",
            "Episode : 1027 \t\t Timestep : 30400 \t\t Average Reward : 60.36\n",
            "Episode : 1045 \t\t Timestep : 32000 \t\t Average Reward : 54.55\n",
            "Episode : 1068 \t\t Timestep : 33600 \t\t Average Reward : 52.87\n",
            "Episode : 1085 \t\t Timestep : 35200 \t\t Average Reward : 67.57\n",
            "Episode : 1101 \t\t Timestep : 36800 \t\t Average Reward : 69.87\n",
            "Episode : 1120 \t\t Timestep : 38400 \t\t Average Reward : 51.79\n",
            "Episode : 1139 \t\t Timestep : 40000 \t\t Average Reward : 55.2\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/RoboschoolWalker2d-v1/PPO_RoboschoolWalker2d-v1_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:01:29\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1155 \t\t Timestep : 41600 \t\t Average Reward : 63.91\n",
            "Episode : 1172 \t\t Timestep : 43200 \t\t Average Reward : 61.27\n",
            "Episode : 1188 \t\t Timestep : 44800 \t\t Average Reward : 64.01\n",
            "Episode : 1207 \t\t Timestep : 46400 \t\t Average Reward : 62.63\n",
            "Episode : 1225 \t\t Timestep : 48000 \t\t Average Reward : 68.1\n",
            "Episode : 1242 \t\t Timestep : 49600 \t\t Average Reward : 73.71\n",
            "Episode : 1260 \t\t Timestep : 51200 \t\t Average Reward : 67.93\n",
            "Episode : 1274 \t\t Timestep : 52800 \t\t Average Reward : 75.27\n",
            "Episode : 1292 \t\t Timestep : 54400 \t\t Average Reward : 70.54\n",
            "Episode : 1310 \t\t Timestep : 56000 \t\t Average Reward : 62.27\n",
            "Episode : 1330 \t\t Timestep : 57600 \t\t Average Reward : 62.43\n",
            "Episode : 1349 \t\t Timestep : 59200 \t\t Average Reward : 72.11\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/RoboschoolWalker2d-v1/PPO_RoboschoolWalker2d-v1_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:02:12\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1367 \t\t Timestep : 60800 \t\t Average Reward : 70.98\n",
            "Episode : 1385 \t\t Timestep : 62400 \t\t Average Reward : 73.02\n",
            "Episode : 1405 \t\t Timestep : 64000 \t\t Average Reward : 59.57\n",
            "Episode : 1424 \t\t Timestep : 65600 \t\t Average Reward : 69.69\n",
            "Episode : 1443 \t\t Timestep : 67200 \t\t Average Reward : 68.94\n",
            "Episode : 1464 \t\t Timestep : 68800 \t\t Average Reward : 62.3\n",
            "Episode : 1486 \t\t Timestep : 70400 \t\t Average Reward : 59.67\n",
            "Episode : 1506 \t\t Timestep : 72000 \t\t Average Reward : 61.53\n",
            "Episode : 1525 \t\t Timestep : 73600 \t\t Average Reward : 70.47\n",
            "Episode : 1545 \t\t Timestep : 75200 \t\t Average Reward : 76.42\n",
            "Episode : 1564 \t\t Timestep : 76800 \t\t Average Reward : 67.19\n",
            "Episode : 1584 \t\t Timestep : 78400 \t\t Average Reward : 64.42\n",
            "Episode : 1605 \t\t Timestep : 80000 \t\t Average Reward : 70.24\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/RoboschoolWalker2d-v1/PPO_RoboschoolWalker2d-v1_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:02:53\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1623 \t\t Timestep : 81600 \t\t Average Reward : 73.42\n",
            "Episode : 1644 \t\t Timestep : 83200 \t\t Average Reward : 61.9\n",
            "Episode : 1661 \t\t Timestep : 84800 \t\t Average Reward : 84.29\n",
            "Episode : 1680 \t\t Timestep : 86400 \t\t Average Reward : 80.02\n",
            "Episode : 1700 \t\t Timestep : 88000 \t\t Average Reward : 69.86\n",
            "Episode : 1717 \t\t Timestep : 89600 \t\t Average Reward : 82.87\n",
            "Episode : 1734 \t\t Timestep : 91200 \t\t Average Reward : 88.56\n",
            "Episode : 1750 \t\t Timestep : 92800 \t\t Average Reward : 76.26\n",
            "Episode : 1769 \t\t Timestep : 94400 \t\t Average Reward : 79.41\n",
            "Episode : 1786 \t\t Timestep : 96000 \t\t Average Reward : 86.29\n",
            "Episode : 1803 \t\t Timestep : 97600 \t\t Average Reward : 78.46\n",
            "Episode : 1821 \t\t Timestep : 99200 \t\t Average Reward : 77.04\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/RoboschoolWalker2d-v1/PPO_RoboschoolWalker2d-v1_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:03:33\n",
            "--------------------------------------------------------------------------------------------\n",
            "============================================================================================\n",
            "Started training at (GMT) :  2022-01-22 04:08:21\n",
            "Finished training at (GMT) :  2022-01-22 04:11:54\n",
            "Total training time  :  0:03:33\n",
            "============================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEy2qKdZF8ha"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "################################ End of Part II ################################\n",
        "\n",
        "\n"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHhK13_1G6zX"
      },
      "source": [
        "################################################################################\n",
        "> # **Part - III**\n",
        "\n",
        "*   load and test preTrained networks on environments\n",
        "\n",
        "################################################################################"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZWyhkq9Gxm5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17a30a9d-fb86-44d4-ccd4-e9e1d93e6e0b"
      },
      "source": [
        "\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "\n",
        "#################################### Testing ###################################\n",
        "\n",
        "\n",
        "################## hyperparameters ##################\n",
        "\n",
        "#env_name = \"CartPole-v1\"\n",
        "#has_continuous_action_space = False\n",
        "#max_ep_len = 400\n",
        "#action_std = None\n",
        "\n",
        "\n",
        "# env_name = \"LunarLander-v2\"\n",
        "# has_continuous_action_space = False\n",
        "# max_ep_len = 300\n",
        "# action_std = None\n",
        "\n",
        "\n",
        "# env_name = \"BipedalWalker-v2\"\n",
        "# has_continuous_action_space = True\n",
        "# max_ep_len = 1500           # max timesteps in one episode\n",
        "# action_std = 0.1            # set same std for action distribution which was used while saving\n",
        "\n",
        "\n",
        "env_name = \"RoboschoolWalker2d-v1\"\n",
        "has_continuous_action_space = True\n",
        "max_ep_len = 1000           # max timesteps in one episode\n",
        "action_std = 0.1            # set same std for action distribution which was used while saving\n",
        "\n",
        "\n",
        "total_test_episodes = 10    # total num of testing episodes\n",
        "\n",
        "K_epochs = 80               # update policy for K epochs\n",
        "eps_clip = 0.2              # clip parameter for PPO\n",
        "gamma = 0.99                # discount factor\n",
        "\n",
        "lr_actor = 0.0003           # learning rate for actor\n",
        "lr_critic = 0.001           # learning rate for critic\n",
        "\n",
        "#####################################################\n",
        "\n",
        "\n",
        "env = gym.make(env_name)\n",
        "\n",
        "# state space dimension\n",
        "state_dim = env.observation_space.shape[0]\n",
        "\n",
        "# action space dimension\n",
        "if has_continuous_action_space:\n",
        "    action_dim = env.action_space.shape[0]\n",
        "else:\n",
        "    action_dim = env.action_space.n\n",
        "\n",
        "\n",
        "# initialize a PPO agent\n",
        "ppo_agent = PPO(state_dim, action_dim, lr_actor, lr_critic, gamma, K_epochs, eps_clip, has_continuous_action_space, action_std)\n",
        "\n",
        "\n",
        "# preTrained weights directory\n",
        "\n",
        "random_seed = 0             #### set this to load a particular checkpoint trained on random seed\n",
        "run_num_pretrained = 0      #### set this to load a particular checkpoint num\n",
        "\n",
        "\n",
        "directory = \"PPO_preTrained\" + '/' + env_name + '/'\n",
        "checkpoint_path = directory + \"PPO_{}_{}_{}.pth\".format(env_name, random_seed, run_num_pretrained)\n",
        "print(\"loading network from : \" + checkpoint_path)\n",
        "\n",
        "ppo_agent.load(checkpoint_path)\n",
        "\n",
        "print(\"--------------------------------------------------------------------------------------------\")\n",
        "\n",
        "\n",
        "\n",
        "test_running_reward = 0\n",
        "\n",
        "for ep in range(1, total_test_episodes+1):\n",
        "    ep_reward = 0\n",
        "    state = env.reset()\n",
        "    \n",
        "    for t in range(1, max_ep_len+1):\n",
        "        action = ppo_agent.select_action(state)\n",
        "        state, reward, done, _ = env.step(action)\n",
        "        ep_reward += reward\n",
        "        \n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    # clear buffer    \n",
        "    ppo_agent.buffer.clear()\n",
        "\n",
        "    test_running_reward +=  ep_reward\n",
        "    print('Episode: {} \\t\\t Reward: {}'.format(ep, round(ep_reward, 2)))\n",
        "    ep_reward = 0\n",
        "\n",
        "env.close()\n",
        "\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "avg_test_reward = test_running_reward / total_test_episodes\n",
        "avg_test_reward = round(avg_test_reward, 2)\n",
        "print(\"average test reward : \" + str(avg_test_reward))\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================================================\n",
            "loading network from : PPO_preTrained/RoboschoolWalker2d-v1/PPO_RoboschoolWalker2d-v1_0_0.pth\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode: 1 \t\t Reward: 82.59\n",
            "Episode: 2 \t\t Reward: 108.53\n",
            "Episode: 3 \t\t Reward: 103.0\n",
            "Episode: 4 \t\t Reward: 91.14\n",
            "Episode: 5 \t\t Reward: 95.93\n",
            "Episode: 6 \t\t Reward: 98.07\n",
            "Episode: 7 \t\t Reward: 99.7\n",
            "Episode: 8 \t\t Reward: 93.18\n",
            "Episode: 9 \t\t Reward: 101.46\n",
            "Episode: 10 \t\t Reward: 86.61\n",
            "============================================================================================\n",
            "average test reward : 96.02\n",
            "============================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6IYC_JCGxlB"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "################################ End of Part III ###############################\n",
        "\n",
        "\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZewQELovHFt4"
      },
      "source": [
        "################################################################################\n",
        "> # **Part - IV**\n",
        "\n",
        "*   load log files using pandas\n",
        "*   plot graph using matplotlib\n",
        "\n",
        "################################################################################"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bY-E5HGcGxiu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "e23ff325-63c5-4f6c-b338-73dc7061e3a6"
      },
      "source": [
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "#env_name = 'CartPole-v1'\n",
        "# env_name = 'LunarLander-v2'\n",
        "# env_name = 'BipedalWalker-v2'\n",
        "env_name = 'RoboschoolWalker2d-v1'\n",
        "\n",
        "\n",
        "fig_num = 0     #### change this to prevent overwriting figures in same env_name folder\n",
        "\n",
        "plot_avg = True    # plot average of all runs; else plot all runs separately\n",
        "\n",
        "fig_width = 10\n",
        "fig_height = 6\n",
        "\n",
        "\n",
        "# smooth out rewards to get a smooth and a less smooth (var) plot lines\n",
        "window_len_smooth = 50\n",
        "min_window_len_smooth = 1\n",
        "linewidth_smooth = 1.5\n",
        "alpha_smooth = 1\n",
        "\n",
        "window_len_var = 5\n",
        "min_window_len_var = 1\n",
        "linewidth_var = 2\n",
        "alpha_var = 0.1\n",
        "\n",
        "\n",
        "colors = ['red', 'blue', 'green', 'orange', 'purple', 'olive', 'brown', 'magenta', 'cyan', 'crimson','gray', 'black']\n",
        "\n",
        "\n",
        "# make directory for saving figures\n",
        "figures_dir = \"PPO_figs\"\n",
        "if not os.path.exists(figures_dir):\n",
        "    os.makedirs(figures_dir)\n",
        "\n",
        "# make environment directory for saving figures\n",
        "figures_dir = figures_dir + '/' + env_name + '/'\n",
        "if not os.path.exists(figures_dir):\n",
        "    os.makedirs(figures_dir)\n",
        "\n",
        "\n",
        "fig_save_path = figures_dir + '/PPO_' + env_name + '_fig_' + str(fig_num) + '.png'\n",
        "\n",
        "\n",
        "# get number of log files in directory\n",
        "log_dir = \"PPO_logs\" + '/' + env_name + '/'\n",
        "\n",
        "current_num_files = next(os.walk(log_dir))[2]\n",
        "num_runs = len(current_num_files)\n",
        "\n",
        "\n",
        "all_runs = []\n",
        "\n",
        "for run_num in range(num_runs):\n",
        "\n",
        "    log_f_name = log_dir + '/PPO_' + env_name + \"_log_\" + str(run_num) + \".csv\"\n",
        "    print(\"loading data from : \" + log_f_name)\n",
        "    data = pd.read_csv(log_f_name)\n",
        "    data = pd.DataFrame(data)\n",
        "    \n",
        "    print(\"data shape : \", data.shape)\n",
        "    \n",
        "    all_runs.append(data)\n",
        "    print(\"--------------------------------------------------------------------------------------------\")\n",
        "\n",
        "\n",
        "ax = plt.gca()\n",
        "\n",
        "if plot_avg:\n",
        "    # average all runs\n",
        "    df_concat = pd.concat(all_runs)\n",
        "    df_concat_groupby = df_concat.groupby(df_concat.index)\n",
        "    data_avg = df_concat_groupby.mean()\n",
        "\n",
        "    # smooth out rewards to get a smooth and a less smooth (var) plot lines\n",
        "    data_avg['reward_smooth'] = data_avg['reward'].rolling(window=window_len_smooth, win_type='triang', min_periods=min_window_len_smooth).mean()\n",
        "    data_avg['reward_var'] = data_avg['reward'].rolling(window=window_len_var, win_type='triang', min_periods=min_window_len_var).mean()\n",
        "\n",
        "    data_avg.plot(kind='line', x='timestep' , y='reward_smooth',ax=ax,color=colors[0],  linewidth=linewidth_smooth, alpha=alpha_smooth)\n",
        "    data_avg.plot(kind='line', x='timestep' , y='reward_var',ax=ax,color=colors[0],  linewidth=linewidth_var, alpha=alpha_var)\n",
        "\n",
        "    # keep only reward_smooth in the legend and rename it\n",
        "    handles, labels = ax.get_legend_handles_labels()\n",
        "    ax.legend([handles[0]], [\"reward_avg_\" + str(len(all_runs)) + \"_runs\"], loc=2)\n",
        "\n",
        "\n",
        "else:\n",
        "    for i, run in enumerate(all_runs):\n",
        "        # smooth out rewards to get a smooth and a less smooth (var) plot lines\n",
        "        run['reward_smooth_' + str(i)] = run['reward'].rolling(window=window_len_smooth, win_type='triang', min_periods=min_window_len_smooth).mean()\n",
        "        run['reward_var_' + str(i)] = run['reward'].rolling(window=window_len_var, win_type='triang', min_periods=min_window_len_var).mean()\n",
        "        \n",
        "        # plot the lines\n",
        "        run.plot(kind='line', x='timestep' , y='reward_smooth_' + str(i),ax=ax,color=colors[i % len(colors)],  linewidth=linewidth_smooth, alpha=alpha_smooth)\n",
        "        run.plot(kind='line', x='timestep' , y='reward_var_' + str(i),ax=ax,color=colors[i % len(colors)],  linewidth=linewidth_var, alpha=alpha_var)\n",
        "\n",
        "    # keep alternate elements (reward_smooth_i) in the legend\n",
        "    handles, labels = ax.get_legend_handles_labels()\n",
        "    new_handles = []\n",
        "    new_labels = []\n",
        "    for i in range(len(handles)):\n",
        "        if(i%2 == 0):\n",
        "            new_handles.append(handles[i])\n",
        "            new_labels.append(labels[i])\n",
        "    ax.legend(new_handles, new_labels, loc=2)\n",
        "\n",
        "\n",
        "\n",
        "# ax.set_yticks(np.arange(0, 1800, 200))\n",
        "# ax.set_xticks(np.arange(0, int(4e6), int(5e5)))\n",
        "\n",
        "\n",
        "ax.grid(color='gray', linestyle='-', linewidth=1, alpha=0.2)\n",
        "\n",
        "ax.set_xlabel(\"Timesteps\", fontsize=12)\n",
        "ax.set_ylabel(\"Rewards\", fontsize=12)\n",
        "\n",
        "plt.title(env_name, fontsize=14)\n",
        "\n",
        "\n",
        "fig = plt.gcf()\n",
        "fig.set_size_inches(fig_width, fig_height)\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "plt.savefig(fig_save_path)\n",
        "print(\"figure saved at : \", fig_save_path)\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "\n",
        "plt.show()\n",
        "\n",
        "\n"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================================================\n",
            "loading data from : PPO_logs/RoboschoolWalker2d-v1//PPO_RoboschoolWalker2d-v1_log_0.csv\n",
            "data shape :  (1500, 3)\n",
            "--------------------------------------------------------------------------------------------\n",
            "loading data from : PPO_logs/RoboschoolWalker2d-v1//PPO_RoboschoolWalker2d-v1_log_1.csv\n",
            "data shape :  (1500, 3)\n",
            "--------------------------------------------------------------------------------------------\n",
            "loading data from : PPO_logs/RoboschoolWalker2d-v1//PPO_RoboschoolWalker2d-v1_log_2.csv\n",
            "data shape :  (1500, 3)\n",
            "--------------------------------------------------------------------------------------------\n",
            "loading data from : PPO_logs/RoboschoolWalker2d-v1//PPO_RoboschoolWalker2d-v1_log_3.csv\n",
            "data shape :  (125, 3)\n",
            "--------------------------------------------------------------------------------------------\n",
            "loading data from : PPO_logs/RoboschoolWalker2d-v1//PPO_RoboschoolWalker2d-v1_log_4.csv\n",
            "data shape :  (125, 3)\n",
            "--------------------------------------------------------------------------------------------\n",
            "============================================================================================\n",
            "figure saved at :  PPO_figs/RoboschoolWalker2d-v1//PPO_RoboschoolWalker2d-v1_fig_0.png\n",
            "============================================================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YaWPRW9EGxgH"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "################################ End of Part IV ################################\n",
        "\n",
        "\n"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8uG43MtHNGC"
      },
      "source": [
        "################################################################################\n",
        "> # **Part - V**\n",
        "\n",
        "*   install virtual display libraries for rendering on colab / remote server ^\n",
        "*   load preTrained networks and save images for gif\n",
        "*   generate and save gif from previously saved images\n",
        "\n",
        "*   ^ If running locally; do not install xvbf and pyvirtualdisplay. Just comment out the virtual display code and render it normally. \n",
        "*   ^ You will still require to use ipythondisplay, if you want to render it in the Jupyter Notebook.\n",
        "\n",
        "################################################################################"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VL3tpKf3HLAq"
      },
      "source": [
        "\n",
        "\n",
        "#### to render on colab / server / headless machine install virtual display libraries\n",
        "\n",
        "!apt-get install -y xvfb python-opengl > /dev/null 2>&1\n",
        "\n",
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "\n"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5Rx_IFKHK-D",
        "outputId": "f7555493-add1-457b-cc94-0f032f8da53f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "\n",
        "\n",
        "############################# save images for gif ##############################\n",
        "\n",
        "\n",
        "import os\n",
        "import glob\n",
        "\n",
        "import gym\n",
        "import roboschool\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "from pyvirtualdisplay import Display\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "One frame corresponding to each timestep is saved in a folder :\n",
        "\n",
        "PPO_gif_images/env_name/000001.jpg\n",
        "PPO_gif_images/env_name/000002.jpg\n",
        "PPO_gif_images/env_name/000003.jpg\n",
        "...\n",
        "...\n",
        "...\n",
        "\n",
        "\n",
        "if this section is run multiple times or for multiple episodes for the same env_name; \n",
        "then the saved images will be overwritten.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#### beginning of virtual display code section\n",
        "\n",
        "display = Display(visible=0, size=(400, 300))\n",
        "display.start()\n",
        "\n",
        "#### end of virtual display code section\n",
        "\n",
        "\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "\n",
        "################## hyperparameters ##################\n",
        "\n",
        "#env_name = \"CartPole-v1\"\n",
        "#has_continuous_action_space = False\n",
        "#max_ep_len = 400\n",
        "#action_std = None\n",
        "\n",
        "\n",
        "# env_name = \"LunarLander-v2\"\n",
        "# has_continuous_action_space = False\n",
        "# max_ep_len = 300\n",
        "# action_std = None\n",
        "\n",
        "# env_name = \"BipedalWalker-v2\"\n",
        "# has_continuous_action_space = True\n",
        "# max_ep_len = 1500           # max timesteps in one episode\n",
        "# action_std = 0.1            # set same std for action distribution which was used while saving\n",
        "\n",
        "env_name = \"RoboschoolWalker2d-v1\"\n",
        "has_continuous_action_space = True\n",
        "max_ep_len = 1000           # max timesteps in one episode\n",
        "action_std = 0.1            # set same std for action distribution which was used while saving\n",
        "\n",
        "\n",
        "total_test_episodes = 1     # save gif for only one episode\n",
        "\n",
        "render_ipython = False      # plot the images using matplotlib and ipythondisplay before saving (slow)\n",
        "\n",
        "K_epochs = 80               # update policy for K epochs\n",
        "eps_clip = 0.2              # clip parameter for PPO\n",
        "gamma = 0.99                # discount factor\n",
        "\n",
        "lr_actor = 0.0003         # learning rate for actor\n",
        "lr_critic = 0.001         # learning rate for critic\n",
        "\n",
        "#####################################################\n",
        "\n",
        "\n",
        "env = gym.make(env_name)\n",
        "\n",
        "# state space dimension\n",
        "state_dim = env.observation_space.shape[0]\n",
        "\n",
        "# action space dimension\n",
        "if has_continuous_action_space:\n",
        "    action_dim = env.action_space.shape[0]\n",
        "else:\n",
        "    action_dim = env.action_space.n\n",
        "\n",
        "\n",
        "\n",
        "# make directory for saving gif images\n",
        "gif_images_dir = \"PPO_gif_images\" + '/'\n",
        "if not os.path.exists(gif_images_dir):\n",
        "    os.makedirs(gif_images_dir)\n",
        "\n",
        "# make environment directory for saving gif images\n",
        "gif_images_dir = gif_images_dir + '/' + env_name + '/'\n",
        "if not os.path.exists(gif_images_dir):\n",
        "    os.makedirs(gif_images_dir)\n",
        "\n",
        "# make directory for gif\n",
        "gif_dir = \"PPO_gifs\" + '/'\n",
        "if not os.path.exists(gif_dir):\n",
        "    os.makedirs(gif_dir)\n",
        "\n",
        "# make environment directory for gif\n",
        "gif_dir = gif_dir + '/' + env_name  + '/'\n",
        "if not os.path.exists(gif_dir):\n",
        "    os.makedirs(gif_dir)\n",
        "\n",
        "\n",
        "\n",
        "ppo_agent = PPO(state_dim, action_dim, lr_actor, lr_critic, gamma, K_epochs, eps_clip, has_continuous_action_space, action_std)\n",
        "\n",
        "\n",
        "# preTrained weights directory\n",
        "\n",
        "random_seed = 0             #### set this to load a particular checkpoint trained on random seed\n",
        "run_num_pretrained = 0      #### set this to load a particular checkpoint num\n",
        "\n",
        "\n",
        "directory = \"PPO_preTrained\" + '/' + env_name + '/'\n",
        "checkpoint_path = directory + \"PPO_{}_{}_{}.pth\".format(env_name, random_seed, run_num_pretrained)\n",
        "print(\"loading network from : \" + checkpoint_path)\n",
        "\n",
        "ppo_agent.load(checkpoint_path)\n",
        "\n",
        "print(\"--------------------------------------------------------------------------------------------\")\n",
        "\n",
        "\n",
        "\n",
        "test_running_reward = 0\n",
        "\n",
        "for ep in range(1, total_test_episodes+1):\n",
        "    \n",
        "    ep_reward = 0\n",
        "    state = env.reset()\n",
        "\n",
        "    for t in range(1, max_ep_len+1):\n",
        "        action = ppo_agent.select_action(state)\n",
        "        state, reward, done, _ = env.step(action)\n",
        "        ep_reward += reward\n",
        "\n",
        "        img = env.render(mode = 'rgb_array')\n",
        "\n",
        "\n",
        "        #### beginning of ipythondisplay code section 1\n",
        "\n",
        "        if render_ipython:\n",
        "            plt.imshow(img)\n",
        "            ipythondisplay.clear_output(wait=True)\n",
        "            ipythondisplay.display(plt.gcf())\n",
        "\n",
        "        #### end of ipythondisplay code section 1\n",
        "\n",
        "\n",
        "        img = Image.fromarray(img)\n",
        "        img.save(gif_images_dir + '/' + str(t).zfill(6) + '.jpg')\n",
        "        \n",
        "        if done:\n",
        "            break\n",
        "    \n",
        "    # clear buffer    \n",
        "    ppo_agent.buffer.clear()\n",
        "    \n",
        "    test_running_reward +=  ep_reward\n",
        "    print('Episode: {} \\t\\t Reward: {}'.format(ep, round(ep_reward, 2)))\n",
        "    ep_reward = 0\n",
        "\n",
        "\n",
        "\n",
        "env.close()\n",
        "\n",
        "\n",
        "#### beginning of ipythondisplay code section 2\n",
        "\n",
        "if render_ipython:\n",
        "    ipythondisplay.clear_output(wait=True)\n",
        "\n",
        "#### end of ipythondisplay code section 2\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "print(\"total number of frames / timesteps / images saved : \", t)\n",
        "\n",
        "avg_test_reward = test_running_reward / total_test_episodes\n",
        "avg_test_reward = round(avg_test_reward, 2)\n",
        "print(\"average test reward : \" + str(avg_test_reward))\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================================================\n",
            "loading network from : PPO_preTrained/RoboschoolWalker2d-v1/PPO_RoboschoolWalker2d-v1_0_0.pth\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode: 1 \t\t Reward: 111.19\n",
            "============================================================================================\n",
            "total number of frames / timesteps / images saved :  83\n",
            "average test reward : 111.19\n",
            "============================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BoVshl_ZHK7s",
        "outputId": "8a789f88-9c8c-4ef0-d819-969eb1305309",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "######################## generate gif from saved images ########################\n",
        "\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "#env_name = 'CartPole-v1'\n",
        "# env_name = 'LunarLander-v2'\n",
        "# env_name = 'BipedalWalker-v2'\n",
        "env_name = 'RoboschoolWalker2d-v1'\n",
        "\n",
        "\n",
        "gif_num = 0     #### change this to prevent overwriting gifs in same env_name folder\n",
        "\n",
        "# adjust following parameters to get desired duration, size (bytes) and smoothness of gif\n",
        "total_timesteps = 300\n",
        "step = 10\n",
        "frame_duration = 150\n",
        "\n",
        "\n",
        "# input images\n",
        "gif_images_dir = \"PPO_gif_images/\" + env_name + '/*.jpg'\n",
        "\n",
        "\n",
        "# ouput gif path\n",
        "gif_dir = \"PPO_gifs\"\n",
        "if not os.path.exists(gif_dir):\n",
        "    os.makedirs(gif_dir)\n",
        "\n",
        "gif_dir = gif_dir + '/' + env_name\n",
        "if not os.path.exists(gif_dir):\n",
        "    os.makedirs(gif_dir)\n",
        "\n",
        "gif_path = gif_dir + '/PPO_' + env_name + '_gif_' + str(gif_num) + '.gif'\n",
        "\n",
        "\n",
        "\n",
        "img_paths = sorted(glob.glob(gif_images_dir))\n",
        "img_paths = img_paths[:total_timesteps]\n",
        "img_paths = img_paths[::step]\n",
        "\n",
        "\n",
        "print(\"total frames in gif : \", len(img_paths))\n",
        "print(\"total duration of gif : \" + str(round(len(img_paths) * frame_duration / 1000, 2)) + \" seconds\")\n",
        "\n",
        "\n",
        "\n",
        "# save gif\n",
        "img, *imgs = [Image.open(f) for f in img_paths]\n",
        "img.save(fp=gif_path, format='GIF', append_images=imgs, save_all=True, optimize=True, duration=frame_duration, loop=0)\n",
        "\n",
        "print(\"saved gif at : \", gif_path)\n",
        "\n",
        "\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "\n"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================================================\n",
            "total frames in gif :  9\n",
            "total duration of gif : 1.35 seconds\n",
            "saved gif at :  PPO_gifs/RoboschoolWalker2d-v1/PPO_RoboschoolWalker2d-v1_gif_0.gif\n",
            "============================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20d1bR8xHK5j",
        "outputId": "6848ca0a-44f8-42b3-badc-6a0f8654501f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "\n",
        "############################# check gif byte size ##############################\n",
        "\n",
        "\n",
        "import os\n",
        "import glob\n",
        "\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "#env_name = 'CartPole-v1'\n",
        "# env_name = 'LunarLander-v2'\n",
        "# env_name = 'BipedalWalker-v2'\n",
        "env_name = 'RoboschoolWalker2d-v1'\n",
        "\n",
        "\n",
        "\n",
        "gif_dir = \"PPO_gifs/\" + env_name + '/*.gif'\n",
        "\n",
        "gif_paths = sorted(glob.glob(gif_dir))\n",
        "\n",
        "for gif_path in gif_paths:\n",
        "    file_size = os.path.getsize(gif_path)\n",
        "    print(gif_path + '\\t\\t' + str(round(file_size / (1024 * 1024), 2)) + \" MB\")\n",
        "\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================================================\n",
            "PPO_gifs/RoboschoolWalker2d-v1/PPO_RoboschoolWalker2d-v1_gif_0.gif\t\t1.35 MB\n",
            "============================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rM5UIAkcGxeA"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "################################# End of Part V ################################\n",
        "\n",
        "\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7YUzQOu1HYHR"
      },
      "source": [
        "################################################################################\n",
        "\n",
        "---------------------------------------------------------------------------- That's all folks ! ----------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "################################################################################"
      ]
    }
  ]
}