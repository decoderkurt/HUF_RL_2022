{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "simple_ppo.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyON0e3OtATh/hFiLT/4wTQS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/decoderkurt/HUF_RL_2022/blob/main/24/simple_ppo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir PPO"
      ],
      "metadata": {
        "id": "6rLu467vAbkx"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch as T\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.distributions.categorical import Categorical\n",
        "\n",
        "class PPOMemory:\n",
        "    def __init__(self, batch_size):\n",
        "        self.states = []\n",
        "        self.probs = []\n",
        "        self.vals = []\n",
        "        self.actions = []\n",
        "        self.rewards = []\n",
        "        self.dones = []\n",
        "\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def generate_batches(self):\n",
        "        n_states = len(self.states)\n",
        "        batch_start = np.arange(0, n_states, self.batch_size)\n",
        "        indices = np.arange(n_states, dtype=np.int64)\n",
        "        np.random.shuffle(indices)\n",
        "        batches = [indices[i:i+self.batch_size] for i in batch_start]\n",
        "\n",
        "        return np.array(self.states),\\\n",
        "                np.array(self.actions),\\\n",
        "                np.array(self.probs),\\\n",
        "                np.array(self.vals),\\\n",
        "                np.array(self.rewards),\\\n",
        "                np.array(self.dones),\\\n",
        "                batches\n",
        "\n",
        "    def store_memory(self, state, action, probs, vals, reward, done):\n",
        "        self.states.append(state)\n",
        "        self.actions.append(action)\n",
        "        self.probs.append(probs)\n",
        "        self.vals.append(vals)\n",
        "        self.rewards.append(reward)\n",
        "        self.dones.append(done)\n",
        "\n",
        "    def clear_memory(self):\n",
        "        self.states = []\n",
        "        self.probs = []\n",
        "        self.actions = []\n",
        "        self.rewards = []\n",
        "        self.dones = []\n",
        "        self.vals = []\n",
        "\n",
        "class ActorNetwork(nn.Module):\n",
        "    def __init__(self, n_actions, input_dims, alpha,\n",
        "            fc1_dims=256, fc2_dims=256, chkpt_dir='PPO'):\n",
        "        super(ActorNetwork, self).__init__()\n",
        "\n",
        "        self.checkpoint_file = os.path.join(chkpt_dir, 'actor_torch_ppo')\n",
        "        self.actor = nn.Sequential(\n",
        "                nn.Linear(*input_dims, fc1_dims),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(fc1_dims, fc2_dims),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(fc2_dims, n_actions),\n",
        "                nn.Softmax(dim=-1)\n",
        "        )\n",
        "\n",
        "        self.optimizer = optim.Adam(self.parameters(), lr=alpha)\n",
        "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
        "        self.to(self.device)\n",
        "\n",
        "    def forward(self, state):\n",
        "        dist = self.actor(state)\n",
        "        dist = Categorical(dist)\n",
        "        \n",
        "        return dist\n",
        "\n",
        "    def save_checkpoint(self):\n",
        "        T.save(self.state_dict(), self.checkpoint_file)\n",
        "\n",
        "    def load_checkpoint(self):\n",
        "        self.load_state_dict(T.load(self.checkpoint_file))\n",
        "\n",
        "class CriticNetwork(nn.Module):\n",
        "    def __init__(self, input_dims, alpha, fc1_dims=256, fc2_dims=256,\n",
        "            chkpt_dir='PPO'):\n",
        "        super(CriticNetwork, self).__init__()\n",
        "\n",
        "        self.checkpoint_file = os.path.join(chkpt_dir, 'critic_torch_ppo')\n",
        "        self.critic = nn.Sequential(\n",
        "                nn.Linear(*input_dims, fc1_dims),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(fc1_dims, fc2_dims),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(fc2_dims, 1)\n",
        "        )\n",
        "\n",
        "        self.optimizer = optim.Adam(self.parameters(), lr=alpha)\n",
        "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
        "        self.to(self.device)\n",
        "\n",
        "    def forward(self, state):\n",
        "        value = self.critic(state)\n",
        "\n",
        "        return value\n",
        "\n",
        "    def save_checkpoint(self):\n",
        "        T.save(self.state_dict(), self.checkpoint_file)\n",
        "\n",
        "    def load_checkpoint(self):\n",
        "        self.load_state_dict(T.load(self.checkpoint_file))\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self, n_actions, input_dims, gamma=0.99, alpha=0.0003, gae_lambda=0.95,\n",
        "            policy_clip=0.2, batch_size=64, n_epochs=10):\n",
        "        self.gamma = gamma\n",
        "        self.policy_clip = policy_clip\n",
        "        self.n_epochs = n_epochs\n",
        "        self.gae_lambda = gae_lambda\n",
        "\n",
        "        self.actor = ActorNetwork(n_actions, input_dims, alpha)\n",
        "        self.critic = CriticNetwork(input_dims, alpha)\n",
        "        self.memory = PPOMemory(batch_size)\n",
        "       \n",
        "    def remember(self, state, action, probs, vals, reward, done):\n",
        "        self.memory.store_memory(state, action, probs, vals, reward, done)\n",
        "\n",
        "    def save_models(self):\n",
        "        print('... saving models ...')\n",
        "        self.actor.save_checkpoint()\n",
        "        self.critic.save_checkpoint()\n",
        "\n",
        "    def load_models(self):\n",
        "        print('... loading models ...')\n",
        "        self.actor.load_checkpoint()\n",
        "        self.critic.load_checkpoint()\n",
        "\n",
        "    def choose_action(self, observation):\n",
        "        state = T.tensor([observation], dtype=T.float).to(self.actor.device)\n",
        "\n",
        "        dist = self.actor(state)\n",
        "        value = self.critic(state)\n",
        "        action = dist.sample()\n",
        "\n",
        "        probs = T.squeeze(dist.log_prob(action)).item()\n",
        "        action = T.squeeze(action).item()\n",
        "        value = T.squeeze(value).item()\n",
        "\n",
        "        return action, probs, value\n",
        "\n",
        "    def learn(self):\n",
        "        for _ in range(self.n_epochs):\n",
        "            state_arr, action_arr, old_prob_arr, vals_arr,\\\n",
        "            reward_arr, dones_arr, batches = \\\n",
        "                    self.memory.generate_batches()\n",
        "\n",
        "            values = vals_arr\n",
        "            advantage = np.zeros(len(reward_arr), dtype=np.float32)\n",
        "\n",
        "            for t in range(len(reward_arr)-1):\n",
        "                discount = 1\n",
        "                a_t = 0\n",
        "                for k in range(t, len(reward_arr)-1):\n",
        "                    a_t += discount*(reward_arr[k] + self.gamma*values[k+1]*\\\n",
        "                            (1-int(dones_arr[k])) - values[k])\n",
        "                    discount *= self.gamma*self.gae_lambda\n",
        "                advantage[t] = a_t\n",
        "            advantage = T.tensor(advantage).to(self.actor.device)\n",
        "\n",
        "            values = T.tensor(values).to(self.actor.device)\n",
        "            for batch in batches:\n",
        "                states = T.tensor(state_arr[batch], dtype=T.float).to(self.actor.device)\n",
        "                old_probs = T.tensor(old_prob_arr[batch]).to(self.actor.device)\n",
        "                actions = T.tensor(action_arr[batch]).to(self.actor.device)\n",
        "\n",
        "                dist = self.actor(states)\n",
        "                critic_value = self.critic(states)\n",
        "\n",
        "                critic_value = T.squeeze(critic_value)\n",
        "\n",
        "                new_probs = dist.log_prob(actions)\n",
        "                prob_ratio = new_probs.exp() / old_probs.exp()\n",
        "                #prob_ratio = (new_probs - old_probs).exp()\n",
        "                weighted_probs = advantage[batch] * prob_ratio\n",
        "                weighted_clipped_probs = T.clamp(prob_ratio, 1-self.policy_clip,\n",
        "                        1+self.policy_clip)*advantage[batch]\n",
        "                actor_loss = -T.min(weighted_probs, weighted_clipped_probs).mean()\n",
        "\n",
        "                returns = advantage[batch] + values[batch]\n",
        "                critic_loss = (returns-critic_value)**2\n",
        "                critic_loss = critic_loss.mean()\n",
        "\n",
        "                total_loss = actor_loss + 0.5*critic_loss\n",
        "                self.actor.optimizer.zero_grad()\n",
        "                self.critic.optimizer.zero_grad()\n",
        "                total_loss.backward()\n",
        "                self.actor.optimizer.step()\n",
        "                self.critic.optimizer.step()\n",
        "\n",
        "        self.memory.clear_memory()               \n",
        "\n"
      ],
      "metadata": {
        "id": "E16zWxaLAk6I"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_learning_curve(x, scores, figure_file):\n",
        "    running_avg = np.zeros(len(scores))\n",
        "    for i in range(len(running_avg)):\n",
        "        running_avg[i] = np.mean(scores[max(0, i-100):(i+1)])\n",
        "    plt.plot(x, running_avg)\n",
        "    plt.title('Running average of previous 100 scores')\n",
        "    plt.savefig(figure_file)"
      ],
      "metadata": {
        "id": "FjiGDZ7sAtWy"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "env = gym.make('CartPole-v0')\n",
        "N = 20\n",
        "batch_size = 5\n",
        "n_epochs = 4\n",
        "alpha = 0.0003\n",
        "agent = Agent(n_actions=env.action_space.n, batch_size=batch_size, \n",
        "                alpha=alpha, n_epochs=n_epochs, \n",
        "                input_dims=env.observation_space.shape)\n",
        "n_games = 300\n",
        "\n",
        "figure_file = 'PPO/cartpole.png'\n",
        "\n",
        "best_score = env.reward_range[0]\n",
        "score_history = []\n",
        "\n",
        "learn_iters = 0\n",
        "avg_score = 0\n",
        "n_steps = 0\n",
        "\n",
        "for i in range(n_games):\n",
        "    observation = env.reset()\n",
        "    done = False\n",
        "    score = 0\n",
        "    while not done:\n",
        "        action, prob, val = agent.choose_action(observation)\n",
        "        observation_, reward, done, info = env.step(action)\n",
        "        n_steps += 1\n",
        "        score += reward\n",
        "        agent.remember(observation, action, prob, val, reward, done)\n",
        "        if n_steps % N == 0:\n",
        "            agent.learn()\n",
        "            learn_iters += 1\n",
        "        observation = observation_\n",
        "    score_history.append(score)\n",
        "    avg_score = np.mean(score_history[-100:])\n",
        "\n",
        "    if avg_score > best_score:\n",
        "        best_score = avg_score\n",
        "        agent.save_models()\n",
        "\n",
        "    print('episode', i, 'score %.1f' % score, 'avg score %.1f' % avg_score,\n",
        "            'time_steps', n_steps, 'learning_steps', learn_iters)\n",
        "x = [i+1 for i in range(len(score_history))]\n",
        "plot_learning_curve(x, score_history, figure_file)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ptfy20jLA1ED",
        "outputId": "d3b72a21-2afd-48e6-bfa1-550b2aeb90dd"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:130: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "... saving models ...\n",
            "episode 0 score 22.0 avg score 22.0 time_steps 22 learning_steps 1\n",
            "episode 1 score 11.0 avg score 16.5 time_steps 33 learning_steps 1\n",
            "episode 2 score 27.0 avg score 20.0 time_steps 60 learning_steps 3\n",
            "episode 3 score 15.0 avg score 18.8 time_steps 75 learning_steps 3\n",
            "episode 4 score 13.0 avg score 17.6 time_steps 88 learning_steps 4\n",
            "episode 5 score 14.0 avg score 17.0 time_steps 102 learning_steps 5\n",
            "episode 6 score 8.0 avg score 15.7 time_steps 110 learning_steps 5\n",
            "episode 7 score 10.0 avg score 15.0 time_steps 120 learning_steps 6\n",
            "episode 8 score 14.0 avg score 14.9 time_steps 134 learning_steps 6\n",
            "episode 9 score 15.0 avg score 14.9 time_steps 149 learning_steps 7\n",
            "episode 10 score 16.0 avg score 15.0 time_steps 165 learning_steps 8\n",
            "episode 11 score 13.0 avg score 14.8 time_steps 178 learning_steps 8\n",
            "episode 12 score 15.0 avg score 14.8 time_steps 193 learning_steps 9\n",
            "episode 13 score 15.0 avg score 14.9 time_steps 208 learning_steps 10\n",
            "episode 14 score 12.0 avg score 14.7 time_steps 220 learning_steps 11\n",
            "episode 15 score 12.0 avg score 14.5 time_steps 232 learning_steps 11\n",
            "episode 16 score 18.0 avg score 14.7 time_steps 250 learning_steps 12\n",
            "episode 17 score 10.0 avg score 14.4 time_steps 260 learning_steps 13\n",
            "episode 18 score 13.0 avg score 14.4 time_steps 273 learning_steps 13\n",
            "episode 19 score 11.0 avg score 14.2 time_steps 284 learning_steps 14\n",
            "episode 20 score 11.0 avg score 14.0 time_steps 295 learning_steps 14\n",
            "episode 21 score 15.0 avg score 14.1 time_steps 310 learning_steps 15\n",
            "episode 22 score 23.0 avg score 14.5 time_steps 333 learning_steps 16\n",
            "episode 23 score 16.0 avg score 14.5 time_steps 349 learning_steps 17\n",
            "episode 24 score 14.0 avg score 14.5 time_steps 363 learning_steps 18\n",
            "episode 25 score 25.0 avg score 14.9 time_steps 388 learning_steps 19\n",
            "episode 26 score 22.0 avg score 15.2 time_steps 410 learning_steps 20\n",
            "episode 27 score 11.0 avg score 15.0 time_steps 421 learning_steps 21\n",
            "episode 28 score 16.0 avg score 15.1 time_steps 437 learning_steps 21\n",
            "episode 29 score 17.0 avg score 15.1 time_steps 454 learning_steps 22\n",
            "episode 30 score 11.0 avg score 15.0 time_steps 465 learning_steps 23\n",
            "episode 31 score 13.0 avg score 14.9 time_steps 478 learning_steps 23\n",
            "episode 32 score 25.0 avg score 15.2 time_steps 503 learning_steps 25\n",
            "episode 33 score 26.0 avg score 15.6 time_steps 529 learning_steps 26\n",
            "episode 34 score 76.0 avg score 17.3 time_steps 605 learning_steps 30\n",
            "episode 35 score 19.0 avg score 17.3 time_steps 624 learning_steps 31\n",
            "episode 36 score 20.0 avg score 17.4 time_steps 644 learning_steps 32\n",
            "episode 37 score 33.0 avg score 17.8 time_steps 677 learning_steps 33\n",
            "episode 38 score 75.0 avg score 19.3 time_steps 752 learning_steps 37\n",
            "episode 39 score 20.0 avg score 19.3 time_steps 772 learning_steps 38\n",
            "episode 40 score 17.0 avg score 19.2 time_steps 789 learning_steps 39\n",
            "episode 41 score 56.0 avg score 20.1 time_steps 845 learning_steps 42\n",
            "episode 42 score 40.0 avg score 20.6 time_steps 885 learning_steps 44\n",
            "episode 43 score 70.0 avg score 21.7 time_steps 955 learning_steps 47\n",
            "... saving models ...\n",
            "episode 44 score 64.0 avg score 22.6 time_steps 1019 learning_steps 50\n",
            "... saving models ...\n",
            "episode 45 score 26.0 avg score 22.7 time_steps 1045 learning_steps 52\n",
            "episode 46 score 18.0 avg score 22.6 time_steps 1063 learning_steps 53\n",
            "... saving models ...\n",
            "episode 47 score 40.0 avg score 23.0 time_steps 1103 learning_steps 55\n",
            "... saving models ...\n",
            "episode 48 score 34.0 avg score 23.2 time_steps 1137 learning_steps 56\n",
            "... saving models ...\n",
            "episode 49 score 25.0 avg score 23.2 time_steps 1162 learning_steps 58\n",
            "... saving models ...\n",
            "episode 50 score 32.0 avg score 23.4 time_steps 1194 learning_steps 59\n",
            "... saving models ...\n",
            "episode 51 score 72.0 avg score 24.3 time_steps 1266 learning_steps 63\n",
            "... saving models ...\n",
            "episode 52 score 37.0 avg score 24.6 time_steps 1303 learning_steps 65\n",
            "... saving models ...\n",
            "episode 53 score 55.0 avg score 25.1 time_steps 1358 learning_steps 67\n",
            "... saving models ...\n",
            "episode 54 score 35.0 avg score 25.3 time_steps 1393 learning_steps 69\n",
            "... saving models ...\n",
            "episode 55 score 74.0 avg score 26.2 time_steps 1467 learning_steps 73\n",
            "... saving models ...\n",
            "episode 56 score 74.0 avg score 27.0 time_steps 1541 learning_steps 77\n",
            "... saving models ...\n",
            "episode 57 score 87.0 avg score 28.1 time_steps 1628 learning_steps 81\n",
            "... saving models ...\n",
            "episode 58 score 55.0 avg score 28.5 time_steps 1683 learning_steps 84\n",
            "... saving models ...\n",
            "episode 59 score 30.0 avg score 28.6 time_steps 1713 learning_steps 85\n",
            "... saving models ...\n",
            "episode 60 score 29.0 avg score 28.6 time_steps 1742 learning_steps 87\n",
            "episode 61 score 20.0 avg score 28.4 time_steps 1762 learning_steps 88\n",
            "... saving models ...\n",
            "episode 62 score 52.0 avg score 28.8 time_steps 1814 learning_steps 90\n",
            "... saving models ...\n",
            "episode 63 score 59.0 avg score 29.3 time_steps 1873 learning_steps 93\n",
            "... saving models ...\n",
            "episode 64 score 47.0 avg score 29.5 time_steps 1920 learning_steps 96\n",
            "... saving models ...\n",
            "episode 65 score 35.0 avg score 29.6 time_steps 1955 learning_steps 97\n",
            "... saving models ...\n",
            "episode 66 score 99.0 avg score 30.7 time_steps 2054 learning_steps 102\n",
            "... saving models ...\n",
            "episode 67 score 43.0 avg score 30.8 time_steps 2097 learning_steps 104\n",
            "... saving models ...\n",
            "episode 68 score 47.0 avg score 31.1 time_steps 2144 learning_steps 107\n",
            "... saving models ...\n",
            "episode 69 score 142.0 avg score 32.7 time_steps 2286 learning_steps 114\n",
            "episode 70 score 32.0 avg score 32.6 time_steps 2318 learning_steps 115\n",
            "episode 71 score 15.0 avg score 32.4 time_steps 2333 learning_steps 116\n",
            "... saving models ...\n",
            "episode 72 score 79.0 avg score 33.0 time_steps 2412 learning_steps 120\n",
            "... saving models ...\n",
            "episode 73 score 39.0 avg score 33.1 time_steps 2451 learning_steps 122\n",
            "... saving models ...\n",
            "episode 74 score 91.0 avg score 33.9 time_steps 2542 learning_steps 127\n",
            "episode 75 score 30.0 avg score 33.8 time_steps 2572 learning_steps 128\n",
            "... saving models ...\n",
            "episode 76 score 168.0 avg score 35.6 time_steps 2740 learning_steps 137\n",
            "... saving models ...\n",
            "episode 77 score 114.0 avg score 36.6 time_steps 2854 learning_steps 142\n",
            "... saving models ...\n",
            "episode 78 score 86.0 avg score 37.2 time_steps 2940 learning_steps 147\n",
            "... saving models ...\n",
            "episode 79 score 51.0 avg score 37.4 time_steps 2991 learning_steps 149\n",
            "... saving models ...\n",
            "episode 80 score 90.0 avg score 38.0 time_steps 3081 learning_steps 154\n",
            "... saving models ...\n",
            "episode 81 score 92.0 avg score 38.7 time_steps 3173 learning_steps 158\n",
            "... saving models ...\n",
            "episode 82 score 175.0 avg score 40.3 time_steps 3348 learning_steps 167\n",
            "... saving models ...\n",
            "episode 83 score 105.0 avg score 41.1 time_steps 3453 learning_steps 172\n",
            "... saving models ...\n",
            "episode 84 score 140.0 avg score 42.3 time_steps 3593 learning_steps 179\n",
            "... saving models ...\n",
            "episode 85 score 165.0 avg score 43.7 time_steps 3758 learning_steps 187\n",
            "... saving models ...\n",
            "episode 86 score 176.0 avg score 45.2 time_steps 3934 learning_steps 196\n",
            "... saving models ...\n",
            "episode 87 score 81.0 avg score 45.6 time_steps 4015 learning_steps 200\n",
            "episode 88 score 31.0 avg score 45.5 time_steps 4046 learning_steps 202\n",
            "... saving models ...\n",
            "episode 89 score 200.0 avg score 47.2 time_steps 4246 learning_steps 212\n",
            "... saving models ...\n",
            "episode 90 score 53.0 avg score 47.2 time_steps 4299 learning_steps 214\n",
            "... saving models ...\n",
            "episode 91 score 200.0 avg score 48.9 time_steps 4499 learning_steps 224\n",
            "... saving models ...\n",
            "episode 92 score 130.0 avg score 49.8 time_steps 4629 learning_steps 231\n",
            "... saving models ...\n",
            "episode 93 score 60.0 avg score 49.9 time_steps 4689 learning_steps 234\n",
            "... saving models ...\n",
            "episode 94 score 133.0 avg score 50.8 time_steps 4822 learning_steps 241\n",
            "... saving models ...\n",
            "episode 95 score 200.0 avg score 52.3 time_steps 5022 learning_steps 251\n",
            "... saving models ...\n",
            "episode 96 score 200.0 avg score 53.8 time_steps 5222 learning_steps 261\n",
            "... saving models ...\n",
            "episode 97 score 200.0 avg score 55.3 time_steps 5422 learning_steps 271\n",
            "... saving models ...\n",
            "episode 98 score 200.0 avg score 56.8 time_steps 5622 learning_steps 281\n",
            "... saving models ...\n",
            "episode 99 score 200.0 avg score 58.2 time_steps 5822 learning_steps 291\n",
            "... saving models ...\n",
            "episode 100 score 200.0 avg score 60.0 time_steps 6022 learning_steps 301\n",
            "... saving models ...\n",
            "episode 101 score 200.0 avg score 61.9 time_steps 6222 learning_steps 311\n",
            "... saving models ...\n",
            "episode 102 score 200.0 avg score 63.6 time_steps 6422 learning_steps 321\n",
            "... saving models ...\n",
            "episode 103 score 200.0 avg score 65.5 time_steps 6622 learning_steps 331\n",
            "... saving models ...\n",
            "episode 104 score 200.0 avg score 67.3 time_steps 6822 learning_steps 341\n",
            "... saving models ...\n",
            "episode 105 score 142.0 avg score 68.6 time_steps 6964 learning_steps 348\n",
            "... saving models ...\n",
            "episode 106 score 189.0 avg score 70.4 time_steps 7153 learning_steps 357\n",
            "... saving models ...\n",
            "episode 107 score 200.0 avg score 72.3 time_steps 7353 learning_steps 367\n",
            "... saving models ...\n",
            "episode 108 score 200.0 avg score 74.2 time_steps 7553 learning_steps 377\n",
            "... saving models ...\n",
            "episode 109 score 200.0 avg score 76.0 time_steps 7753 learning_steps 387\n",
            "... saving models ...\n",
            "episode 110 score 200.0 avg score 77.9 time_steps 7953 learning_steps 397\n",
            "... saving models ...\n",
            "episode 111 score 200.0 avg score 79.8 time_steps 8153 learning_steps 407\n",
            "... saving models ...\n",
            "episode 112 score 200.0 avg score 81.6 time_steps 8353 learning_steps 417\n",
            "... saving models ...\n",
            "episode 113 score 200.0 avg score 83.5 time_steps 8553 learning_steps 427\n",
            "... saving models ...\n",
            "episode 114 score 200.0 avg score 85.3 time_steps 8753 learning_steps 437\n",
            "... saving models ...\n",
            "episode 115 score 200.0 avg score 87.2 time_steps 8953 learning_steps 447\n",
            "... saving models ...\n",
            "episode 116 score 200.0 avg score 89.0 time_steps 9153 learning_steps 457\n",
            "... saving models ...\n",
            "episode 117 score 200.0 avg score 90.9 time_steps 9353 learning_steps 467\n",
            "... saving models ...\n",
            "episode 118 score 200.0 avg score 92.8 time_steps 9553 learning_steps 477\n",
            "... saving models ...\n",
            "episode 119 score 200.0 avg score 94.7 time_steps 9753 learning_steps 487\n",
            "... saving models ...\n",
            "episode 120 score 200.0 avg score 96.6 time_steps 9953 learning_steps 497\n",
            "... saving models ...\n",
            "episode 121 score 200.0 avg score 98.4 time_steps 10153 learning_steps 507\n",
            "... saving models ...\n",
            "episode 122 score 142.0 avg score 99.6 time_steps 10295 learning_steps 514\n",
            "... saving models ...\n",
            "episode 123 score 200.0 avg score 101.5 time_steps 10495 learning_steps 524\n",
            "... saving models ...\n",
            "episode 124 score 200.0 avg score 103.3 time_steps 10695 learning_steps 534\n",
            "... saving models ...\n",
            "episode 125 score 200.0 avg score 105.1 time_steps 10895 learning_steps 544\n",
            "... saving models ...\n",
            "episode 126 score 199.0 avg score 106.8 time_steps 11094 learning_steps 554\n",
            "... saving models ...\n",
            "episode 127 score 200.0 avg score 108.7 time_steps 11294 learning_steps 564\n",
            "... saving models ...\n",
            "episode 128 score 200.0 avg score 110.6 time_steps 11494 learning_steps 574\n",
            "... saving models ...\n",
            "episode 129 score 200.0 avg score 112.4 time_steps 11694 learning_steps 584\n",
            "... saving models ...\n",
            "episode 130 score 200.0 avg score 114.3 time_steps 11894 learning_steps 594\n",
            "... saving models ...\n",
            "episode 131 score 200.0 avg score 116.2 time_steps 12094 learning_steps 604\n",
            "... saving models ...\n",
            "episode 132 score 200.0 avg score 117.9 time_steps 12294 learning_steps 614\n",
            "... saving models ...\n",
            "episode 133 score 200.0 avg score 119.7 time_steps 12494 learning_steps 624\n",
            "... saving models ...\n",
            "episode 134 score 200.0 avg score 120.9 time_steps 12694 learning_steps 634\n",
            "... saving models ...\n",
            "episode 135 score 200.0 avg score 122.7 time_steps 12894 learning_steps 644\n",
            "... saving models ...\n",
            "episode 136 score 200.0 avg score 124.5 time_steps 13094 learning_steps 654\n",
            "... saving models ...\n",
            "episode 137 score 200.0 avg score 126.2 time_steps 13294 learning_steps 664\n",
            "... saving models ...\n",
            "episode 138 score 200.0 avg score 127.4 time_steps 13494 learning_steps 674\n",
            "... saving models ...\n",
            "episode 139 score 200.0 avg score 129.2 time_steps 13694 learning_steps 684\n",
            "... saving models ...\n",
            "episode 140 score 200.0 avg score 131.1 time_steps 13894 learning_steps 694\n",
            "... saving models ...\n",
            "episode 141 score 200.0 avg score 132.5 time_steps 14094 learning_steps 704\n",
            "... saving models ...\n",
            "episode 142 score 200.0 avg score 134.1 time_steps 14294 learning_steps 714\n",
            "... saving models ...\n",
            "episode 143 score 200.0 avg score 135.4 time_steps 14494 learning_steps 724\n",
            "... saving models ...\n",
            "episode 144 score 200.0 avg score 136.8 time_steps 14694 learning_steps 734\n",
            "... saving models ...\n",
            "episode 145 score 200.0 avg score 138.5 time_steps 14894 learning_steps 744\n",
            "... saving models ...\n",
            "episode 146 score 200.0 avg score 140.3 time_steps 15094 learning_steps 754\n",
            "... saving models ...\n",
            "episode 147 score 200.0 avg score 141.9 time_steps 15294 learning_steps 764\n",
            "... saving models ...\n",
            "episode 148 score 200.0 avg score 143.6 time_steps 15494 learning_steps 774\n",
            "... saving models ...\n",
            "episode 149 score 200.0 avg score 145.3 time_steps 15694 learning_steps 784\n",
            "... saving models ...\n",
            "episode 150 score 200.0 avg score 147.0 time_steps 15894 learning_steps 794\n",
            "... saving models ...\n",
            "episode 151 score 200.0 avg score 148.3 time_steps 16094 learning_steps 804\n",
            "... saving models ...\n",
            "episode 152 score 200.0 avg score 149.9 time_steps 16294 learning_steps 814\n",
            "... saving models ...\n",
            "episode 153 score 200.0 avg score 151.4 time_steps 16494 learning_steps 824\n",
            "... saving models ...\n",
            "episode 154 score 200.0 avg score 153.0 time_steps 16694 learning_steps 834\n",
            "... saving models ...\n",
            "episode 155 score 200.0 avg score 154.3 time_steps 16894 learning_steps 844\n",
            "... saving models ...\n",
            "episode 156 score 200.0 avg score 155.5 time_steps 17094 learning_steps 854\n",
            "... saving models ...\n",
            "episode 157 score 200.0 avg score 156.7 time_steps 17294 learning_steps 864\n",
            "... saving models ...\n",
            "episode 158 score 157.0 avg score 157.7 time_steps 17451 learning_steps 872\n",
            "... saving models ...\n",
            "episode 159 score 200.0 avg score 159.4 time_steps 17651 learning_steps 882\n",
            "... saving models ...\n",
            "episode 160 score 200.0 avg score 161.1 time_steps 17851 learning_steps 892\n",
            "... saving models ...\n",
            "episode 161 score 200.0 avg score 162.9 time_steps 18051 learning_steps 902\n",
            "... saving models ...\n",
            "episode 162 score 200.0 avg score 164.4 time_steps 18251 learning_steps 912\n",
            "... saving models ...\n",
            "episode 163 score 200.0 avg score 165.8 time_steps 18451 learning_steps 922\n",
            "... saving models ...\n",
            "episode 164 score 200.0 avg score 167.3 time_steps 18651 learning_steps 932\n",
            "... saving models ...\n",
            "episode 165 score 200.0 avg score 169.0 time_steps 18851 learning_steps 942\n",
            "... saving models ...\n",
            "episode 166 score 200.0 avg score 170.0 time_steps 19051 learning_steps 952\n",
            "... saving models ...\n",
            "episode 167 score 200.0 avg score 171.5 time_steps 19251 learning_steps 962\n",
            "... saving models ...\n",
            "episode 168 score 149.0 avg score 172.6 time_steps 19400 learning_steps 970\n",
            "... saving models ...\n",
            "episode 169 score 200.0 avg score 173.1 time_steps 19600 learning_steps 980\n",
            "... saving models ...\n",
            "episode 170 score 200.0 avg score 174.8 time_steps 19800 learning_steps 990\n",
            "... saving models ...\n",
            "episode 171 score 200.0 avg score 176.7 time_steps 20000 learning_steps 1000\n",
            "... saving models ...\n",
            "episode 172 score 200.0 avg score 177.9 time_steps 20200 learning_steps 1010\n",
            "... saving models ...\n",
            "episode 173 score 200.0 avg score 179.5 time_steps 20400 learning_steps 1020\n",
            "... saving models ...\n",
            "episode 174 score 200.0 avg score 180.6 time_steps 20600 learning_steps 1030\n",
            "... saving models ...\n",
            "episode 175 score 200.0 avg score 182.3 time_steps 20800 learning_steps 1040\n",
            "... saving models ...\n",
            "episode 176 score 200.0 avg score 182.6 time_steps 21000 learning_steps 1050\n",
            "... saving models ...\n",
            "episode 177 score 200.0 avg score 183.5 time_steps 21200 learning_steps 1060\n",
            "... saving models ...\n",
            "episode 178 score 200.0 avg score 184.6 time_steps 21400 learning_steps 1070\n",
            "... saving models ...\n",
            "episode 179 score 200.0 avg score 186.1 time_steps 21600 learning_steps 1080\n",
            "... saving models ...\n",
            "episode 180 score 200.0 avg score 187.2 time_steps 21800 learning_steps 1090\n",
            "... saving models ...\n",
            "episode 181 score 200.0 avg score 188.3 time_steps 22000 learning_steps 1100\n",
            "... saving models ...\n",
            "episode 182 score 200.0 avg score 188.5 time_steps 22200 learning_steps 1110\n",
            "... saving models ...\n",
            "episode 183 score 200.0 avg score 189.5 time_steps 22400 learning_steps 1120\n",
            "... saving models ...\n",
            "episode 184 score 200.0 avg score 190.1 time_steps 22600 learning_steps 1130\n",
            "... saving models ...\n",
            "episode 185 score 200.0 avg score 190.4 time_steps 22800 learning_steps 1140\n",
            "... saving models ...\n",
            "episode 186 score 200.0 avg score 190.7 time_steps 23000 learning_steps 1150\n",
            "... saving models ...\n",
            "episode 187 score 200.0 avg score 191.8 time_steps 23200 learning_steps 1160\n",
            "... saving models ...\n",
            "episode 188 score 200.0 avg score 193.5 time_steps 23400 learning_steps 1170\n",
            "episode 189 score 200.0 avg score 193.5 time_steps 23600 learning_steps 1180\n",
            "... saving models ...\n",
            "episode 190 score 200.0 avg score 195.0 time_steps 23800 learning_steps 1190\n",
            "episode 191 score 200.0 avg score 195.0 time_steps 24000 learning_steps 1200\n",
            "... saving models ...\n",
            "episode 192 score 200.0 avg score 195.7 time_steps 24200 learning_steps 1210\n",
            "... saving models ...\n",
            "episode 193 score 200.0 avg score 197.1 time_steps 24400 learning_steps 1220\n",
            "... saving models ...\n",
            "episode 194 score 200.0 avg score 197.8 time_steps 24600 learning_steps 1230\n",
            "episode 195 score 200.0 avg score 197.8 time_steps 24800 learning_steps 1240\n",
            "episode 196 score 200.0 avg score 197.8 time_steps 25000 learning_steps 1250\n",
            "episode 197 score 200.0 avg score 197.8 time_steps 25200 learning_steps 1260\n",
            "episode 198 score 200.0 avg score 197.8 time_steps 25400 learning_steps 1270\n",
            "episode 199 score 200.0 avg score 197.8 time_steps 25600 learning_steps 1280\n",
            "episode 200 score 200.0 avg score 197.8 time_steps 25800 learning_steps 1290\n",
            "episode 201 score 200.0 avg score 197.8 time_steps 26000 learning_steps 1300\n",
            "episode 202 score 200.0 avg score 197.8 time_steps 26200 learning_steps 1310\n",
            "episode 203 score 200.0 avg score 197.8 time_steps 26400 learning_steps 1320\n",
            "episode 204 score 200.0 avg score 197.8 time_steps 26600 learning_steps 1330\n",
            "... saving models ...\n",
            "episode 205 score 200.0 avg score 198.4 time_steps 26800 learning_steps 1340\n",
            "... saving models ...\n",
            "episode 206 score 200.0 avg score 198.5 time_steps 27000 learning_steps 1350\n",
            "episode 207 score 200.0 avg score 198.5 time_steps 27200 learning_steps 1360\n",
            "episode 208 score 200.0 avg score 198.5 time_steps 27400 learning_steps 1370\n",
            "episode 209 score 200.0 avg score 198.5 time_steps 27600 learning_steps 1380\n",
            "episode 210 score 200.0 avg score 198.5 time_steps 27800 learning_steps 1390\n",
            "episode 211 score 200.0 avg score 198.5 time_steps 28000 learning_steps 1400\n",
            "episode 212 score 200.0 avg score 198.5 time_steps 28200 learning_steps 1410\n",
            "episode 213 score 200.0 avg score 198.5 time_steps 28400 learning_steps 1420\n",
            "episode 214 score 200.0 avg score 198.5 time_steps 28600 learning_steps 1430\n",
            "episode 215 score 200.0 avg score 198.5 time_steps 28800 learning_steps 1440\n",
            "episode 216 score 200.0 avg score 198.5 time_steps 29000 learning_steps 1450\n",
            "episode 217 score 64.0 avg score 197.1 time_steps 29064 learning_steps 1453\n",
            "episode 218 score 200.0 avg score 197.1 time_steps 29264 learning_steps 1463\n",
            "episode 219 score 200.0 avg score 197.1 time_steps 29464 learning_steps 1473\n",
            "episode 220 score 200.0 avg score 197.1 time_steps 29664 learning_steps 1483\n",
            "episode 221 score 200.0 avg score 197.1 time_steps 29864 learning_steps 1493\n",
            "episode 222 score 200.0 avg score 197.7 time_steps 30064 learning_steps 1503\n",
            "episode 223 score 200.0 avg score 197.7 time_steps 30264 learning_steps 1513\n",
            "episode 224 score 200.0 avg score 197.7 time_steps 30464 learning_steps 1523\n",
            "episode 225 score 200.0 avg score 197.7 time_steps 30664 learning_steps 1533\n",
            "episode 226 score 200.0 avg score 197.7 time_steps 30864 learning_steps 1543\n",
            "episode 227 score 200.0 avg score 197.7 time_steps 31064 learning_steps 1553\n",
            "episode 228 score 200.0 avg score 197.7 time_steps 31264 learning_steps 1563\n",
            "episode 229 score 200.0 avg score 197.7 time_steps 31464 learning_steps 1573\n",
            "episode 230 score 200.0 avg score 197.7 time_steps 31664 learning_steps 1583\n",
            "episode 231 score 200.0 avg score 197.7 time_steps 31864 learning_steps 1593\n",
            "episode 232 score 200.0 avg score 197.7 time_steps 32064 learning_steps 1603\n",
            "episode 233 score 200.0 avg score 197.7 time_steps 32264 learning_steps 1613\n",
            "episode 234 score 200.0 avg score 197.7 time_steps 32464 learning_steps 1623\n",
            "episode 235 score 200.0 avg score 197.7 time_steps 32664 learning_steps 1633\n",
            "episode 236 score 200.0 avg score 197.7 time_steps 32864 learning_steps 1643\n",
            "episode 237 score 200.0 avg score 197.7 time_steps 33064 learning_steps 1653\n",
            "episode 238 score 200.0 avg score 197.7 time_steps 33264 learning_steps 1663\n",
            "episode 239 score 200.0 avg score 197.7 time_steps 33464 learning_steps 1673\n",
            "episode 240 score 200.0 avg score 197.7 time_steps 33664 learning_steps 1683\n",
            "episode 241 score 200.0 avg score 197.7 time_steps 33864 learning_steps 1693\n",
            "episode 242 score 200.0 avg score 197.7 time_steps 34064 learning_steps 1703\n",
            "episode 243 score 200.0 avg score 197.7 time_steps 34264 learning_steps 1713\n",
            "episode 244 score 200.0 avg score 197.7 time_steps 34464 learning_steps 1723\n",
            "episode 245 score 200.0 avg score 197.7 time_steps 34664 learning_steps 1733\n",
            "episode 246 score 200.0 avg score 197.7 time_steps 34864 learning_steps 1743\n",
            "episode 247 score 200.0 avg score 197.7 time_steps 35064 learning_steps 1753\n",
            "episode 248 score 200.0 avg score 197.7 time_steps 35264 learning_steps 1763\n",
            "episode 249 score 200.0 avg score 197.7 time_steps 35464 learning_steps 1773\n",
            "episode 250 score 200.0 avg score 197.7 time_steps 35664 learning_steps 1783\n",
            "episode 251 score 200.0 avg score 197.7 time_steps 35864 learning_steps 1793\n",
            "episode 252 score 200.0 avg score 197.7 time_steps 36064 learning_steps 1803\n",
            "episode 253 score 200.0 avg score 197.7 time_steps 36264 learning_steps 1813\n",
            "episode 254 score 200.0 avg score 197.7 time_steps 36464 learning_steps 1823\n",
            "episode 255 score 200.0 avg score 197.7 time_steps 36664 learning_steps 1833\n",
            "episode 256 score 200.0 avg score 197.7 time_steps 36864 learning_steps 1843\n",
            "episode 257 score 200.0 avg score 197.7 time_steps 37064 learning_steps 1853\n",
            "episode 258 score 200.0 avg score 198.1 time_steps 37264 learning_steps 1863\n",
            "episode 259 score 200.0 avg score 198.1 time_steps 37464 learning_steps 1873\n",
            "episode 260 score 200.0 avg score 198.1 time_steps 37664 learning_steps 1883\n",
            "episode 261 score 200.0 avg score 198.1 time_steps 37864 learning_steps 1893\n",
            "episode 262 score 200.0 avg score 198.1 time_steps 38064 learning_steps 1903\n",
            "episode 263 score 200.0 avg score 198.1 time_steps 38264 learning_steps 1913\n",
            "episode 264 score 200.0 avg score 198.1 time_steps 38464 learning_steps 1923\n",
            "episode 265 score 200.0 avg score 198.1 time_steps 38664 learning_steps 1933\n",
            "episode 266 score 200.0 avg score 198.1 time_steps 38864 learning_steps 1943\n",
            "episode 267 score 200.0 avg score 198.1 time_steps 39064 learning_steps 1953\n",
            "... saving models ...\n",
            "episode 268 score 200.0 avg score 198.6 time_steps 39264 learning_steps 1963\n",
            "episode 269 score 200.0 avg score 198.6 time_steps 39464 learning_steps 1973\n",
            "episode 270 score 200.0 avg score 198.6 time_steps 39664 learning_steps 1983\n",
            "episode 271 score 200.0 avg score 198.6 time_steps 39864 learning_steps 1993\n",
            "episode 272 score 200.0 avg score 198.6 time_steps 40064 learning_steps 2003\n",
            "episode 273 score 200.0 avg score 198.6 time_steps 40264 learning_steps 2013\n",
            "episode 274 score 200.0 avg score 198.6 time_steps 40464 learning_steps 2023\n",
            "episode 275 score 200.0 avg score 198.6 time_steps 40664 learning_steps 2033\n",
            "episode 276 score 200.0 avg score 198.6 time_steps 40864 learning_steps 2043\n",
            "episode 277 score 200.0 avg score 198.6 time_steps 41064 learning_steps 2053\n",
            "episode 278 score 200.0 avg score 198.6 time_steps 41264 learning_steps 2063\n",
            "episode 279 score 200.0 avg score 198.6 time_steps 41464 learning_steps 2073\n",
            "episode 280 score 83.0 avg score 197.5 time_steps 41547 learning_steps 2077\n",
            "episode 281 score 32.0 avg score 195.8 time_steps 41579 learning_steps 2078\n",
            "episode 282 score 89.0 avg score 194.7 time_steps 41668 learning_steps 2083\n",
            "episode 283 score 109.0 avg score 193.8 time_steps 41777 learning_steps 2088\n",
            "episode 284 score 47.0 avg score 192.2 time_steps 41824 learning_steps 2091\n",
            "episode 285 score 114.0 avg score 191.4 time_steps 41938 learning_steps 2096\n",
            "episode 286 score 200.0 avg score 191.4 time_steps 42138 learning_steps 2106\n",
            "episode 287 score 200.0 avg score 191.4 time_steps 42338 learning_steps 2116\n",
            "episode 288 score 200.0 avg score 191.4 time_steps 42538 learning_steps 2126\n",
            "episode 289 score 200.0 avg score 191.4 time_steps 42738 learning_steps 2136\n",
            "episode 290 score 200.0 avg score 191.4 time_steps 42938 learning_steps 2146\n",
            "episode 291 score 200.0 avg score 191.4 time_steps 43138 learning_steps 2156\n",
            "episode 292 score 200.0 avg score 191.4 time_steps 43338 learning_steps 2166\n",
            "episode 293 score 200.0 avg score 191.4 time_steps 43538 learning_steps 2176\n",
            "episode 294 score 200.0 avg score 191.4 time_steps 43738 learning_steps 2186\n",
            "episode 295 score 200.0 avg score 191.4 time_steps 43938 learning_steps 2196\n",
            "episode 296 score 200.0 avg score 191.4 time_steps 44138 learning_steps 2206\n",
            "episode 297 score 200.0 avg score 191.4 time_steps 44338 learning_steps 2216\n",
            "episode 298 score 200.0 avg score 191.4 time_steps 44538 learning_steps 2226\n",
            "episode 299 score 200.0 avg score 191.4 time_steps 44738 learning_steps 2236\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "fgmvz4H9B97Z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}